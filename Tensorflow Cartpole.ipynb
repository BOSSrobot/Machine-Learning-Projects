{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow's implementation of an Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to see how to correctly use tensorflow to write a reinforcement learning agent. In this notebook we simply write down exactly what is in tensorflow's example but at the same time try to add notes and other useful information. The goal is to be able to read this page and understand how to correctly write an agent. \n",
    "\n",
    "Note that the algorithm being used here is an advantage actor critic that uses the returns G as the estimate and a value function V as a baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents  \n",
    "#### Setup:$\\hspace{5 mm}$ <a href = '#Imports'>Imports</a>\n",
    "#### Model:$\\hspace{5 mm}$ <a href = '#The Model'>The Model</a>\n",
    "\n",
    "#### Training:$\\hspace{5 mm}$ <a href = '#Collecting Training Data'>Collecting Training Data</a> $\\hspace{5 mm}$ <a href = '#Computing Expected Returns'>Computing Expected Returns</a> $\\hspace{5 mm}$ <a href = '#Computing the loss'>Computing the loss</a> $\\hspace{5 mm}$ <a href = '#Updating parameters'>Updating parameters</a> $\\hspace{5 mm}$ <a href = '#Training Loop'>Training Loop</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Imports'> </a>\n",
    "\n",
    "### Imports \n",
    "\n",
    "All the libraries you will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym                                     # Host the environment \n",
    "import numpy as np                             # Fast linear algebra\n",
    "import tensorflow as tf                        # Fast machine learning \n",
    "import tqdm                                    # Only used once, is a progress bar (so optional)\n",
    "\n",
    "from tensorflow.keras import layers            # Makes it easier to use functional API\n",
    "from typing import Any, List, Sequence, Tuple  # Lets us use type checking by giving us names to call \n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The Model'> </a>\n",
    "\n",
    "### The Model \n",
    "\n",
    "Define the model architecture that we will use. With an actor critic the network has one shared portion and then splits off into the actor (policy network) and the critic (value function). The ActorCritic Class does just that. Note that below we create a model as a global variable that we can generally use. The plan is to not have an explicit agent class but rather simply code the features ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions: int, num_hidden_units: int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units)\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "        \n",
    "num_actions = env.action_space.n  # Good practice, in this case it is 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Collecting Training Data'> </a>\n",
    "\n",
    "### Collecting Training Data\n",
    "\n",
    "Now we actually run an episode. The first function simply acts as a wrapper for the env.step() function. It adds on function type hints and specifies the input data type. \n",
    "\n",
    "The second function turns what we have into an tensorflow operation. Since we don't need to be able to take gradients through it we should be fine not using tf.py_function.  \n",
    "\n",
    "The third function actually runs the full epsiode using the previous function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: #ndarray is the data type of a numpy array\n",
    "    \"\"\"Returns state, reward, and done flag given an action\"\"\"\n",
    "    \n",
    "    state, reward, done, _ = env.step(action)                                  # Returns ndarray, float, boolean\n",
    "    return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "    \n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])\n",
    "\n",
    "def run_episode(initial_state: tf.Tensor, model: tf.keras.Model, max_steps: int) -> List[tf.Tensor]:\n",
    "    \"\"\"Runs a single epsiode to collect training data.\"\"\"\n",
    "\n",
    "    # tf.TensorArray allows you to create variable sized arrays. This means we can don't need to worry about episode length\n",
    "    action_probs = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    # env.reset() returns an intial observation. That is what initial_state is. \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state \n",
    "    \n",
    "    for t in tf.range(max_steps):\n",
    "        # Convert state into a batched tensor (batch size = 1) so that shape is (1, 4) in our case\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        # Run the model and get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "        \n",
    "        # Sample next action from probability distribution and track probability distribution\n",
    "        # categorical basically takes the unnormalized values and then pretends the probability is the softmax of them. \n",
    "        # this is why if you only wanted to normalize probabilities (0.1, 0.4 => 20%, 80%) you would just take ln (do the math)\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0] # This ends up having shape (1,1) so taking [0,0] corrects this\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "        \n",
    "        # Store critic values and log probabilities of action getting chosen \n",
    "        values = values.write(t, tf.squeeze(value)) #Squeezing gets rid of the batch aspect\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        \n",
    "        # Move forward in the environment\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "        # Store the reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "        # End episode?\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "    \n",
    "    # Get one dimensional tensors representing each of the following\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    \n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_state_shape:  (4,)\n",
      "initial_state:  [-0.01258566 -0.00156614  0.04207708 -0.00180545]\n",
      "\n",
      "NEW======================================\n",
      "Expanded state:  tf.Tensor([[-0.01258566 -0.00156614  0.04207708 -0.00180545]], shape=(1, 4), dtype=float64)\n",
      "model output: tf.Tensor([[-0.01142954  0.00853915]], shape=(1, 2), dtype=float32) tf.Tensor([[-0.01699052]], shape=(1, 1), dtype=float32)\n",
      "selected action:  tf.Tensor(1, shape=(), dtype=int64)\n",
      "action probabilities:  tf.Tensor([[0.495008 0.504992]], shape=(1, 2), dtype=float32)\n",
      "Normal state:  tf.Tensor([-0.01261699  0.1929279   0.04204097 -0.28092128], shape=(4,), dtype=float32)\n",
      "rewards:  tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n",
      "NEW======================================\n",
      "Expanded state:  tf.Tensor([[-0.01261699  0.1929279   0.04204097 -0.28092128]], shape=(1, 4), dtype=float32)\n",
      "model output: tf.Tensor([[-0.03415646 -0.07397287]], shape=(1, 2), dtype=float32) tf.Tensor([[-0.05889579]], shape=(1, 1), dtype=float32)\n",
      "selected action:  tf.Tensor(0, shape=(), dtype=int64)\n",
      "action probabilities:  tf.Tensor([[0.5099528 0.4900472]], shape=(1, 2), dtype=float32)\n",
      "Normal state:  tf.Tensor([-0.00875843 -0.00276775  0.03642255  0.02471923], shape=(4,), dtype=float32)\n",
      "rewards:  tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n",
      "Final: \n",
      "tf.Tensor([0.504992  0.5099528], shape=(2,), dtype=float32)\n",
      "tf.Tensor([-0.01699052 -0.05889579], shape=(2,), dtype=float32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Show what is happening inside run_epsiode\n",
    "initial_state = env.reset() # Note that I'm being lazy, env.reset() should be a Tensor\n",
    "max_steps = 2\n",
    "\n",
    "action_probs = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "\n",
    "initial_state_shape = initial_state.shape\n",
    "state = initial_state \n",
    "print(\"initial_state_shape: \", initial_state_shape)\n",
    "print(\"initial_state: \", state)\n",
    "\n",
    "for t in tf.range(max_steps):\n",
    "    print(\"\\nNEW======================================\")\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    print(\"Expanded state: \", state)\n",
    "    \n",
    "    action_logits_t, value = model(state)\n",
    "    print(\"model output:\", action_logits_t, value)\n",
    "\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "    print(\"selected action: \", action)\n",
    "    print(\"action probabilities: \", action_probs_t)\n",
    "\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "    \n",
    "    state, reward, done = tf_env_step(action)\n",
    "    print(\"Normal state: \", state)\n",
    "    state.set_shape(initial_state_shape)\n",
    "\n",
    "    rewards = rewards.write(t, reward)\n",
    "    print(\"rewards: \", reward)\n",
    "\n",
    "    if tf.cast(done, tf.bool):\n",
    "        print(\"leaving\")\n",
    "        break\n",
    "\n",
    "action_probs = action_probs.stack()\n",
    "values = values.stack()\n",
    "rewards = rewards.stack()\n",
    "print(\"\\nFinal: \")\n",
    "print(action_probs)\n",
    "print(values)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Computing Expected Returns'> </a>\n",
    "\n",
    "#### Computing Expected Returns\n",
    "\n",
    "Using the data we gain from an episode of training, we can now compute the return G. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(rewards: tf.Tensor, gamma: float, standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "    \n",
    "    # Create the variables we must track. We use tf.shape(rewards)[0] because that is the epsiode length\n",
    "    n = tf.shape(rewards)[0]                           \n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "    \n",
    "    # To efficiently compute the return, we start by computing it for the last epsiode and then * by gamma followed by + r. \n",
    "    # We basically work from down up to find the returns\n",
    "    \n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32) # We use [::-1] to order the rewards from last one to first\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    \n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum \n",
    "        discounted_sum.set_shape(discounted_sum_shape) # Allows us to ensure that everything is going correctly\n",
    "        returns = returns.write(i, discounted_sum) \n",
    "        \n",
    "    returns = returns.stack()[::-1] # Now the returns are ordered from last to first so we swap em again. Shape is (n, )\n",
    "    \n",
    "    # 0 mean and unit std can help with convergence \n",
    "    if standardize:\n",
    "        returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns) + eps)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1.3388376   0.7397628   0.07412429 -0.66547424 -1.4872503 ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Show returns\n",
    "action_probs, values, rewards = run_episode(env.reset(), model, 5) # Note that I'm being lazy, env.reset() should be a Tensor\n",
    "returns = get_expected_return(rewards, 0.9)\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Computing the loss'> </a> \n",
    "\n",
    "### Computing the loss\n",
    "\n",
    "The critic loss is just a regression problem, where we are tying to make V as close to G. As such, we can use:\n",
    "\n",
    "$L_{critic} = L_{\\delta}(G, V_{\\theta}^\\pi)$ where $L_{\\delta}$ is the huber loss (less sensitive to outliers than MSE)\n",
    "\n",
    "The actor loss is based off of policy gradients with the critic as a state-dependent baseline\n",
    "\n",
    "$L_{actor} = - \\sum^{T}_{t=1} \\log{\\pi_{\\theta}(a_t|s_t)} * [G(s_t, a_t) - V^{\\pi}_{\\theta}(s_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(action_probs: tf.Tensor, values: tf.Tensor, returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    " \n",
    "    advantage = returns - values\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "    \n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Updating parameters'> </a>\n",
    "\n",
    "### Updating parameters\n",
    "\n",
    "Now we update the parameters. Using tf.function() turns this into a callable graph which can help speed things up greatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(initial_state: tf.Tensor, model: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer,\n",
    "               gamma: float, max_steps_per_episode: int) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards = run_episode(initial_state, model, max_steps_per_episode)\n",
    "        \n",
    "        # Calculate expected returns\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "        \n",
    "        # Convert the data to get the correct shape: (n, 1) \n",
    "        action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "        \n",
    "    # Compute the gradients from the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # Apply the gradients to the model's parameters\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    # Get the final reward\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'Training Loop'> </a>\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "Now we write the actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 539:  54%|██████████████            | 539/1000 [00:52<00:45, 10.19it/s, episode_reward=200, running_reward=195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 539: average reward: 195.03!\n",
      "Wall time: 52.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "max_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# The Cartpole environment is considered solved if the reward is over 195 over 100 trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(), dtype = tf.float32)\n",
    "        episode_reward = int(train_step(initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "        \n",
    "        running_reward = 0.01 * episode_reward + running_reward * 0.99\n",
    "        \n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        if running_reward > reward_threshold:  \n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = tf.constant(env.reset(), dtype = tf.float32)\n",
    "max_steps = 10000\n",
    "\n",
    "# env.reset() returns an intial observation. That is what initial_state is. \n",
    "initial_state_shape = initial_state.shape\n",
    "state = initial_state \n",
    "\n",
    "for t in tf.range(max_steps):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "\n",
    "    action_logits_t, value = model(state)        \n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0] # This ends up having shape (1,1) so taking [0,0] corrects this\n",
    "\n",
    "    env.render()\n",
    "    state, reward, done = tf_env_step(action)\n",
    "\n",
    "    # End episode?\n",
    "    if tf.cast(done, tf.bool):\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
