{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Top'></a> \n",
    "\n",
    "# Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers # here, this module is referred to as layers and a layers subclass instance is referred to l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will be used later on\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "#### <a href = '#Making a Model (Built-in)'>Section: Making a Model (Built-in)</a>\n",
    "#### <a href = '#Making a Model (Subclassing)'>Section: Making a Model (Subclassing)</a>\n",
    "#### <a href = '#Training and Evaluation (Built-in)'>Section: Training and Evaluation (Built-in)</a>\n",
    "#### <a href = '#Customizing Training'>Section: Customizing Training (Built-in)</a>\n",
    "#### <a href = '#Index'>The Index</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Making a Model (Built-in)'></a>\n",
    "\n",
    "## Making a Model (Built-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Layers\n",
    "\n",
    "This is the easiest model to make but it is also the least flexible. You should use sequential when you just have a plain stack of layers, one after the other, where each layer has one input tensor and one output tensor. Creating one is extremely simple. \n",
    "\n",
    "First note that you can get layers from tensorflow.keras.layers. The reason you don't need to pass in input dimensions is that the first time you call a layer it internally calls a build() method that uses the input to configure the variables. One possible layer could be:\n",
    "* <b>layers.Dense(output_features, name = 'None'...)</b> => Creates your favorite classic layer with output_features neurons at the end. \n",
    "\n",
    "* NOTE: There is a ton more arguments you can pass here visit $ \\hspace{2mm} $\n",
    "<a href='https://keras.io/api/layers/regularizers/'>regularization</a> $\\hspace{2mm}$ \n",
    "<a href = 'https://keras.io/api/layers/activations/'>activation functions</a> $ \\hspace{2mm} $\n",
    "<a href = 'https://keras.io/api/layers/initializers/'>initializers</a>\n",
    "\n",
    "\n",
    "A couple attributes of a specific layer(see <a href = 'https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#attributes_1'>attr</a> for more)\n",
    "\n",
    "* <b>l.activation</b> => Returns the activation function used by the specific layer\n",
    "* <b>l.trainable_weights</b> => Returns a list of all the variables that gradient descent can be applied on\n",
    "* <b>l.non_trainable_weights</b> => Returns a list of all the variables not being trained\n",
    "* <b>l.weights</b> => Returns the concatenation of both of the lists above\n",
    "* <b>l.name</b> => Returns name of layer\n",
    "* <b>l.output</b> => Returns output of that layer\n",
    "\n",
    "Note here we use:\n",
    "* <b>tf.random.normal(shape, name = None)</b> => creates an array from a normal distribution with shape shape and name name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.17495963, -0.03159937]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_l = layers.Dense(2, name = 'dense_l', kernel_initializer = 'random_normal')\n",
    "dense_l(tf.random.normal([1,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a Sequential Model \n",
    "\n",
    "You can create a Sequential model by using the first method or using the other 2 methods\n",
    "* <b>pass a list of layers/models to the Sequential constructor </b>\n",
    "* <b>smodel.add(l)</b> => Appends l as a layer to the end\n",
    "* <b>smodel.pop()</b> => Pops layer at the end\n",
    "\n",
    "You can also check what layers currently exist with \n",
    "* <b>smodel.layers</b> => Returns list of current layers \n",
    "\n",
    "Note that simply making a instantiating Dense layers won't create the biases and the weights (because of how build() works). Normally you won't notice since the first input you pass is used to create this. However, this also means you won't be able to use methods like <b>model.summary()</b> or <b>model.weights</b>. If you want to specifiy the input shape using either works\n",
    "* <b>keras.Input(input_features, name = 'None'...)</b> => tells the model the number of features in the input. Don't include batch size as that is assumed to be variable. dtype is also assumed to be tf.float32\n",
    "* <b>layers.Dense(..., input_shape, ...)</b> => passing in a tuple with the input shape to the first layer in a model automatically builds it as well\n",
    "\n",
    "If you want to see a summary of what the model looks like you can use\n",
    "* <b>smodel.summary()</b> => Prints a summary of what the model looks like. Useful in debugging when used with <b>add()</b>. Note that the None dimension simply means that dimension can have any shape. For us this is because batch sizes can be variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MySequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 56\n",
      "Trainable params: 56\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smodel = keras.Sequential([\n",
    "        keras.Input(5, name = 'inp'),\n",
    "        layers.Dense(5, activation=\"relu\", name = 'dense_1'),\n",
    "        layers.Dense(3, activation=\"relu\", name = 'dense_2'),\n",
    "        layers.Dense(2, name = 'dense_3'),\n",
    "], name = 'MySequential')\n",
    "\n",
    "smodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main idea\n",
    "This API is a way to create models that are more flexible than Sequential. It can handle non-linear topologies, shared layers, and multiple inputs and outputs. The way it works is based on the assumption that the model is a directed acyclic graph (no closed loops) so the API builds a graph of layers. The way it works is that everytime you call a function on some tensor it makes a connection between that tensor and the function. When you have multiple calls they all build up and form the graph. \n",
    "\n",
    "Specifically, you start with an input tensor (usually keras.Input). Then you call your layers (or models ;) ) in order on input. Finally, you use the first one. You can also see what you have so far by (again) using the second one\n",
    "* <b>keras.Model(inputs, outputs, name = None)</b> => creates a model using the functions used to get from input to output\n",
    "* <b>model.summary()</b> => Prints out what layers and output shapes the model has\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inp (InputLayer)             [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 56\n",
      "Trainable params: 56\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = keras.Input(5, name='inp')\n",
    "x = layers.Dense(5, activation=\"relu\", name = 'dense_1')(inp)\n",
    "x = layers.Dense(3, activation=\"relu\", name = 'dense_2')(x)\n",
    "out = layers.Dense(2, name = 'dense_3')(x)\n",
    "\n",
    "model = keras.Model(inp, out, name = 'my sequential')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some things you can do \n",
    "\n",
    "You can reuse layers/models to create models, that is, you can proceed normally, but create another model using some input and output between what you already had. See <a href='https://www.tensorflow.org/guide/keras/functional#all_models_are_callable_just_like_layers'>this</a> to how this can be used to create an autoencoder model\n",
    "\n",
    "You can also have multiple inputs and outputs and have layers/models just enter in the middle of your structure (still a graph). With these you can also assign your own losses to each output and weights on the losses too! See <a href='https://www.tensorflow.org/guide/keras/functional#all_models_are_callable_just_like_layers'>this</a> to how this can be used to create something cool\n",
    "\n",
    "By using layer.output you can also create a feature extraction model in one line of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Making a Model (Subclassing)'></a>\n",
    "\n",
    "## Making a Model (Subclassing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making layers for models\n",
    "\n",
    "Layers are a key part of Keras. They encapsulate both a state (consisting of the layer's variables) and a transformation from inputs to outputs. Creating a layer looks the same as it would in tensorflow, except for the way building variables is done. To start, subclass <b>tf.keras.layers.Layer</b>. You will need to implement the following methods:\n",
    "\n",
    "1. <b>\\_\\_init__(self)</b> => call super and store basic information about the layer like the output dimension. Hold off on the specifying the input dimension. \n",
    "2. <b>build(self, input_shape)</b> => takes in an input_shape and creates the models variables accordingly through the method \n",
    "    * <b>self.add_weights(shape, initializer=None)</b> => Add trainable variable with specific shape and initializer provided. This can be the string identifier or an initializer object (like <b>tf.random_normal_initializer()</b>)\n",
    "3. <b>call(self, x)</b> => Put in whatever logic you use to tranform the input to the output. If you are writing a layer that depends on whether you are training or not (like dropout) you can also accept the boolean argument training and keras will be able to use it.\n",
    "    \n",
    "Note that if you assign a layer instance as an attribute of another layer (to create a block), the outer layer automatically tracks the variables of the inner layer. It is a good idea to create these layers in the <b>\\_\\_init()__</b> method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.14019845,  0.23774377, -0.3617659 ,  0.33904552,  0.26466587,\n",
       "         0.22282706,  0.21561241, -0.11508423,  0.40910867,  0.12016662]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDense(layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_dimension, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dimension\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.output_dim), initializer= tf.random_normal_initializer())\n",
    "        self.B = self.add_weight(shape= (self.output_dim,), initializer = 'zeros')\n",
    "    \n",
    "    def call(self, x): \n",
    "        return tf.nn.tanh(x @ self.W + self.B)\n",
    "    \n",
    "class MyMLPBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self): #kwargs not needed but helpful for using all of keras\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.l1 = MyDense(300)\n",
    "        self.l2 = MyDense(100)\n",
    "        self.l3 = MyDense(10)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "        \n",
    "m = MyMLPBlock()\n",
    "m(keras.layers.Input(784)) #This is optional but put here to demonstrate it works\n",
    "m(tf.random.normal((1,784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding losses and metrics to your layers\n",
    "\n",
    "Losses refer to a quantity your model's endgoal is to minimize and metrics refer to things you track during training. Note that every time you call the layer its losses are reset so that it is fresh from the most recent pass. Additionally, any out layers also track inner layer's losses. To add a loss use the following.\n",
    "* <b>self.add_loss(value)</b> => Makes specific layer also have this loss. \n",
    "To retrieve this loss later on use \n",
    "* <b>layer.losses</b> => Returns the loss that layer has\n",
    "\n",
    "The way metrics work is that when you add a metric it adds a metric object that keeps track of whatever you make it keep track of. To add a metric use\n",
    "* <b>self.add_metric(value, name)</b> => Makes specific layer track this metric. Name is useful to know which number is from which metric. \n",
    "To then retrieve the result you want to use this method of a metric\n",
    "* <b>metric.result()</b> => Returns the value it stored\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.035382714>]\n",
      "tf.Tensor([[0.4227244  0.51615036 0.39882913 0.44017404 0.40596846]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MyDense(layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_dimension, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dimension\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.output_dim), initializer='random_normal')\n",
    "        self.B = self.add_weight(shape= (self.output_dim,), initializer = 'zeros')\n",
    "        self.B.assign_add(tf.fill(self.B.shape, 0.5)) #Increase bias so that activations are more likely above 0.5\n",
    "    \n",
    "    def call(self, x): \n",
    "        output = tf.nn.tanh(x @ self.W + self.B)\n",
    "    \n",
    "        #New stuff here:\n",
    "        self.add_loss(tf.norm(self.W)**2)     \n",
    "        num_under_half = tf.cast(tf.math.count_nonzero(tf.math.greater(0.5, output)), dtype=tf.float32)\n",
    "        self.add_metric(num_under_half, name='num under 5')\n",
    "        \n",
    "        return output\n",
    "\n",
    "layer = MyDense(5)\n",
    "layer(keras.layers.Input(4))\n",
    "output = layer(tf.constant([[1,1,0,0]], dtype = tf.float32))\n",
    "\n",
    "print(layer.losses)                \n",
    "print(output)\n",
    "print(layer.metrics[0].result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making models using layers\n",
    "\n",
    "Generally layers are used to describe the inner computation blocks and models are used to define the outer model you will be training. The Model class has the same API as the Layer class but has a couple extra features like being able to use built-in training methods <b>model.fit(), model.predict(), model.evaluate()</b>, you can use <b>model.layers</b> to see the layers the model has, and you can save and serialize your model. Basically if your training or saving you should use Model\n",
    "\n",
    "Don't worry about the code that follows. That will be explained in the next section. For now just note the l2 loss gets factored in and the metric is seen in fit()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5487 - sparse_categorical_accuracy: 0.7653 - l2_loss: 0.0721\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0778 - sparse_categorical_accuracy: 0.9144 - l2_loss: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201861c7e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDense(layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_dimension, activation=tf.nn.relu, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dimension\n",
    "        self.regular = 0.005\n",
    "        self.activation = activation\n",
    "    \n",
    "    def build(self, inp):\n",
    "        \n",
    "        self.W = self.add_weight(shape=(inp[-1], self.output_dim))\n",
    "        init = tf.math.divide(tf.random.normal([inp[-1], self.output_dim]), tf.sqrt(tf.cast(inp[-1], dtype = tf.float32)))\n",
    "        self.W.assign(init)\n",
    "        self.B = self.add_weight(shape= (self.output_dim,), initializer = 'zeros')\n",
    "    \n",
    "    def call(self, x): \n",
    "        l2 = self.regular*tf.norm(self.W)**2\n",
    "        self.add_loss(l2)   \n",
    "        self.add_metric(l2, name = 'l2_loss')\n",
    "        return self.activation(x @ self.W + self.B)\n",
    "    \n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = MyDense(300)\n",
    "        self.l2 = MyDense(100)\n",
    "        self.l3 = MyDense(10, tf.nn.softmax)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.l1(inputs)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "digit_predictor = MyModel()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=7e-2)\n",
    "\n",
    "digit_predictor.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "digit_predictor.fit(x_train, y_train, epochs=2, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training and Evaluation (Built-in)'></a>\n",
    "\n",
    "## Training and Evaluation (Built-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code will be used frequently!\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Info \n",
    "\n",
    "This section looks at how you can use built-in Keras methods to train and see your model's performance. When using data with built-in models you should make sure they are either NumPy Arrays or a <b>tf.data.Dataset</b> instance. This section is long so here is what you can expect:\n",
    "* Specifying optimizers, losses and metrics \n",
    "* Custom losses\n",
    "* Custom metrics\n",
    "* Training and evaluating the model \n",
    "* Sample/Class Weighting \n",
    "* Models with multiple inputs and outputs\n",
    "* Using callbacks \n",
    "* Learning Rate Schedules\n",
    "* Using the tensorboard callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying optimizers, losses and metrics\n",
    "\n",
    "Making a model is great but you also need to train it. Before training you need to specify the optimizer, loss, and optional metrics you want to track. These are all arguments you can pass into this method:\n",
    "* <b>model.compile(optimzer, loss, metrics = None)</b> => Makes the model use specified functions when training with built-in methods\n",
    "\n",
    "You can find options for <a href = 'https://keras.io/api/optimizers'>optimizers</a>, <a href = 'https://keras.io/api/losses'>losses</a>, and <a href = 'https://keras.io/api/metrics'>metrics</a> in the keras docs. Since we're using built-in methods we can also just pass in the string identifier instead of the object. \n",
    "\n",
    "Here are some of the most useful ones:\n",
    "1. Optimizers: <b>SGD()</b> (with or without momentum), <b>RMSprop()</b>, <b>Adam()</b>\n",
    "2. Losses: <b>MeanSquaredError()</b> - regression, <b>CosineSimilarity()</b> - regression, <b>KLDivergence()</b> - probabilistic, <b>CategoricalCrossentropy</b> - probabilistic\n",
    "3. Metrics: <b>BinaryAccuracy()</b> - 2 targets, <b>CategoricalAccruacy()</b> - one-hot encoded targets, <b>SparseCategorialAccuracy()</b> - integer targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom losses\n",
    "\n",
    "To create custom losses you can do 2 things. The first way is to create a function that accepts two inputs, y_true and y_pred and then return the error.\n",
    "\n",
    "If you need to accept other arguments you can subclass <b>tf.keras.losses.Loss</b>. You will need to implement\n",
    "* <b>\\_\\_init(self)__</b> => accept parameters to use during call. Remember to call super\n",
    "* <b>call(self, y_true, y_pred)</b> => Use the targets and predictions to compute the model's loss\n",
    "\n",
    "Note that certain losses like regularization don't require targets and predicitions. For those kinds of losses you can add losses from within a custom layer (as seen above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true-y_pred))\n",
    "\n",
    "#model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error, metrics = ['accuracy'])\n",
    "\n",
    "class CustomMSE(keras.losses.Loss):\n",
    "    def __init__(self, reg_factor=0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reg = reg_factor\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true-y_pred))\n",
    "        confidence_unbooster = tf.math.reduce_mean(tf.square(0.5-y_pred)) #Reduce overfitting by making it less confident?\n",
    "        return mse + self.reg * confidence_unbooster\n",
    "\n",
    "model.compile(optimizer='Adam', loss = CustomMSE(name='nice'), metrics = ['accuracy'])\n",
    "\n",
    "#Note that both need one-hot encoded labels so we would need to convert the sparse y_train. \n",
    "#y_train_one_hot = tf.one_hot(y_train, depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Metrics\n",
    "\n",
    "To create a custom metric you have to subclass <b>tf.keras.metrics.Metric</b>. You will need to implement \n",
    "1. <b>\\_\\_init(self)</b> => Create state variables that you will need to track for your metric. You might need to use \n",
    "    * self.add_weight()\n",
    "2. <b>update_state(self, y_true, y_pred, sample_weight=None)</b> => Use the targets, predicitions, and optional weights to update the state variables keeping track of the metric\n",
    "3. <b>result(self)</b> => Use the state variables to return the final result\n",
    "4. <b>reset_states(self)</b> => Reinitialize the state of the metric\n",
    "\n",
    "Note that certain metrics like the one we made above don't require targets and predicitions. For those kinds of metrics you can use add_metric() from within a custom layer (as seen above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many samples were correctly identified as belonging to the right class\n",
    "class CategoricalTruePositives(keras.metrics.Metric):\n",
    "    def __init__(self, name = 'categorical_true_positives'):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name)\n",
    "        self.true_positives = self.add_weight(name='ctp', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None): \n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis = 1), shape = (-1,1)) \n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        \n",
    "        if sample_weight:\n",
    "            sample_weight = tf.cast(sample_weight, tf.float32)\n",
    "            values *= sample_weight\n",
    "            \n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0.0)\n",
    "        \n",
    "model.compile(optimizer='Adam', loss = 'sparse_categorical_crossentropy', metrics=[CategoricalTruePositives(), 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluating the model\n",
    "\n",
    "keras makes things super easy. All you need to do is call one function. If your using <b>np.array</b> use it like this\n",
    "* <b>model.fit(x_train, y_train, batch_size = None, validation_split = 0.0, epochs = 1, validation_data = None)</b> => Trains the model for specified epochs. If validation_split is given it takes away that much data to reserve for validation. If validation_data (a tuple containing x_train and y_train) is given it keeps that for validation. Note that validation_split should only be used with numpy data \n",
    "\n",
    "You can also use a <b>tf.data.Dataset</b> object. To start we use the following methods to get the input into the form of a Dataset (Note that although there are many different dataset classes they can all be used with fit). Finally, we use fit in a different way\n",
    "* <b>tf.data.Dataset.from_tensor_slices((x_train, y_train))</b> => Take an aeeay \n",
    "* <b>dataset.shuffle(buffer_size)</b> => Creates a buffer with the specified size. When getting data it chooses a random element in the buffer and then replaces it with the next element in the dataset. (So buffer_size of 1 actually gets no shuffling). This returns a ShuffleDataset Object\n",
    "* <b>dataset.batch(batch_size)</b> => Returns BatchDataset. This contains data but in batches with size that you specify\n",
    "* <b>model.fit(dataset, epochs=1, steps_per_epoch=None, validation_data=None, validation_steps=None)</b> => Trains the model on the dataset for specified number of epochs. If you don't want to use the entire dataset per epoch you can use steps_per_epoch to specify how many batches you want to see per epoch. You can also pass in validation data and do the same thing with steps with the last two arguments\n",
    "\n",
    "You should use these depending on the circumstances. Usually you should use:\n",
    "* NumPy data when your data is small and fits in memory \n",
    "* Dataset objects if your data is large and you need to do distributed training\n",
    "* Sequence objects if you have large datasets and you need to use a lot of custom Python processing. See <a href='https://www.tensorflow.org/guide/keras/train_and_evaluate#other_input_formats_supported'>this</a> to learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 1s 3ms/step - loss: 1.9984 - categorical_true_positives: 1350.7327 - accuracy: 0.3352\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.4414 - categorical_true_positives: 2861.7327 - accuracy: 0.8716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2e493eee0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "data = data.shuffle(x_train.shape[0]).batch(64)\n",
    "model.fit(data, epochs=2, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2283 - categorical_true_positives: 37332.0000 - accuracy: 0.9333 - val_loss: 0.1943 - val_categorical_true_positives: 9386.0000 - val_accuracy: 0.9386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f28c286d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use one of the above compiling schemes\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample/Class weighting\n",
    "\n",
    "Sometimes your data doesn't have a lot of samples of one class. In these cases it is a good idea to weight them. By default fit() uses the frequency of classes to weight the data but you can set them yourself as well. To weight by class use:\n",
    "* <b>model.fit(..., class_weights=None, ...)</b> => Weights the data during training based on class_weights. This argument should be a dictionary with keys being the indexes of the class (0-n-1) and the values being the weight of the corresponding class.\n",
    "When weighting by samples it depends on whether you are using NumPy or Dataset. With NumPy use the former in fit() and with Dataset use the latter when creating the dataset:\n",
    "* <b>model.fit(..., sample_weight=None, ...)</b> => Gives each specific sample weightage based on sample_weight. This argument should be a array of numbers that has the number of elements in your dataset in its size. Each index should have the corresponding weight of the sample. \n",
    "* <b>tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))</b> => Here sample_weight should be the exact same as above\n",
    "\n",
    "EXAMPLES NOT SHOWN HERE. CHECK OUT <a href='https://www.tensorflow.org/guide/keras/train_and_evaluate#using_sample_weighting_and_class_weighting'>DOCS</a> TO LEARN MORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models with multiple inputs and outputs\n",
    "\n",
    "Sometimes your models have multiple inputs and outputs and they might need different losses and different metrics that they need to follow. Furthermore, each loss might need to be weighted differently. To accomplish this use:\n",
    "* <b>model.compile(loss, metrics, loss_weights)</b> => instead of passing one object pass in a list for loss and metrics. If you gave the output layers a name you could also pass in a dict and have the keys be the names. If you don't want to train on one output layer you can also make its corresponding loss None. loss_weights can be a list or a dictionary containing the real valued weights.\n",
    "\n",
    "Similarly, when using fit(), just pass in a list to fit() or the Dataset() object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 5.6363 - out1_loss: 1.5590 - out2_loss: 5.1686 - out1_mean_absolute_percentage_error: 602.5876 - out1_mean_absolute_error: 0.9348 - out2_categorical_accuracy: 0.5266\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.5920 - out1_loss: 1.2042 - out2_loss: 4.2307 - out1_mean_absolute_percentage_error: 648.1147 - out1_mean_absolute_error: 0.8102 - out2_categorical_accuracy: 0.4798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x182c31314c0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = keras.Input(9, name='in1')\n",
    "inp2 = keras.Input(4, name='in2')\n",
    "\n",
    "x = layers.concatenate([inp1, inp2])\n",
    "\n",
    "out1 = layers.Dense(4, name='out1')(x)\n",
    "out2 = layers.Dense(2, name='out2')(x)\n",
    "\n",
    "model = keras.Model(inputs=[inp1, inp2], outputs=[out1, out2])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss= {'out1' : keras.losses.MeanSquaredError(), 'out2' : keras.losses.CategoricalCrossentropy()},\n",
    "    loss_weights = [0.3,1],\n",
    "    metrics=[[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanAbsoluteError()], \n",
    "             [keras.metrics.CategoricalAccuracy()]])\n",
    "\n",
    "# Dummy NumPy data\n",
    "x_1 = np.random.random_sample(size=(1000,9))\n",
    "x_2 = np.random.random_sample(size=(1000, 4))\n",
    "y_1 = np.random.random_sample(size=(1000, 4))\n",
    "y_2 = np.random.random_sample(size=(1000, 2))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit({'in1':x_1, 'in2':x_2}, [y_1, y_2], batch_size=32, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Callbacks \n",
    "Callbacks are objects that are called at different points during training (start/end of epoch/batch etc). They have many applications like checkpointing, implementing learning rate schedules, sending an email when training is done, basically anything dynamic. They get passed as a list to a callbacks argument to fit() like metrics. Here are some useful callbacks:\n",
    "* <b>keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-2, patience=2, verbose=1)</b> => Implements Early stopping into your model. monitor is a string that refers to the name of what you are monitoring. min_delta is the threshold below which it doesn't count as improving. patience is how many epochs you want to give it a chance. verbose decides how much detail is in the updates it provides. \n",
    "* <b>keras.callbacks.ModelCheckpoint(filepath=\"mymodel_{epoch}\", save_best_only=True, monitor=\"val_loss\", verbose=1, save_freq=None)</b> => Saves the model. filepath is where you save the model. save_best_only and monitor make it so that the current checkpoint gets overwritten only if whatever you are monitoring has improved. verbose once again decides how much it displays. save_freq decides how many batches pass before it saves. Check out the <a href='https://www.tensorflow.org/guide/keras/train_and_evaluate#checkpointing_models'>DOCS</a> to see it in action being more organized (having its own directory, etc.)\n",
    "\n",
    "If you want to create your own custom callback you would need to subclass <b>keras.callbacks.Callback</b> and use functions you can check out <a href='https://www.tensorflow.org/guide/keras/custom_callback/'>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1398 - categorical_true_positives: 38352.0000 - accuracy: 0.9588 - val_loss: 0.1572 - val_categorical_true_positives: 9520.0000 - val_accuracy: 0.9520\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15724, saving model to mymodel_1\n",
      "INFO:tensorflow:Assets written to: mymodel_1\\assets\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1047 - categorical_true_positives: 38777.0000 - accuracy: 0.9694 - val_loss: 0.1418 - val_categorical_true_positives: 9570.0000 - val_accuracy: 0.9570\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15724 to 0.14180, saving model to mymodel_2\n",
      "INFO:tensorflow:Assets written to: mymodel_2\\assets\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0825 - categorical_true_positives: 39001.0000 - accuracy: 0.9750 - val_loss: 0.1301 - val_categorical_true_positives: 9604.0000 - val_accuracy: 0.9604\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14180 to 0.13010, saving model to mymodel_3\n",
      "INFO:tensorflow:Assets written to: mymodel_3\\assets\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0669 - categorical_true_positives: 39221.0000 - accuracy: 0.9805 - val_loss: 0.1377 - val_categorical_true_positives: 9603.0000 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13010\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0552 - categorical_true_positives: 39336.0000 - accuracy: 0.9834 - val_loss: 0.1258 - val_categorical_true_positives: 9636.0000 - val_accuracy: 0.9636\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13010 to 0.12584, saving model to mymodel_5\n",
      "INFO:tensorflow:Assets written to: mymodel_5\\assets\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f28e68e820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-2, patience=2, verbose=1), \n",
    "            keras.callbacks.ModelCheckpoint(filepath=\"mymodel_{epoch}\", save_best_only=True, monitor=\"val_loss\", verbose=1)] \n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=64, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Schedules\n",
    "\n",
    "You can either have a static schedule which has all the options preset, or you can have a dynamic schedule that uses callbacks to decide what to do. \n",
    "\n",
    "If you are using a static schedule, first create the object using the function below (other built-in schedules inlcude: ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, and InverseTimeDecay). Then pass that object in as the argument learning_rate to the optimzer: \n",
    "* <b>keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=False)</b> => Creates a learning rate schedule object which calculates the rate with the following formula: inital_lr * drate ^ (steps/dsteps). Making staircase True makes that exponent use integer division. Think about what the formula means.\n",
    "\n",
    "If you are using a dynamic shedule you can make your own callback or even just use the ReduceLROnPlateau callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(0.1, decay_steps=1e-5, decay_rate=0.96, staircase=True)\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Tensorboard Callback\n",
    "\n",
    "Tensorboard is epic for visualizing your model so this is nice to use. To start, since you have installed tensorflow with pip (right?), you can launch tensorboard from the in the notebook using <b>%tensorboard --logdir=/full_path_to_your_logs</b> or from the command line by taking out the %\n",
    "\n",
    "Then just use the tensorboard callback:\n",
    "* <b>keras.callbacks.TensorBoard(log_dir, histogram_freq=0, embeddings_freq=0,  update_freq=\"epoch\")</b> => Lets you use TensorBoard. log_dir should be the FULL PATH to your logs. the frequencies tell it how often to log those components. update_freq decides how often it writes logs in the first place.  \n",
    "\n",
    "CURRENTLY ON THIS MACHINE SOME ERROR OCCURS THAT PREVENTS TENSORBOARD FROM PROPERLY RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/megat/Yash/yashcoding/Python Coding/jupyter/Reinforcement Learning\"\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=path, histogram_freq=0, embeddings_freq=0, update_freq=\"epoch\")]\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Customizing Training'></a>\n",
    "\n",
    "## Customizing Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic info\n",
    "\n",
    "Using fit() is nice and easy but sometimes you need that additional functionality. There are basically two different things you can do. Either control what happens in fit or just completely write a training loop from scratch. Both methods are relatively easy to use, the latter allowing you more control over what is happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing fit() and evaluate()\n",
    "\n",
    "To customize what happens in fit() you need to create a class that subclasses <b>keras.Model</b>. With that in hand all that is left to do is to override this function:\n",
    "* <b>train_step(self, data)</b> => A function that gets run in fit to train your model. This function should use the data passed into it to train the model and updates its variables (and metrics). At the very end it should return a dictionary with keys being the metric's names and values being the result. \n",
    "\n",
    "It is useful to use the following attributes and methods in your code: \n",
    "* <b>self.compiled_loss(target, pred, regularization_losses = None, sample_weight = None)</b> => Calculates the loss using the loss function passed in compile. Regularization losses are extra losses that get append like the losses from each layer and sample_weights are the importantance for each sample \n",
    "* <b>self.losses</b> => Returns a list of all the layer losses\n",
    "* <b>self.trainable_variables</b> => Returns a list containing all the variables that are trainable\n",
    "* <b>self.optimizer.apply_gradients(g_v_tuples)</b> => Using the optimizer passed in compile updates variables based on input. Input should be a list containing tuples of gradient - variable pairs. It is common to zip() tape.gradient() and trainable variables to accomplish this.  \n",
    "* <b>self.compiled_metrics</b> => An object containing all the gradients passed in compile. Useful to call update_state() and result() from it during the train_step.\n",
    "\n",
    "If for some reason you don't want to pass metrics in compile you can also just use it directly in train_step. To make sure reset_states() is called however, you need to make sure you keep those metrics in a property called metrics and don't pass in your selected metrics in compile. (Note this isn't shown below)\n",
    "\n",
    "Customizing evaluate also works in the exact same way. The only difference would be that you are not updating your trainable_variables and you are instead overriding test_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 5.8127 - mae: 1.8514\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.4687 - mae: 1.4491\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 3.3821 - mae: 1.4387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af0a049100>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    \n",
    "    def train_step(self, data):       \n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "            sample_weight = None\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        grad = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        \n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(784)\n",
    "x = keras.layers.Dense(100)(inputs)\n",
    "outputs = keras.layers.Dense(10)(x)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "model.fit(x_train, y_train, epochs=3, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a custom training loops \n",
    "\n",
    "To write your own training loop you will have to use <b>tf.GradientTape()</b>.\n",
    "\n",
    "You can handle metrics by calling the following three functions of a metric at the right time:\n",
    "* <b>metric.update_state(y_true, y_pred)</b> => Call after every batch is done\n",
    "* <b>metric.result()</b> => Get the results to display the metric when you want to (typically at the end of epoch)\n",
    "* <b>metric.reset_states()</b> => Clear the data the metric has stored (typically also at the end of epoch)\n",
    "\n",
    "It is a good idea to put the computation heavy stuff into a graph. To do this decorate a function with\n",
    "* <b>@tf.function()</b> => traces a graph of the whatever is inside, useful for speeding up computation greatly. From a simple test the code below was about 3x slower without tf.function(). An amazing speedup!\n",
    "\n",
    "Your structure should basically be:\n",
    "1. Make the model & Instantiate the loss, optimizer, and metrics & Get the data into the desired format\n",
    "2. Create a trainstep function that \n",
    "    - runs the model under a tape context\n",
    "    - updates the metrics and does gradient descent\n",
    "    - is decorated by tf.function()\n",
    "3. Iterate over the epochs\n",
    "    - iterate over batches and run trainstep/ display any tracking information\n",
    "    - display metrics and other information at end of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 0 with accuracy 0.8713\n",
      "Finished Epoch 1 with accuracy 0.9084\n",
      "Finished Epoch 2 with accuracy 0.9143\n"
     ]
    }
   ],
   "source": [
    "@tf.function()\n",
    "def train_step(x, y):\n",
    "    # Open a tape context to record operations so we can do gradient descent \n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(y, pred)\n",
    "        loss += sum(model.losses) # Add in the regularization losses from the layers as well!\n",
    "        \n",
    "    # Update metric\n",
    "    train_acc_metric.update_state(y, pred)\n",
    "\n",
    "    # Do Gradient Descent!\n",
    "    gradient = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_weights))\n",
    "\n",
    "# Make the model \n",
    "inp = keras.Input(784)\n",
    "x = layers.Dense(100)(inp)\n",
    "out = layers.Dense(10, activation=tf.nn.softmax)(x) #Our loss function is probabilistic \n",
    "model = keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Instantiate the loss, optimizer, metrics\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.05, momentum=0.04, name=\"SGD\")\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Get the data into a dataset (easier to shuffle and take batches)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size=64)\n",
    "num_steps = len(train_dataset)\n",
    "\n",
    "# Iterate over epochs and batches\n",
    "for epoch in range(3):\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        # Use the fast training step\n",
    "        train_step(x_batch_train, y_batch_train)\n",
    "        \n",
    "        # Progress check\n",
    "        print(f\"Epoch {epoch}, {step}/{num_steps} steps done\", end='\\r')\n",
    "    \n",
    "    # Get the results of the metric and display them\n",
    "    acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "    print(f\"Finished Epoch {epoch} with accuracy {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Index'> </a>\n",
    "\n",
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href = '#Top'>Back to top?</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Making a Model (Built-in)'>Section: Making a Model(Built-in)</a>\n",
    "* <b>layers.Dense(output_features, name = 'None'...)</b> => Creates your favorite classic layer with output_features neurons at the end.  \n",
    "NOTE: There is a ton more arguments you can pass here visit $ \\hspace{2mm} $\n",
    "<a href='https://keras.io/api/layers/regularizers/'>regularization</a> $\\hspace{2mm}$ \n",
    "<a href = 'https://keras.io/api/layers/activations/'>activation functions</a> $ \\hspace{2mm} $\n",
    "<a href = 'https://keras.io/api/layers/initializers/'>initializers</a>\n",
    "* <b>l.activation</b> => Returns the activation function used by the specific layer\n",
    "* <b>l.trainable_weights</b> => Returns a list of all the variables that gradient descent can be applied on\n",
    "* <b>l.non_trainable_weights</b> => Returns a list of all the variables not being trained\n",
    "* <b>l.weights</b> => Returns the concatenation of both of the lists above\n",
    "* <b>l.name</b> => Returns name of layer\n",
    "* <b>l.output</b> => Returns output of that layer\n",
    "* <b>tf.random.normal(shape, name = None)</b> => creates an array from a normal distribution with shape shape and name name\n",
    "* <b>pass a list of layers/models to the Sequential constructor </b>\n",
    "* <b>smodel.add(l)</b> => Appends l as a layer to the end\n",
    "* <b>smodel.pop()</b> => Pops layer at the end\n",
    "* <b>smodel.layers</b> => Returns list of current layers \n",
    "* <b>layers.Input(input_features, name = 'None'...)</b> => tells the model the number of features in the input\n",
    "* <b>layers.Dense(..., input_shape, ...)</b> => passing in a tuple with the input shape to the first layer in a model automatically builds it as well\n",
    "* <b>smodel.summary()</b> => Prints a summary of what the model looks like. Useful in debugging when used with <b>add()</b>. Note that the None dimension simply means that dimension can have any shape. For us this is because batch sizes can be variable\n",
    "* <b>keras.Model(inputs, outputs, name = None)</b> => creates a model using the functions used to get from input to output\n",
    "* <b>model.summary()</b> => Prints out what layers and output shapes the model has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Making a Model (Subclassing)'>Section: Making a Model (Subclassing)</a>\n",
    "* <b>tf.keras.layers.Layer</b> => Class to deal with layers\n",
    "* <b>\\_\\_init__()</b> => Method to initialize object. Should be implemented \n",
    "* <b>build()</b> => Method to initialize variables when first input is passed. Should be implemented \n",
    "* <b>call()</b> => Method to transform input to output of layer. Should be implemented\n",
    "* <b>self.add_weights(shape, initializer=None)</b> => Add trainable variable with specific shape and initializer provided.\n",
    "* <b>tf.random_normal_initializer()</b> => Initializer object. Many other exist, also possible to use string identifiers\n",
    "* <b>self.add_loss(value)</b> => Makes specific layer also have this loss. \n",
    "* <b>layer.losses</b> => Returns the loss that layer has\n",
    "* <b>self.add_metric(value, name)</b> => Makes specific layer track this metric. Name is useful to know which number is from which metric. \n",
    "* <b>metric.result()</b> => Returns the value it stored  \n",
    "\n",
    "<b>model.layers</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Training and Evaluation (Built-in)'>Section: Training and Evaluation (Built-in)</a>\n",
    "\n",
    "* <b>model.compile(optimzer, loss, metrics = None)</b> => Makes the model use specified functions when training with built-in methods\n",
    "* <b>model.compile(loss, metrics, loss_weights)</b> => instead of passing one object pass in a list for loss and metrics. If you gave the output layers a name you could also pass in a dict and have the keys be the names. If you don't want to train on one output layer you can also make its corresponding loss None. loss_weights can be a list or a dictionary containing the real valued weights.\n",
    "\n",
    "<b>tf.keras.losses.Loss</b>\n",
    "* <b>\\_\\_init(self)__</b> => accept parameters to use during call. Remember to call super\n",
    "* <b>call(self, y_true, y_pred)</b> => Use the targets and predictions to compute the model's loss\n",
    "\n",
    "<b>tf.keras.metrics.Metric</b> \n",
    "1. <b>\\_\\_init(self)</b> => Create state variables that you will need to track for your metric. You might need to use \n",
    "    * self.add_weight()\n",
    "2. <b>update_state(self, y_true, y_pred, sample_weight=None)</b> => Use the targets, predicitions, and optional weights to update the state variables keeping track of the metric\n",
    "3. <b>result(self)</b> => Use the state variables to return the final result\n",
    "4. <b>reset_states(self)</b> => Reinitialize the state of the metric\n",
    "\n",
    "1. Optimizers: <b>SGD()</b> (with or without momentum), <b>RMSprop()</b>, <b>Adam()</b>\n",
    "2. Losses: <b>MeanSquaredError()</b> - regression, <b>CosineSimilarity()</b> - regression, <b>KLDivergence()</b> - probabilistic, <b>CategoricalCrossentropy</b> - probabilistic\n",
    "3. Metrics: <b>BinaryAccuracy()</b> - 2 targets, <b>CategoricalAccruacy()</b> - one-hot encoded targets, <b>SparseCategorialAccuracy()</b> - integer targets  \n",
    "\n",
    "<b>np.array</b> \n",
    "* <b>model.fit(x_train, y_train, batch_size = None, validation_split = 0.0, epochs = 1, validation_data = None)</b> => Trains the model for specified epochs. If validation_split is given it takes away that much data to reserve for validation. If validation_data (a tuple containing x_train and y_train) is given it keeps that for validation. Note that validation_split should only be used with numpy data \n",
    "\n",
    "<b>tf.data.Dataset</b> \n",
    "* <b>tf.data.Dataset.from_tensor_slices((x_train, y_train))</b> => Take an aeeay \n",
    "* <b>dataset.shuffle(buffer_size)</b> => Creates a buffer with the specified size. When getting data it chooses a random element in the buffer and then replaces it with the next element in the dataset. (So buffer_size of 1 actually gets no shuffling). This returns a ShuffleDataset Object\n",
    "* <b>dataset.batch(batch_size)</b> => Returns BatchDataset. This contains data but in batches with size that you specify\n",
    "* <b>model.fit(dataset, epochs=1, steps_per_epoch=None, validation_data=None, validation_steps=None)</b> => Trains the model on the dataset for specified number of epochs. If you don't want to use the entire dataset per epoch you can use steps_per_epoch to specify how many batches you want to see per epoch. You can also pass in validation data and do the same thing with steps with the last two arguments\n",
    "\n",
    "<b>keras.callbacks.Callback</b>\n",
    "* <b>keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-2, patience=2, verbose=1)</b> => Implements Early stopping into your model. monitor is a string that refers to the name of what you are monitoring. min_delta is the threshold below which it doesn't count as improving. patience is how many epochs you want to give it a chance. verbose decides how much detail is in the updates it provides. \n",
    "* <b>keras.callbacks.ModelCheckpoint(filepath=\"mymodel_{epoch}\", save_best_only=True, monitor=\"val_loss\", verbose=1, save_freq=None)</b> => Saves the model. filepath is where you save the model. save_best_only and monitor make it so that the current checkpoint gets overwritten only if whatever you are monitoring has improved. verbose once again decides how much it displays. save_freq decides how many batches pass before it saves. Check out the <a href='https://www.tensorflow.org/guide/keras/train_and_evaluate#checkpointing_models'>DOCS</a> to see it in action being more organized (having its own directory, etc.)\n",
    "\n",
    "<b>Other</b>\n",
    "* <b>model.fit(..., class_weights=None, ...)</b> => Weights the data during training based on class_weights. This argument should be a dictionary with keys being the indexes of the class (0-n-1) and the values being the weight of the corresponding class.\n",
    "* <b>model.fit(..., sample_weight=None, ...)</b> => Gives each specific sample weightage based on sample_weight. This argument should be a array of numbers that has the number of elements in your dataset in its size. Each index should have the corresponding weight of the sample. \n",
    "* <b>keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=False)</b> => Creates a learning rate schedule object which calculates the rate with the following formula: inital_lr * drate ^ (steps/dsteps). Making staircase True makes that exponent use integer division. Think about what the formula means.\n",
    "\n",
    "NOTE TENSORBOARD DOESN'T CURRENTLY WORK ON THIS MACHINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Customizing Training'>Section: Customizing Training (Built-in)</a>\n",
    "\n",
    "* <b>train_step(self, data)</b> => A function that gets run in fit to train your model. This function should use the data passed into it to train the model and updates its variables (and metrics). At the very end it should return a dictionary with keys being the metric's names and values being the result. \n",
    "* <b>self.compiled_loss(target, pred, regularization_losses = None, sample_weight = None)</b> => Calculates the loss using the loss function passed in compile. Regularization losses are extra losses that get append like the losses from each layer and sample_weights are the importantance for each sample \n",
    "* <b>self.losses</b> => Returns a list of all the layer losses\n",
    "* <b>self.trainable_variables</b> => Returns a list containing all the variables that are trainable\n",
    "* <b>self.optimizer.apply_gradients(g_v_tuples)</b> => Using the optimizer passed in compile updates variables based on input. Input should be a list containing tuples of gradient - variable pairs. It is common to zip() tape.gradient() and trainable variables to accomplish this.  \n",
    "* <b>self.compiled_metrics</b> => An object containing all the gradients passed in compile. Useful to call update_state() and result() from it during the train_step.\n",
    "* <b>metric.update_state(y_true, y_pred)</b> => Call after every batch is done\n",
    "* <b>metric.result()</b> => Get the results to display the metric when you want to (typically at the end of epoch)\n",
    "* <b>metric.reset_states()</b> => Clear the data the metric has stored (typically also at the end of epoch)\n",
    "* <b>@tf.function()</b> => traces a graph of the whatever is inside, useful for speeding up computation greatly. From a simple test the code below was about 3x slower without tf.function(). An amazing speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
