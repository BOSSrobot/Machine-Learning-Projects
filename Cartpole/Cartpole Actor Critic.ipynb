{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is heavily inspired by the example on the official documentation on TensorFlow. I rewrote it to gain more of an understanding of how they would do things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections                             # Using a deque is helpful for keeping track of environment states and such\n",
    "import gym                                     # Host the environment \n",
    "import numpy as np                             # Fast linear algebra\n",
    "import tensorflow as tf                        # Fast machine learning \n",
    "import tqdm                                    # Only used once, is a progress bar (so optional)\n",
    "\n",
    "from tensorflow.keras import layers            # Makes it easier to use functional API\n",
    "from typing import Any, List, Sequence, Tuple  # Lets us use type checking by giving us names to call \n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "\n",
    "* eps\n",
    "\n",
    "#### Game: \n",
    "* env: gym.make() => env.step(), env.reset(), env.action_space.n, env.render(), env.close()\n",
    "* env_step()\n",
    "* tf_env_step()\n",
    "  \n",
    "#### Agent: \n",
    "* Model: ActorCritic()\n",
    "* Get Loss: compute_loss()\n",
    "* Get action and values: info()\n",
    "* Feedforward: __call__()\n",
    "* Property: trainable_weights\n",
    "\n",
    "#### Main loop: \n",
    "* run_episode()\n",
    "* get_expected_return()\n",
    "* train_step()\n",
    "* Training Loop\n",
    "\n",
    "### Main Logic\n",
    "\n",
    "#### Game: \n",
    "* advance a time step\n",
    "* reset to initial state\n",
    "* render time step \n",
    "\n",
    "#### Training:\n",
    "* Run an episode and collect data\n",
    "* Update a model's weights based on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game:\n",
    "    \n",
    "    def __init__(self, seed = None):\n",
    "        self.internal = gym.make(\"CartPole-v0\")\n",
    "        if seed is not None: self.internal.seed(seed)\n",
    "            \n",
    "        self.action_space = self.internal.action_space\n",
    "    \n",
    "    def helper_step(self, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        state, reward, done, _ = self.internal.step(action)\n",
    "        return state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32)\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> List[tf.Tensor]: \n",
    "        return tf.numpy_function(self.helper_step, [action], [tf.float32, tf.int32, tf.int32])\n",
    "    \n",
    "    def reset(self): return self.internal.reset()\n",
    "    def render(self): return self.internal.render()\n",
    "    def close(self): return self.internal.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_actions: int, num_hidden_units: int):\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units)\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, num_actions: int, num_hidden_units: int, \n",
    "                 loss : tf.keras.losses = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)):\n",
    "        \n",
    "        self.model = ActorCritic(num_actions, num_hidden_units)\n",
    "        self.loss = loss \n",
    "    \n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return self.model.trainable_variables\n",
    "    \n",
    "    @tf.function()\n",
    "    def compute_loss(self, action_probs: tf.Tensor, values: tf.Tensor, returns: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "        advantage = returns - values\n",
    "        action_log_probs = tf.math.log(action_probs)\n",
    "\n",
    "        actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "        critic_loss = self.loss(values, returns)\n",
    "\n",
    "        return actor_loss + critic_loss\n",
    "\n",
    "    @tf.function()\n",
    "    def __call__(self, state: tf.Tensor):\n",
    "        return self.model(state)\n",
    "    \n",
    "    def action(self, state: tf.Tensor):\n",
    "        return tf.random.categorical(self(state)[0], 1)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def run_episode(initial_state: tf.Tensor, agent: Agent, max_steps: int, env: game) -> List[tf.Tensor]:\n",
    "    \"\"\"Runs a single epsiode to collect training data.\"\"\"\n",
    "    \n",
    "    action_probs = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    \n",
    "    for t in tf.range(max_steps):\n",
    "        \n",
    "        state = tf.expand_dims(state, 0) #Get it into batch form because right now it is one dimensional\n",
    "        action_logits_t, value = agent(state)\n",
    "        \n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0] #Shape is (1,1) so use [0,0] to get single number tensor\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "        \n",
    "        values = values.write(t, tf.squeeze(value)) #Squeeze to get rid of batch aspect\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        \n",
    "        state, reward, done = env.step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "            \n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    \n",
    "    return action_probs, values, rewards # Currently one-dimensional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def get_expected_return(rewards: tf.Tensor, gamma: float) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "    \n",
    "    n = tf.shape(rewards)[0]                           \n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "    \n",
    "\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32) # We use [::-1] to order the rewards from last one to first\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    \n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum \n",
    "        discounted_sum.set_shape(discounted_sum_shape) \n",
    "        returns = returns.write(i, discounted_sum) \n",
    "        \n",
    "    returns = returns.stack()[::-1] # Now the returns are ordered from last to first so we swap em again. Shape is (n, )\n",
    "    returns = (returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns) + eps)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(initial_state: tf.Tensor, agent: Agent, optimizer: tf.keras.optimizers.Optimizer,\n",
    "               gamma: float, max_steps_per_episode: int, env: game) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        action_probs, values, rewards = run_episode(initial_state, agent, max_steps_per_episode, env)\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "        action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "        loss = agent.compute_loss(action_probs, values, returns)\n",
    "        \n",
    "    grads = tape.gradient(loss, agent.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, agent.trainable_variables))        \n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = game(seed)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "num_hidden_units = 128\n",
    "agent = Agent(num_actions, num_hidden_units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 9999: 100%|██████████████████████| 10000/10000 [10:03<00:00, 16.57it/s, episode_reward=186, running_reward=166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 9999: average reward: 166.06!\n",
      "Wall time: 10min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(), dtype = tf.float32)\n",
    "        episode_reward = int(train_step(initial_state, agent, optimizer, gamma, max_steps_per_episode, env))\n",
    "        \n",
    "        running_reward = 0.01 * episode_reward + running_reward * 0.99\n",
    "        \n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        if running_reward > reward_threshold:  \n",
    "            pass#break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = tf.constant(env.reset(), dtype = tf.float32)\n",
    "max_steps = 1000\n",
    "\n",
    "# env.reset() returns an intial observation. That is what initial_state is. \n",
    "initial_state_shape = initial_state.shape\n",
    "state = initial_state \n",
    "\n",
    "for t in tf.range(max_steps):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "\n",
    "    action_logits_t, value = agent(state)        \n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0] # This ends up having shape (1,1) so taking [0,0] corrects this\n",
    "\n",
    "    env.render()\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "\n",
    "    # End episode?\n",
    "    if tf.cast(done, tf.bool):\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
