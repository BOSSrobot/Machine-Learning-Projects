{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import numpy as np\n",
    "import time\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "import matplotlib.pyplot as plt\n",
    "from Common_functions import file_io, pygame_visuals as pv\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class board:\n",
    "    \n",
    "    def __init__(self):    \n",
    "        #Game specific Variables\n",
    "        self.turn = 1\n",
    "        self.moves_left = 9\n",
    "        self.state = np.array([0 for _ in range(9)])\n",
    "        \n",
    "    def update(self, action):\n",
    "        \n",
    "        if self.state[action] != 0:\n",
    "            return 'invalid move'\n",
    "        \n",
    "        self.state[action] = self.turn\n",
    "        self.turn = (self.turn == 1)*(-1)+(self.turn == -1)*(1)\n",
    "        self.moves_left -= 1\n",
    "        \n",
    "        if self.moves_left < 5:\n",
    "            win_cond = -3*self.turn\n",
    "                        \n",
    "            # row check\n",
    "            temp = action//3 * 3\n",
    "            if self.state[temp] + self.state[temp+1] + self.state[temp+2] == win_cond:\n",
    "                return f\"{-1*self.turn} won\"\n",
    "            \n",
    "            # col check\n",
    "            if self.state[action] + self.state[(action+3)%9] + self.state[(action+6)%9] == win_cond:\n",
    "                return f\"{-1*self.turn} won\"\n",
    "            \n",
    "            #diag check\n",
    "            if action%2 == 0:\n",
    "                if self.state[0] + self.state[4] + self.state[8] == win_cond or self.state[2] + self.state[4] + self.state[6] == win_cond:\n",
    "                    return f\"{-1*self.turn} won\"\n",
    "                            \n",
    "            if self.moves_left == 0:\n",
    "                return 'tie'\n",
    "        \n",
    "        return 'valid move'\n",
    "\n",
    "    def reset(self):\n",
    "        self.turn = 1\n",
    "        self.moves_left = 9\n",
    "        self.state = np.array([0 for _ in range(9)])\n",
    "\n",
    "class game:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #RL Variables\n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "        self.score = 0\n",
    "        \n",
    "        #Tic Tac Toe Board\n",
    "        self.gameboard = board()\n",
    "        \n",
    "    def update(self, action):                    \n",
    "        result = self.gameboard.update(action)\n",
    "        \n",
    "        if result == 'invalid move':\n",
    "            self.reward = -1\n",
    "            self.terminal = True\n",
    "            \n",
    "        elif result == 'valid move':\n",
    "            self.reward = 0\n",
    "            self.terminal = False\n",
    "            \n",
    "        elif result == '1 won':\n",
    "            self.reward = 1 * -1 * self.gameboard.turn\n",
    "            self.terminal = True\n",
    "            \n",
    "        elif result == '-1 won':\n",
    "            self.reward = -1 * -1 * self.gameboard.turn\n",
    "            self.terminal = True\n",
    "        \n",
    "        elif result == 'tie':\n",
    "            self.reward = 0\n",
    "            self.terminal = True\n",
    "        \n",
    "        self.score += self.reward\n",
    "        \n",
    "    def reset(self):\n",
    "        self.gameboard.reset()\n",
    "        \n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "        self.score = 0\n",
    "    \n",
    "    def return_state_features(self):\n",
    "        return self.gameboard.state.reshape(9,1)*self.gameboard.turn\n",
    "    \n",
    "class illustrator:\n",
    "    \n",
    "    def __init__(self, host_game):\n",
    "        \n",
    "        self.host = host_game\n",
    "    \n",
    "    def draw(self):\n",
    "        def convert(n):\n",
    "            return \"X\"*(n==1) + \" \"*(n==0) + \"O\"*(n==-1)\n",
    "        temp = [[convert(int(self.host.gameboard.state[3*j+i])) for i in range(3)] for j in range(3)]\n",
    "        \n",
    "        print(f\"{temp[0][0]}|{temp[0][1]}|{temp[0][2]}\\n\")\n",
    "        print(\"-----\")\n",
    "        print(f\"{temp[1][0]}|{temp[1][1]}|{temp[1][2]}\\n\")\n",
    "        print(\"-----\")\n",
    "        print(f\"{temp[2][0]}|{temp[2][1]}|{temp[2][2]}\\n\\n\")\n",
    "\n",
    "def draw(st):\n",
    "    def convert(n):\n",
    "        return \"X\"*(n==1) + \" \"*(n==0) + \"O\"*(n==-1)\n",
    "    temp = [[convert(int(st[3*j+i])) for i in range(3)] for j in range(3)]\n",
    "        \n",
    "    print(f\"{temp[0][0]}|{temp[0][1]}|{temp[0][2]}\\n\")\n",
    "    print(\"-----\")\n",
    "    print(f\"{temp[1][0]}|{temp[1][1]}|{temp[1][2]}\\n\")\n",
    "    print(\"-----\")\n",
    "    print(f\"{temp[2][0]}|{temp[2][1]}|{temp[2][2]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def use(z):\n",
    "        return 0.5 * (1 + np.tanh(0.5 * z))\n",
    "\n",
    "    def prime(z):\n",
    "        return sigmoid.use(z)*(1-sigmoid.use(z)) \n",
    "\n",
    "class tanh:\n",
    "    def use(z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def prime(z):\n",
    "        return 1-np.tanh(z)**2\n",
    "\n",
    "class network:\n",
    "    \n",
    "    def __init__(self, layers, afunc = sigmoid, rho = 0, lamda = 0):\n",
    "        # Gradient descent variables\n",
    "        self.bias = [np.random.randn(x, 1) for x in layers[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        self.b_vel = [np.zeros((x,1)) for x in layers[1:]]\n",
    "        self.w_vel = [np.zeros((y, x)) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        self.rho = rho\n",
    "        self.lamda = lamda\n",
    "        \n",
    "        self.size = len(layers)\n",
    "        self.afunc = afunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            a = self.afunc.use(np.matmul(w, a) + b)\n",
    "        return a \n",
    "    \n",
    "    def SGD(self, train, eta, mini_batch_size, epochs, monitor=None):\n",
    "        # Train is a tuple containing two matricies of size (x,n) where \n",
    "        # x is the number of input neurons and n is the number of samples\n",
    "        # To avoid shuffling the original data pass in a copy of the data\n",
    "        \n",
    "        # Define all the things we need\n",
    "        \n",
    "        train_x, train_y = train\n",
    "        \n",
    "        acc, cost = [], []\n",
    "        \n",
    "        num_train = train_y.shape[1]\n",
    "        if monitor:\n",
    "            num_test = monitor[0].shape[1]\n",
    "            \n",
    "        weight_update = []\n",
    "        \n",
    "        # Train the network\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            # Shuffle the data in unison\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(train_x.transpose())\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(train_y.transpose())\n",
    "            \n",
    "            #Track weights:\n",
    "            wu = 0\n",
    "            # Go through each mini batch \n",
    "            for i in range(0, num_train, mini_batch_size):\n",
    "                wu += self.update_mini_batch(eta, train_x[:, i:i+mini_batch_size], train_y[:, i:i+mini_batch_size], mini_batch_size)\n",
    "            \n",
    "            weight_update.append(wu)\n",
    "            \n",
    "            # Monitor progress, if monitor contains something then evaluate on it\n",
    "            if monitor:\n",
    "                a, c = self.evaluate(monitor, num_test)\n",
    "                acc.append(a)\n",
    "                cost.append(c)\n",
    "                print(\"Epoch {} done! Cost: {}. Accuracy {} / {}\".format(e+1, c, a, num_test))\n",
    "            else:\n",
    "                print(\"Epoch {} done!\".format(e+1))\n",
    "        \n",
    "        return acc, cost, weight_update  \n",
    "    \n",
    "    def evaluate(self, data, num_test):\n",
    "        # Get the inputs and outputs\n",
    "        test_x, test_y = data\n",
    "        \n",
    "        # Predict\n",
    "        pred = self.feedforward(test_x)\n",
    "        \n",
    "        # Find accuracy\n",
    "        acc = np.sum(test_y.T.nonzero()[1]==pred.argmax(axis=0))\n",
    "        \n",
    "        # Use C = 1/N *( sum_x 0.5 * ||a^L-y||^2 + 0.5*sum||w|| )\n",
    "        cost= 1/num_test*(0.5*np.linalg.norm(pred - test_y)**2 + 0.5*self.lamda*sum(np.linalg.norm(w)**2 for w in self.weights))\n",
    "        \n",
    "        return acc, cost        \n",
    "        \n",
    "    def update_mini_batch(self, eta, x, y, mini_batch_size):\n",
    "        gradient_bias, gradient_weights = self.backprop(x,y)\n",
    "        \n",
    "        # v = pv + (1-p)gradientC\n",
    "        self.b_vel = [self.rho*vb + grad_b for vb, grad_b in zip(self.b_vel, gradient_bias)]\n",
    "        self.w_vel = [self.rho*vw + (grad_w+self.lamda*w) for vw, grad_w, w in zip(self.w_vel, gradient_weights, self.weights)] \n",
    "        \n",
    "        # x = x - eta/batch size *v\n",
    "        self.bias = [b - (eta/mini_batch_size)*vb for b, vb in zip(self.bias, self.b_vel)]\n",
    "        self.weights = [w - (eta/mini_batch_size)*vw for w, vw in zip(self.weights, self.w_vel)] \n",
    "        \n",
    "        return sum(np.linalg.norm(w)**2 for w in gradient_weights) \n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        gradient_bias, gradient_weights = [], []\n",
    "        \n",
    "        act = x\n",
    "        activations = [act]\n",
    "        z_values = []\n",
    "    \n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.matmul(w, act) + b\n",
    "            z_values.append(z)\n",
    "            \n",
    "            act = self.afunc.use(z)\n",
    "            activations.append(act)\n",
    "            \n",
    "        delta = (activations[-1]-y)*self.afunc.prime(z_values[-1])\n",
    "        #gradient_bias.append(delta)\n",
    "        gradient_bias.append(np.sum(delta, axis = 1).reshape(delta.shape[0], 1))\n",
    "        gradient_weights.append(np.matmul(delta, activations[-2].transpose()))  \n",
    " \n",
    "        for i in range(2, self.size):\n",
    "            delta = np.matmul(self.weights[-i+1].transpose(), delta) * self.afunc.prime(z_values[-i])\n",
    "            #gradient_bias.insert(0, delta)\n",
    "            gradient_bias.insert(0, np.sum(delta, axis = 1).reshape(delta.shape[0], 1))\n",
    "            gradient_weights.insert(0, np.matmul(delta, activations[-i-1].transpose()))\n",
    "    \n",
    "        return gradient_bias, gradient_weights\n",
    "    \n",
    "    def make_copy_for_predicition(self):\n",
    "        net = network([1,2], self.afunc, self.rho, self.lamda)\n",
    "        net.size = self.size\n",
    "        \n",
    "        net.bias = [el.copy() for el in self.bias]\n",
    "        net.weights = [el.copy() for el in self.weights]\n",
    "        \n",
    "        return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json \n",
    "\n",
    "def save(agent, filename = 'agent'):\n",
    "    \n",
    "    data = {\"net size\": agent.net.size,\n",
    "            \"net weights\": [w.tolist() for w in agent.net.weights],\n",
    "            \"net bias\": [b.tolist() for b in agent.net.bias],\n",
    "            \"net w_vel\": [w.tolist() for w in agent.net.w_vel],\n",
    "            \"net b_vel\": [b.tolist() for b in agent.net.b_vel],\n",
    "            \"net rho\": agent.net.rho,\n",
    "            \"net lamda\": agent.net.lamda,\n",
    "            \"gamma\": agent.gamma,\n",
    "            \"alpha\": agent.alpha,\n",
    "            \"epsilon\": agent.epsilon,\n",
    "            \"min_epsilon\": agent.min_epsilon}\n",
    "    \n",
    "    f = open(filename, \"w\")\n",
    "    json.dump(data, f)\n",
    "    f.close()\n",
    "\n",
    "def load(filename = 'agent'):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an instance of func_sarsa.\"\"\"\n",
    "    \n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    agent = func_sarsa(data['gamma'], data['alpha'], data['epsilon'], data['min_epsilon'], \n",
    "                       (1,1), data['net rho'], data['net lamda'])\n",
    "    \n",
    "    afunc = tanh\n",
    "    agent.net.afunc = afunc\n",
    "    agent.net.size = data['net size']\n",
    "    \n",
    "    agent.net.weights = [np.array(w) for w in data[\"net weights\"]]\n",
    "    agent.net.bias = [np.array(b) for b in data[\"net bias\"]]\n",
    "    agent.net.w_vel = [np.array(w) for w in data[\"net w_vel\"]]\n",
    "    agent.net.b_vel = [np.array(b) for b in data[\"net b_vel\"]]\n",
    "    agent.target_net = agent.net.make_copy_for_predicition()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class func_sarsa():\n",
    "    \"\"\"\n",
    "    Uses Deep Q Learning to update parameters. \n",
    "    States are represented as vectors, actions are represented with natural numbers.\n",
    "    \n",
    "    Game Loop should look like:\n",
    "    Get first action (baction, bstate)\n",
    "    Repeat until terminal state\n",
    "      take the action   (reward)\n",
    "      update env (nstate)\n",
    "      get action    (naction)\n",
    "      update agent (bstate, baction <-- nstate, naction)\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma, alpha, epsilon, min_epsilon, network_layers, rho, lamda, memory_size=1000):\n",
    "        \"\"\"\n",
    "        Initalizes agent. \n",
    "        \n",
    "        Params: \n",
    "        gamma:       disount factor. How much it cares about the future. [0,1]\n",
    "        alpha:       step size rate. How much it updates current prediction towards new one. [0,1]\n",
    "        epsilon:     exploration rate. How often it makes random moves. [0,1]\n",
    "        min_epsilon: lower bound on epsilon. [0,1]\n",
    "        lamda:       weight of n-step return. [0,1]\n",
    "        ...\n",
    "        \"\"\"\n",
    "        #Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        \n",
    "        #Field variables\n",
    "        self.net = network(network_layers, afunc=tanh, rho=rho, lamda = lamda)\n",
    "        self.target_net = self.net.make_copy_for_predicition()\n",
    "        self.not_min_epsilon = True\n",
    "        \n",
    "        self.memory_size = memory_size\n",
    "        self.experience = deque()\n",
    "        \n",
    "    def q_values(self, state_features):\n",
    "        return self.net.feedforward(state_features)\n",
    "        \n",
    "    def choose_action(self, state_features):\n",
    "        \n",
    "        \n",
    "        indicator = np.nonzero(state_features == 0)[0]\n",
    "        \n",
    "        if self.not_min_epsilon: \n",
    "            self.epsilon *= 0.99999\n",
    "            if self.epsilon <= self.min_epsilon:\n",
    "                self.not_min_epsilon = False\n",
    "        \n",
    "        if np.random.rand() < self.epsilon: \n",
    "            np.random.shuffle(indicator)\n",
    "            action = int(indicator[0])\n",
    "            return action\n",
    "        else:            \n",
    "            values = self.net.feedforward(state_features)\n",
    "            # Get vals in values that are allowed. Find index where it resides. Turn into int\n",
    "            return int(np.where(values == values[indicator].max())[0])\n",
    "        \n",
    "    def add_normal(self, bfeature, baction, reward, state_features, action):\n",
    "        \n",
    "        target = self.q_values(bfeature)\n",
    "        target[baction] = reward + self.gamma * self.target_net.feedforward(state_features).max()\n",
    "    \n",
    "        self.add_experience(bfeature.reshape(9), target.reshape(9))\n",
    "\n",
    "                \n",
    "    def add_final(self, bfeature, baction, reward):\n",
    "        \n",
    "        target = self.q_values(bfeature)\n",
    "        target[baction] = reward\n",
    "        \n",
    "        self.add_experience(bfeature.reshape(9), target.reshape(9))\n",
    "        \n",
    "    def add_experience(self, pred, target):\n",
    "         \n",
    "        if len(self.experience) < self.memory_size:\n",
    "            self.experience.append([pred, target])\n",
    "        else:\n",
    "            self.experience.popleft()\n",
    "            self.experience.append([pred, target])\n",
    "            \n",
    "    def sample(self, mb_size):\n",
    "        \n",
    "        sample_size = min(mb_size, len(self.experience))\n",
    "        trainingset = random.sample(self.experience, sample_size)\n",
    "        pred_tuple, target_tuple = zip(*trainingset)\n",
    "        \n",
    "        return np.array(pred_tuple).transpose(), np.array(target_tuple).transpose()\n",
    "        \n",
    "    def update(self, mb_size):\n",
    "        \n",
    "        pred, target = self.sample(mb_size)\n",
    "        w_update = self.net.update_mini_batch(self.alpha, pred, target, mb_size)\n",
    "        \n",
    "        return w_update\n",
    "    \n",
    "    def unfreeze_target_net(self):\n",
    "        self.target_net = self.net.make_copy_for_predicition()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsiodes done:  1000\n",
      "Epsiodes done:  2000\n",
      "Epsiodes done:  3000\n",
      "Epsiodes done:  4000\n",
      "Epsiodes done:  5000\n",
      "Epsiodes done:  6000\n",
      "Epsiodes done:  7000\n",
      "Epsiodes done:  8000\n",
      "Epsiodes done:  9000\n",
      "Epsiodes done:  10000\n",
      "Epsiodes done:  1000\n",
      "Epsiodes done:  2000\n",
      "Epsiodes done:  3000\n",
      "Epsiodes done:  4000\n",
      "Epsiodes done:  5000\n",
      "Epsiodes done:  6000\n",
      "Epsiodes done:  7000\n",
      "Epsiodes done:  8000\n",
      "Epsiodes done:  9000\n",
      "Epsiodes done:  10000\n",
      "20.138041341887263\n",
      "28.071245209007476\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = [0.09,0.009]\n",
    "n_test = len(test)\n",
    "\n",
    "env = game()\n",
    "info_package = [[] for _ in range(n_test)]\n",
    "\n",
    "for j in range(n_test):\n",
    "    agent = func_sarsa(gamma=0.95, alpha = test[j], epsilon = 0.999, \n",
    "                   min_epsilon= 0.10, network_layers = (9,18,12,9), \n",
    "                   rho=0, lamda=0.1, memory_size=100)\n",
    "\n",
    "\n",
    "    num_training_episodes = 10000\n",
    "    iterations_to_unfreeze = 100\n",
    "\n",
    "    for i in range(1, num_training_episodes+1):\n",
    "        if (i*10%num_training_episodes) == 0:\n",
    "            print(\"Epsiodes done: \", i)\n",
    "\n",
    "        if (i%iterations_to_unfreeze == 0):\n",
    "            agent.unfreeze_target_net()\n",
    "        #Actual Training Logic\n",
    "\n",
    "        s1 = env.return_state_features()\n",
    "        a1 = agent.choose_action(s1)    \n",
    "        env.update(a1)\n",
    "        r1 = env.reward\n",
    "\n",
    "        s2 = env.return_state_features()\n",
    "        a2 = agent.choose_action(s2)\n",
    "        env.update(a2)\n",
    "        r2 = env.reward\n",
    "\n",
    "        remember = deque([(s1,a1,r1), (s2,a2,r2)])\n",
    "\n",
    "        steps = 0\n",
    "        while not env.terminal:\n",
    "\n",
    "            s = env.return_state_features()\n",
    "            a = agent.choose_action(s)\n",
    "\n",
    "            env.update(a)\n",
    "            r = env.reward\n",
    "\n",
    "            info = remember.popleft() #MEMORY UPDATE => popleft()\n",
    "            #print(info[0].reshape(9), info[1], info[2], s.reshape(9), a)        AGENT UPDATE  => remember[0], s, a\n",
    "            agent.add_normal(info[0], info[1], info[2], s, a)\n",
    "            remember.append((s,a,r)) #MEMORY UPDATE => append(s & a & r)\n",
    "\n",
    "            info_package[j].append(agent.update(mb_size = 20))\n",
    "\n",
    "        #AGENT UPDATE  => remember[1] (recent S,A,R), SET VALUE TO 0,\n",
    "        agent.add_final(remember[1][0], remember[1][1], remember[1][2])\n",
    "\n",
    "        #AGENT UPDATE  => remember[0] (second recent S,A,R), SET VALUE TO 0,  ****REWARD *= -1\n",
    "        agent.add_final(remember[0][0], remember[0][1], remember[1][2]*-1)        \n",
    "        env.reset()   \n",
    "\n",
    "for i in range(n_test):\n",
    "    print(np.array(info_package[i]).mean())\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.0,0.4,0.9,0.4])\n",
    "\n",
    "for i in range(n_test):\n",
    "    axes.plot(np.arange(len(info_package[i])), info_package[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load('first decent one')\n",
    "agent.memory_size = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = func_sarsa(gamma=0.999, alpha = 0.05, epsilon = 0.999, \n",
    "                   min_epsilon= 0.15, network_layers = (9,18,12,9), \n",
    "                   rho=0.05, lamda=0.1, memory_size=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2760ed7afb42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mremember\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#MEMORY UPDATE => append(s & a & r)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0minfo_package\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m#AGENT UPDATE  => remember[1] (recent S,A,R), SET VALUE TO 0,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-375fd5c42a43>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, mb_size)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mw_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-375fd5c42a43>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, mb_size)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mpred_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrainingset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = game()\n",
    "info_package = []\n",
    "\n",
    "num_training_episodes = 1500000\n",
    "iterations_to_unfreeze = 10000\n",
    "\n",
    "for i in range(1, num_training_episodes+1):\n",
    "    \n",
    "    if (i*100%num_training_episodes) == 0:\n",
    "        print(\"Epsiodes done: \", i)\n",
    "    \n",
    "    if (i%iterations_to_unfreeze == 0):\n",
    "        agent.unfreeze_target_net()\n",
    "    #Actual Training Logic\n",
    "    \n",
    "    s1 = env.return_state_features()\n",
    "    a1 = agent.choose_action(s1)    \n",
    "    env.update(a1)\n",
    "    r1 = env.reward\n",
    "        \n",
    "    s2 = env.return_state_features()\n",
    "    a2 = agent.choose_action(s2)\n",
    "    env.update(a2)\n",
    "    r2 = env.reward\n",
    "    \n",
    "    remember = deque([(s1,a1,r1), (s2,a2,r2)])\n",
    "        \n",
    "    steps = 0\n",
    "    while not env.terminal:\n",
    "        \n",
    "        s = env.return_state_features()\n",
    "        a = agent.choose_action(s)\n",
    "        \n",
    "        env.update(a)\n",
    "        r = env.reward\n",
    "        \n",
    "        info = remember.popleft() #MEMORY UPDATE => popleft()\n",
    "        #print(info[0].reshape(9), info[1], info[2], s.reshape(9), a)        AGENT UPDATE  => remember[0], s, a\n",
    "        agent.add_normal(info[0], info[1], info[2], s, a)\n",
    "        remember.append((s,a,r)) #MEMORY UPDATE => append(s & a & r)\n",
    "        \n",
    "        info_package.append(agent.update(mb_size = 20))\n",
    "        \n",
    "    #AGENT UPDATE  => remember[1] (recent S,A,R), SET VALUE TO 0,\n",
    "    agent.add_final(remember[1][0], remember[1][1], remember[1][2])\n",
    "    \n",
    "    #AGENT UPDATE  => remember[0] (second recent S,A,R), SET VALUE TO 0,  ****REWARD *= -1\n",
    "    agent.add_final(remember[0][0], remember[0][1], remember[1][2]*-1)        \n",
    "    env.reset()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ff7dc887a262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_package\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_package\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1984\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1985\u001b[0m         \"\"\"\n\u001b[1;32m-> 1986\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \"\"\"\n\u001b[0;32m   1010\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0.0,0.4,0.9,0.4])\n",
    "\n",
    "axes.plot(np.arange(len(info_package)), info_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 0.685301315097356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68530132],\n",
       "       [-0.82650848],\n",
       "       [ 0.30707531],\n",
       "       [-0.95338966],\n",
       "       [-0.93318252],\n",
       "       [ 0.07519327],\n",
       "       [-0.80655675],\n",
       "       [-0.89879017],\n",
       "       [ 0.51531648]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1,1,0,-1,-1,0,0,0,0])\n",
    "q = agent.q_values(t.reshape(9,1))\n",
    "print(q.argmin(), q.argmax(), q.max())\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "X| | \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "8\n",
      "X| | \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "2\n",
      "X| |X\n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "5\n",
      "X| |X\n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "6\n",
      "X| |X\n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "-----\n",
      "X| |O\n",
      "\n",
      "\n",
      "1\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "-----\n",
      "X| |O\n",
      "\n",
      "\n",
      "7\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      " | |O\n",
      "\n",
      "-----\n",
      "X|X|O\n",
      "\n",
      "\n",
      "4\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      " |O|O\n",
      "\n",
      "-----\n",
      "X|X|O\n",
      "\n",
      "\n",
      "3\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "-----\n",
      "X|X|O\n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "orig_epsilon = agent.epsilon\n",
    "agent.epsilon = 0\n",
    "\n",
    "i = illustrator(env) \n",
    "\n",
    "steps = 0\n",
    "while not env.terminal:\n",
    "\n",
    "    s = env.return_state_features()\n",
    "    a = agent.choose_action(s)\n",
    "\n",
    "    env.update(a)\n",
    "    \n",
    "    print(a)    \n",
    "    i.draw()\n",
    "\n",
    "print(env.reward)\n",
    "env.reset()   \n",
    "\n",
    "agent.epsilon = orig_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.return_state_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.79633079]\n",
      "Epoch 1 done!\n",
      "[-0.79636816]\n",
      "[[-0.75387723]\n",
      " [-0.80141781]\n",
      " [-0.7501967 ]\n",
      " [-0.81319089]\n",
      " [ 0.29332599]\n",
      " [-0.79636816]\n",
      " [-0.94683971]\n",
      " [-0.8069871 ]\n",
      " [-0.9817044 ]]\n"
     ]
    }
   ],
   "source": [
    "bfeature = np.array([-1,0,1,0,-1,1,0,0,0]).reshape(9,1)\n",
    "action = 5\n",
    "reward = -1\n",
    "\n",
    "print(agent.q_values(bfeature)[action])\n",
    "\n",
    "target = agent.q_values(bfeature)\n",
    "target[action] = reward\n",
    "        \n",
    "train = [bfeature, target]\n",
    "agent.net.SGD(train, 0.0009, 1, 1)\n",
    "\n",
    "print(agent.q_values(bfeature)[action])\n",
    "print(agent.q_values(bfeature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      " | |X\n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "where do you want to go?4\n",
      " | |X\n",
      "\n",
      "-----\n",
      " |O| \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "0\n",
      "X| |X\n",
      "\n",
      "-----\n",
      " |O| \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "where do you want to go?1\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      " |O| \n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "5\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      " |O|X\n",
      "\n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "where do you want to go?7\n",
      "X|O|X\n",
      "\n",
      "-----\n",
      " |O|X\n",
      "\n",
      "-----\n",
      " |O| \n",
      "\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "orig_epsilon = agent.epsilon\n",
    "agent.epsilon = 0\n",
    "\n",
    "i = illustrator(env) \n",
    "\n",
    "steps = 0\n",
    "while not env.terminal:\n",
    "\n",
    "    s = env.return_state_features()\n",
    "    a = agent.choose_action(s)\n",
    "\n",
    "    env.update(a)\n",
    "    \n",
    "    print(a)    \n",
    "    i.draw()\n",
    "    \n",
    "    if not env.terminal:\n",
    "        a = int(input(\"where do you want to go?\"))\n",
    "        env.update(a)\n",
    "        i.draw()\n",
    "    \n",
    "print(env.reward)\n",
    "env.reset()   \n",
    "\n",
    "agent.epsilon = orig_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
