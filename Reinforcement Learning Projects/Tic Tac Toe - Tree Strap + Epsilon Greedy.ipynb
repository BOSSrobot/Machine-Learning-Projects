{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been a while since I have wanted to start this project, yet every time I have delayed writing it up for some reason or another. It is finally time to figure this bad boy out once and for all. I still need to learn theory but we can start by implementing the MC sampling algorithm and see how it does in this simple setting. SIKE -> after watching lecture 10 seems like tree strap is a much easier beginning. We will start with the surefire minimax and then add in treestrap on top of a minimax search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add feature for input type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "from __future__ import annotations\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pygame\n",
    "from pygame.locals import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan \n",
    "\n",
    "Environment: \n",
    "- step(a, player[ENUM]) => state, reward, done, mask \n",
    "- reset() => state\n",
    "- get_internal() => internal_dict: state, reward, terminal, mask, moves_left \n",
    "- set_internal(internal_dict)\n",
    "- render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(Enum):\n",
    "    X = 1.0\n",
    "    O = -1.0\n",
    "    \n",
    "    def other(player : Player) -> Player: \n",
    "        if player == Player.X:\n",
    "            return Player.O\n",
    "        else: \n",
    "            return Player.X\n",
    "        \n",
    "turn_to_player = {0 : Player.X, 1: Player.O}\n",
    "\n",
    "class BG: \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def step(self, a : int, player : Player) -> List[np.array, int, bool, np.array]:\n",
    "        \n",
    "        if self.state[a] != 0: \n",
    "            raise ValueError('invalid move!', self.get_internal())\n",
    "            \n",
    "        self.state[a] = player.value\n",
    "        self.game_state[0][a + (9 if player == Player.O else 0)] = 1\n",
    "        self.mask[a] = 0\n",
    "        self.moves_left -= 1\n",
    "        self.actions_taken.append(a)\n",
    "        \n",
    "        if self.moves_left < 5: \n",
    "            \n",
    "            temp = a//3 * 3\n",
    "            if abs(self.state[temp] + self.state[temp+1] + self.state[temp+2]) == 3:\n",
    "                self.done = True\n",
    "                self.reward = 1 * player.value\n",
    "\n",
    "            \n",
    "            elif abs(self.state[a] + self.state[(a+3)%9] + self.state[(a+6)%9]) == 3:\n",
    "                self.done = True\n",
    "                self.reward = 1 * player.value\n",
    "\n",
    "            elif a%2 == 0 and \\\n",
    "                (abs(self.state[0] + self.state[4] + self.state[8]) == 3 \\\n",
    "                or abs(self.state[2] + self.state[4] + self.state[6]) == 3):\n",
    "                self.done = True\n",
    "                self.reward = 1 * player.value\n",
    "                            \n",
    "            elif self.moves_left == 0:\n",
    "                self.done = True\n",
    "                self.reward = 0  \n",
    "                \n",
    "        return self.game_state, self.reward, self.done, self.mask\n",
    "    \n",
    "    def undo(self): \n",
    "        if len(self.actions_taken) == 0:\n",
    "            raise ValueError('no actions taken yet', self.get_internal())\n",
    "        \n",
    "        recent_action = self.actions_taken.pop()\n",
    "        self.game_state[0][[recent_action, recent_action + 9]] = 0\n",
    "        self.state[recent_action] = 0\n",
    "        self.mask[recent_action] = 1\n",
    "        self.moves_left += 1\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "                \n",
    "    def reset(self) -> List[np.array, int, bool, np.array]:\n",
    "        self.set_internal({})\n",
    "        return self.game_state, self.reward, self.done, self.mask\n",
    "    \n",
    "    def calculate_turn(self) -> int: \n",
    "        return 1 - self.moves_left % 2 \n",
    "    \n",
    "    def get_obs(self) -> List[np.array, int, bool, np.array]:\n",
    "        return self.game_state, self.reward, self.done, self.mask\n",
    "        \n",
    "    def get_internal(self) -> Dict:\n",
    "        return {'game_state' : self.game_state,\n",
    "                'state' : self.state, \n",
    "                'mask' : self.mask, \n",
    "                'reward' : self.reward, \n",
    "                'done' : self.done, \n",
    "                'moves_left' : self.moves_left,\n",
    "                'actions_taken' : self.actions_taken}\n",
    "    \n",
    "    def set_internal(self, internal_dict : Dict):\n",
    "        self.game_state = internal_dict.get('game_state', np.zeros((1, 18)))\n",
    "        self.state = internal_dict.get('state', np.zeros(9))\n",
    "        self.mask = internal_dict.get('mask', np.ones(9))\n",
    "        \n",
    "        self.reward = internal_dict.get('reward', 0)\n",
    "        self.done = internal_dict.get('done', False)\n",
    "        \n",
    "        self.moves_left = internal_dict.get('moves_left', 9)\n",
    "        self.actions_taken = internal_dict.get('actions_taken', [])\n",
    "        \n",
    "    def render(self):\n",
    "        conv = {-1 : 'O', 0 : ' ', 1 : 'X'}\n",
    "        temp = self.state.reshape(3, 3).astype(np.int)\n",
    "        \n",
    "        lines = ['|'.join(conv[v] for v in row) for row in temp]\n",
    "        board = '\\n-----\\n'.join(lines)\n",
    "        print(board, end=\"\\n\\n\")\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human:\n",
    "    \n",
    "    def __init__(self, player): \n",
    "        self.player = player\n",
    "        \n",
    "    def choose_action(self, env):\n",
    "        return int(input(\"Take an action please: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBot: \n",
    "    def __init__(self, player):\n",
    "        self.player = player\n",
    "    \n",
    "    def evaluate(self, env):\n",
    "        return \"IS A RANDOM BOT\"\n",
    "    \n",
    "    def choose_action(self, env):\n",
    "        return (np.random.uniform(low = 0.5, high = 1, size=[9]) * env.mask).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(18,), name=\"inp\")\n",
    "x = layers.Dense(50, activation=\"relu\", name=\"layer_1\")(inputs)\n",
    "x = layers.Dense(50, activation=\"relu\", name=\"layer_2\")(x)\n",
    "x = layers.Dense(25, activation=\"relu\", name=\"layer_3\")(x)\n",
    "outputs = layers.Dense(1, activation=\"tanh\", name=\"value\")(x)\n",
    "value_network = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "value_network = keras.models.load_model(\"epsilon_good_net5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimax:\n",
    "    \n",
    "    def __init__(self, player, max_depth = 3, debug = False, training = True, epsilon=0.9, decay=0.9999, min_epsilon=0.15, explore = True):\n",
    "        self.player = player \n",
    "        self.value = 0\n",
    "        self.e = BG()\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        self.loss_fn = keras.losses.MSE\n",
    "        self.optimizer = keras.optimizers.SGD(learning_rate=0.0012, momentum=0.0003, name=\"SGD\")\n",
    "        self.loss_h = []\n",
    "        self.episode_loss = []\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.explore = explore\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.training = training\n",
    "        \n",
    "    def choose_action(self, env):\n",
    "        \n",
    "        self.e.set_internal(env.get_internal())        \n",
    "        \n",
    "        if self.player == Player.X: \n",
    "            a = self.max_expand(self.max_depth)[0]\n",
    "        else: \n",
    "            a = self.min_expand(self.max_depth)[0]\n",
    "        \n",
    "        if self.training: \n",
    "            r = tf.expand_dims(tf.stack(self.rewards), axis = 1)\n",
    "            \n",
    "            l = self.update(tf.stack(self.states), r)\n",
    "            self.states, self.rewards = [], []\n",
    "            self.episode_loss.append(tf.math.reduce_mean(l))\n",
    "            \n",
    "        if self.explore:      \n",
    "            if self.epsilon > self.min_epsilon:\n",
    "                self.epsilon*=self.decay\n",
    "\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                p = np.random.uniform(size=(9))\n",
    "                p*= env.mask\n",
    "                a = int(p.argmax())\n",
    "        \n",
    "        return a\n",
    "        \n",
    "    def evaluate(self, s): \n",
    "        return tf.squeeze(value_network(s))\n",
    "            \n",
    "    def max_expand(self, depth):\n",
    "        \n",
    "        if depth == 0: return -1, self.evaluate(self.e.game_state)\n",
    "        \n",
    "        info = []\n",
    "        \n",
    "        for a in self.e.mask.nonzero()[0]:\n",
    "            s, r, d, m = self.e.step(a, turn_to_player[self.e.calculate_turn()])\n",
    "            \n",
    "            if d: info.append((a, r))  \n",
    "            else: info.append((a, self.min_expand(depth-1)[1])) \n",
    "                \n",
    "            self.e.undo()\n",
    "            \n",
    "        action, max_reward = max(info, key = lambda x : x[1])\n",
    "        possibilities = [(x, y) for x,y in info if y == max_reward]\n",
    "        self.add_update(self.e.game_state, max_reward)\n",
    "        \n",
    "        if self.debug and depth == self.max_depth:\n",
    "            print([(x, float(y)) for x, y in info])\n",
    "\n",
    "        return action, max_reward\n",
    "    \n",
    "    def min_expand(self, depth):\n",
    "        \n",
    "        if depth == 0: return -1, self.evaluate(self.e.game_state)\n",
    "        \n",
    "        info = []\n",
    "        \n",
    "        for a in self.e.mask.nonzero()[0]:\n",
    "            s, r, d, m = self.e.step(a, turn_to_player[self.e.calculate_turn()])\n",
    "\n",
    "            if d: info.append((a, r))                \n",
    "            else: info.append((a, self.max_expand(depth-1)[1]))\n",
    "                \n",
    "            self.e.undo()\n",
    "            \n",
    "          \n",
    "        action, min_reward = min(info, key = lambda x : x[1])\n",
    "        self.add_update(self.e.game_state, min_reward)\n",
    "\n",
    "        if self.debug and depth == self.max_depth:\n",
    "            print([(x, float(y)) for x, y in info])\n",
    "        return action, min_reward\n",
    "    \n",
    "    @tf.function()\n",
    "    def update(self, s, r):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            r_pred = value_network(s)\n",
    "            loss = tf.norm(self.loss_fn(r, r_pred))\n",
    "            \n",
    "        gradient = tape.gradient(loss, value_network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradient, value_network.trainable_weights))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def reset_metric(self):\n",
    "        if len(self.episode_loss) > 0:\n",
    "            self.loss_h.append(np.mean(self.episode_loss))\n",
    "            self.episode_loss = []\n",
    "            \n",
    "    def add_update(self, s, r):        \n",
    "        self.states.append(tf.squeeze(s))\n",
    "        self.rewards.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy:\n",
    "    def __init__(self):\n",
    "        self.all_strats = set()\n",
    "        \n",
    "    def add_strat(self, move_list):\n",
    "        tot = 0\n",
    "        for i,a in enumerate(move_list):\n",
    "            tot += 10**i * a\n",
    "            \n",
    "        self.all_strats.add(tot)\n",
    "        \n",
    "    @property\n",
    "    def get_total(self):\n",
    "        return len(self.all_strats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 71 calls to <function Minimax.update at 0x000001A49D2148B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 72 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 73 calls to <function Minimax.update at 0x000001A49D2148B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Minimax.update at 0x000001A49D2148B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Minimax.update at 0x000001A49D2148B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 15 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Minimax.update at 0x000001A49D2148B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Minimax.update at 0x000001A49D2148B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function Minimax.update at 0x000001A49D0589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Game number 10 completed in 9.85136342048645 seconds with total loss 2.05171460211277\n",
      "10 strategies were explored with an exploration rate of 0.8964070111162198\n",
      "Game number 20 completed in 17.7414972782135 seconds with total loss 2.284701979160309\n",
      "20 strategies were explored with an exploration rate of 0.892649809453171\n",
      "Game number 30 completed in 25.33602547645569 seconds with total loss 2.3737392902374266\n",
      "30 strategies were explored with an exploration rate of 0.8892640079816001\n",
      "Game number 40 completed in 33.288525342941284 seconds with total loss 1.7389483332633973\n",
      "40 strategies were explored with an exploration rate of 0.8852711110539446\n",
      "Game number 50 completed in 41.38364553451538 seconds with total loss 2.006045562773943\n",
      "50 strategies were explored with an exploration rate of 0.8814724283603314\n",
      "Game number 60 completed in 49.202014207839966 seconds with total loss 1.9240258157253265\n",
      "60 strategies were explored with an exploration rate of 0.8776022767424543\n",
      "Game number 70 completed in 56.71470355987549 seconds with total loss 2.119845288991928\n",
      "70 strategies were explored with an exploration rate of 0.8738365008813538\n",
      "Game number 80 completed in 64.50886034965515 seconds with total loss 2.0100641012191773\n",
      "80 strategies were explored with an exploration rate of 0.8704350056775788\n",
      "Game number 90 completed in 72.19862508773804 seconds with total loss 2.2403239130973818\n",
      "90 strategies were explored with an exploration rate of 0.8670467511310777\n",
      "Game number 100 completed in 79.93955087661743 seconds with total loss 1.7853867411613464\n",
      "100 strategies were explored with an exploration rate of 0.8632399362169605\n",
      "Game number 110 completed in 87.09902501106262 seconds with total loss 2.087597018480301\n",
      "110 strategies were explored with an exploration rate of 0.8596217510803607\n",
      "Game number 120 completed in 94.2456226348877 seconds with total loss 2.0618151843547823\n",
      "120 strategies were explored with an exploration rate of 0.8560187312102568\n",
      "Game number 130 completed in 101.60283756256104 seconds with total loss 2.055054986476898\n",
      "130 strategies were explored with an exploration rate of 0.8526865934412159\n",
      "Game number 140 completed in 109.01996493339539 seconds with total loss 2.0936185657978057\n",
      "140 strategies were explored with an exploration rate of 0.8491126416010275\n",
      "Game number 150 completed in 116.79277634620667 seconds with total loss 1.8877255991101265\n",
      "150 strategies were explored with an exploration rate of 0.8457228057317498\n",
      "Game number 160 completed in 124.36301279067993 seconds with total loss 2.0057045996189116\n",
      "160 strategies were explored with an exploration rate of 0.8424307458732777\n",
      "Game number 170 completed in 132.68427538871765 seconds with total loss 1.7023824512958527\n",
      "170 strategies were explored with an exploration rate of 0.839151500684149\n",
      "Game number 180 completed in 140.76449465751648 seconds with total loss 1.9685220777988435\n",
      "180 strategies were explored with an exploration rate of 0.8356342798516226\n",
      "Game number 190 completed in 148.74965286254883 seconds with total loss 1.7917628020048142\n",
      "190 strategies were explored with an exploration rate of 0.8322150226059208\n",
      "Game number 200 completed in 155.7896695137024 seconds with total loss 1.8886893928050994\n",
      "200 strategies were explored with an exploration rate of 0.8291413631135119\n",
      "Game number 210 completed in 162.773921251297 seconds with total loss 1.31714625954628\n",
      "210 strategies were explored with an exploration rate of 0.8259138481641621\n",
      "Game number 220 completed in 169.77253603935242 seconds with total loss 1.810381668806076\n",
      "220 strategies were explored with an exploration rate of 0.8222876294500439\n",
      "Game number 230 completed in 176.7637779712677 seconds with total loss 1.7647394448518754\n",
      "230 strategies were explored with an exploration rate of 0.8190048846590615\n",
      "Game number 240 completed in 183.76178860664368 seconds with total loss 1.4048921182751655\n",
      "240 strategies were explored with an exploration rate of 0.815245926471046\n",
      "Game number 250 completed in 190.7743501663208 seconds with total loss 1.5433208465576171\n",
      "250 strategies were explored with an exploration rate of 0.8118289034973899\n",
      "Game number 260 completed in 197.82728242874146 seconds with total loss 1.6739620059728622\n",
      "260 strategies were explored with an exploration rate of 0.8082645254825785\n",
      "Game number 270 completed in 204.8423240184784 seconds with total loss 1.6812809586524964\n",
      "270 strategies were explored with an exploration rate of 0.8051182756932447\n",
      "Game number 280 completed in 211.8510444164276 seconds with total loss 1.7156272292137147\n",
      "280 strategies were explored with an exploration rate of 0.8019842729931058\n",
      "Game number 290 completed in 218.7840542793274 seconds with total loss 1.7606018394231797\n",
      "289 strategies were explored with an exploration rate of 0.7991021763898044\n",
      "Game number 300 completed in 225.85552597045898 seconds with total loss 1.580064445734024\n",
      "299 strategies were explored with an exploration rate of 0.7958324015941103\n",
      "Game number 310 completed in 232.8061089515686 seconds with total loss 1.6577987611293792\n",
      "309 strategies were explored with an exploration rate of 0.7928931157736416\n",
      "Game number 320 completed in 239.8895468711853 seconds with total loss 1.6456297099590302\n",
      "319 strategies were explored with an exploration rate of 0.7898856892893084\n",
      "Game number 330 completed in 247.0111584663391 seconds with total loss 1.674793741106987\n",
      "328 strategies were explored with an exploration rate of 0.7868109809417648\n",
      "Game number 340 completed in 254.02564978599548 seconds with total loss 1.6511990636587144\n",
      "337 strategies were explored with an exploration rate of 0.7836698663771425\n",
      "Game number 350 completed in 260.98834109306335 seconds with total loss 1.5672860383987426\n",
      "347 strategies were explored with an exploration rate of 0.780775501028915\n",
      "Game number 360 completed in 268.06148505210876 seconds with total loss 1.5324992001056672\n",
      "356 strategies were explored with an exploration rate of 0.7774252071513432\n",
      "Game number 370 completed in 275.03896021842957 seconds with total loss 1.5297587454319\n",
      "366 strategies were explored with an exploration rate of 0.7742441304092361\n",
      "Game number 380 completed in 282.02718901634216 seconds with total loss 1.6091623723506927\n",
      "376 strategies were explored with an exploration rate of 0.7709989624238176\n",
      "Game number 390 completed in 289.0224928855896 seconds with total loss 1.5524410486221314\n",
      "386 strategies were explored with an exploration rate of 0.7679977725328545\n",
      "Game number 400 completed in 296.13301515579224 seconds with total loss 1.6177821844816207\n",
      "396 strategies were explored with an exploration rate of 0.765008265071052\n",
      "Game number 410 completed in 303.00032114982605 seconds with total loss 1.4868587970733642\n",
      "406 strategies were explored with an exploration rate of 0.7620303945633965\n",
      "Game number 420 completed in 310.0150375366211 seconds with total loss 1.3027718275785447\n",
      "416 strategies were explored with an exploration rate of 0.758532930207792\n",
      "Game number 430 completed in 317.0195243358612 seconds with total loss 1.5478887498378753\n",
      "426 strategies were explored with an exploration rate of 0.755429157085683\n",
      "Game number 440 completed in 323.96785402297974 seconds with total loss 1.5586810439825058\n",
      "436 strategies were explored with an exploration rate of 0.7524133253480245\n",
      "Game number 450 completed in 330.9803214073181 seconds with total loss 1.3905434668064118\n",
      "446 strategies were explored with an exploration rate of 0.7491098145917953\n",
      "Game number 460 completed in 337.93969440460205 seconds with total loss 1.4955688118934631\n",
      "456 strategies were explored with an exploration rate of 0.7459699946146113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 470 completed in 344.9532632827759 seconds with total loss 1.5014955163002015\n",
      "466 strategies were explored with an exploration rate of 0.742843334883492\n",
      "Game number 480 completed in 351.95090651512146 seconds with total loss 1.5782701969146729\n",
      "476 strategies were explored with an exploration rate of 0.7398777483894622\n",
      "Game number 490 completed in 359.0413203239441 seconds with total loss 1.3311987668275833\n",
      "486 strategies were explored with an exploration rate of 0.7367029460457484\n",
      "Game number 500 completed in 365.9976556301117 seconds with total loss 1.3791417211294175\n",
      "496 strategies were explored with an exploration rate of 0.733248394025671\n",
      "Game number 510 completed in 373.0151159763336 seconds with total loss 1.43775355219841\n",
      "506 strategies were explored with an exploration rate of 0.7301750556299456\n",
      "Game number 520 completed in 379.92078495025635 seconds with total loss 1.390764319896698\n",
      "516 strategies were explored with an exploration rate of 0.7272600435653988\n",
      "Game number 530 completed in 386.8864941596985 seconds with total loss 1.4639524847269059\n",
      "526 strategies were explored with an exploration rate of 0.7242842331739056\n",
      "Game number 540 completed in 401.1840708255768 seconds with total loss 1.4692836582660675\n",
      "535 strategies were explored with an exploration rate of 0.7213927385089154\n",
      "Game number 550 completed in 412.59343957901 seconds with total loss 1.402786073088646\n",
      "545 strategies were explored with an exploration rate of 0.7185127872974684\n",
      "Game number 560 completed in 422.10343313217163 seconds with total loss 1.3328604593873024\n",
      "555 strategies were explored with an exploration rate of 0.7156443334556749\n",
      "Game number 570 completed in 431.63255739212036 seconds with total loss 1.3899606674909593\n",
      "565 strategies were explored with an exploration rate of 0.7127160523505148\n",
      "Game number 580 completed in 439.18296670913696 seconds with total loss 1.4535315483808517\n",
      "575 strategies were explored with an exploration rate of 0.7097287732418449\n",
      "Game number 590 completed in 446.17596435546875 seconds with total loss 1.479847976565361\n",
      "585 strategies were explored with an exploration rate of 0.7068953870276705\n",
      "Game number 600 completed in 453.3069751262665 seconds with total loss 1.347307249903679\n",
      "595 strategies were explored with an exploration rate of 0.7036509738954534\n",
      "Game number 610 completed in 460.42980575561523 seconds with total loss 1.5791506230831147\n",
      "605 strategies were explored with an exploration rate of 0.700841851531823\n",
      "Game number 620 completed in 467.3700704574585 seconds with total loss 1.5703026324510574\n",
      "615 strategies were explored with an exploration rate of 0.6981137551497357\n",
      "Game number 630 completed in 474.4971649646759 seconds with total loss 1.2933155983686446\n",
      "625 strategies were explored with an exploration rate of 0.6951181613629881\n",
      "Game number 640 completed in 481.5129773616791 seconds with total loss 1.329634627699852\n",
      "634 strategies were explored with an exploration rate of 0.692481593171594\n",
      "Game number 650 completed in 488.64463329315186 seconds with total loss 1.27075175344944\n",
      "644 strategies were explored with an exploration rate of 0.6892344041934618\n",
      "Game number 660 completed in 495.91866302490234 seconds with total loss 1.3768130540847778\n",
      "653 strategies were explored with an exploration rate of 0.6864828358016994\n",
      "Game number 670 completed in 502.8404071331024 seconds with total loss 1.3792120426893235\n",
      "663 strategies were explored with an exploration rate of 0.6836055106354038\n",
      "Game number 680 completed in 509.8011348247528 seconds with total loss 1.4905972123146056\n",
      "673 strategies were explored with an exploration rate of 0.6812169543303472\n",
      "Game number 690 completed in 516.895726442337 seconds with total loss 1.3187293961644173\n",
      "683 strategies were explored with an exploration rate of 0.6782938644173268\n",
      "Game number 700 completed in 523.9002113342285 seconds with total loss 1.385710433125496\n",
      "693 strategies were explored with an exploration rate of 0.675585972956451\n",
      "Game number 710 completed in 530.928638458252 seconds with total loss 1.4427295684814454\n",
      "703 strategies were explored with an exploration rate of 0.6728216030773982\n",
      "Game number 720 completed in 538.2078325748444 seconds with total loss 1.342719155550003\n",
      "713 strategies were explored with an exploration rate of 0.6702696052506139\n",
      "Game number 730 completed in 546.2675483226776 seconds with total loss 1.3749940901994706\n",
      "723 strategies were explored with an exploration rate of 0.6673934902190404\n",
      "Game number 740 completed in 554.2968258857727 seconds with total loss 1.460377386212349\n",
      "730 strategies were explored with an exploration rate of 0.6651946119003767\n",
      "Game number 750 completed in 561.3214299678802 seconds with total loss 1.3755427494645118\n",
      "740 strategies were explored with an exploration rate of 0.6626715430866025\n",
      "Game number 760 completed in 568.9964518547058 seconds with total loss 1.2751370012760161\n",
      "750 strategies were explored with an exploration rate of 0.6599600166092282\n",
      "Game number 770 completed in 577.324453830719 seconds with total loss 1.413539505004883\n",
      "760 strategies were explored with an exploration rate of 0.6574568025024755\n",
      "Game number 780 completed in 587.4266006946564 seconds with total loss 1.4084577977657318\n",
      "770 strategies were explored with an exploration rate of 0.6547666137561597\n",
      "Game number 790 completed in 596.0890538692474 seconds with total loss 1.150650641322136\n",
      "779 strategies were explored with an exploration rate of 0.6520222240095309\n",
      "Game number 800 completed in 603.9657731056213 seconds with total loss 1.1771469444036484\n",
      "788 strategies were explored with an exploration rate of 0.6490945697790178\n",
      "Game number 810 completed in 611.4582493305206 seconds with total loss 1.219362336397171\n",
      "798 strategies were explored with an exploration rate of 0.6464385977056165\n",
      "Game number 820 completed in 620.719489812851 seconds with total loss 1.2239891469478608\n",
      "808 strategies were explored with an exploration rate of 0.6436003746321822\n",
      "Game number 830 completed in 630.3437840938568 seconds with total loss 1.2217706620693207\n",
      "818 strategies were explored with an exploration rate of 0.6409668837649952\n",
      "Game number 840 completed in 638.2301304340363 seconds with total loss 1.2869179666042327\n",
      "828 strategies were explored with an exploration rate of 0.6383441686437853\n",
      "Game number 850 completed in 646.0977909564972 seconds with total loss 1.280450662970543\n",
      "838 strategies were explored with an exploration rate of 0.6355414845920165\n",
      "Game number 860 completed in 658.4725425243378 seconds with total loss 1.198843416571617\n",
      "847 strategies were explored with an exploration rate of 0.633004269603883\n",
      "Game number 870 completed in 666.7810804843903 seconds with total loss 1.24201857149601\n",
      "857 strategies were explored with an exploration rate of 0.6303510945784992\n",
      "Game number 880 completed in 677.1339991092682 seconds with total loss 1.2333381682634355\n",
      "867 strategies were explored with an exploration rate of 0.6279601864743038\n",
      "Game number 890 completed in 684.6898674964905 seconds with total loss 1.108342817425728\n",
      "877 strategies were explored with an exploration rate of 0.6253281532263583\n",
      "Game number 900 completed in 692.1840891838074 seconds with total loss 1.2281432271003723\n",
      "887 strategies were explored with an exploration rate of 0.6226448811712452\n",
      "Game number 910 completed in 699.7924523353577 seconds with total loss 1.2179735898971558\n",
      "897 strategies were explored with an exploration rate of 0.62028320256827\n",
      "Game number 920 completed in 707.6508204936981 seconds with total loss 1.2121337324380874\n",
      "905 strategies were explored with an exploration rate of 0.6179304817612914\n",
      "Game number 930 completed in 715.4103136062622 seconds with total loss 1.07425899207592\n",
      "914 strategies were explored with an exploration rate of 0.6152789529837084\n",
      "Game number 940 completed in 723.2341587543488 seconds with total loss 1.2739413559436799\n",
      "923 strategies were explored with an exploration rate of 0.6127613480112425\n",
      "Game number 950 completed in 731.0439302921295 seconds with total loss 1.3321251660585403\n",
      "933 strategies were explored with an exploration rate of 0.6102540446016149\n",
      "Game number 960 completed in 738.9118454456329 seconds with total loss 1.0305881589651107\n",
      "943 strategies were explored with an exploration rate of 0.6076962249027213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 970 completed in 746.2532348632812 seconds with total loss 1.2392573326826095\n",
      "952 strategies were explored with an exploration rate of 0.6053307071058865\n",
      "Game number 980 completed in 753.9263200759888 seconds with total loss 1.0675189912319183\n",
      "961 strategies were explored with an exploration rate of 0.6026127031172794\n",
      "Game number 990 completed in 761.9695432186127 seconds with total loss 1.260574886202812\n",
      "971 strategies were explored with an exploration rate of 0.6004470895469797\n",
      "Game number 1000 completed in 769.9607992172241 seconds with total loss 1.1953599125146865\n",
      "980 strategies were explored with an exploration rate of 0.5981696066887984\n",
      "Game number 1010 completed in 778.6906957626343 seconds with total loss 1.0808906883001328\n",
      "989 strategies were explored with an exploration rate of 0.5958411721977422\n",
      "Game number 1020 completed in 785.7316653728485 seconds with total loss 1.1323715224862099\n",
      "998 strategies were explored with an exploration rate of 0.5936405235370643\n",
      "Game number 1030 completed in 794.0218861103058 seconds with total loss 1.1954530149698257\n",
      "1008 strategies were explored with an exploration rate of 0.5912114589206549\n",
      "Game number 1040 completed in 801.5380148887634 seconds with total loss 1.2433891981840133\n",
      "1018 strategies were explored with an exploration rate of 0.5888512186985827\n",
      "Game number 1050 completed in 808.6303043365479 seconds with total loss 1.116666316986084\n",
      "1025 strategies were explored with an exploration rate of 0.5864417510107172\n",
      "Game number 1060 completed in 816.5223321914673 seconds with total loss 1.1069298312067986\n",
      "1034 strategies were explored with an exploration rate of 0.5839837381941564\n",
      "Game number 1070 completed in 824.0423669815063 seconds with total loss 1.1204196035861969\n",
      "1043 strategies were explored with an exploration rate of 0.5814197265059067\n",
      "Game number 1080 completed in 831.6170725822449 seconds with total loss 1.3737413883209229\n",
      "1051 strategies were explored with an exploration rate of 0.5792144140252938\n",
      "Game number 1090 completed in 839.9386224746704 seconds with total loss 1.1964478850364686\n",
      "1061 strategies were explored with an exploration rate of 0.5767290152102014\n",
      "Game number 1100 completed in 848.471697807312 seconds with total loss 1.144236186146736\n",
      "1070 strategies were explored with an exploration rate of 0.5743117123687411\n",
      "Game number 1110 completed in 856.9427261352539 seconds with total loss 1.2381681442260741\n",
      "1078 strategies were explored with an exploration rate of 0.5720761470963778\n",
      "Game number 1120 completed in 866.8957629203796 seconds with total loss 0.9725245565176011\n",
      "1086 strategies were explored with an exploration rate of 0.569963270938005\n",
      "Game number 1130 completed in 874.6831464767456 seconds with total loss 1.1798812001943588\n",
      "1096 strategies were explored with an exploration rate of 0.5678014125462296\n",
      "Game number 1140 completed in 884.4328858852386 seconds with total loss 0.982874047756195\n",
      "1105 strategies were explored with an exploration rate of 0.5651388746437508\n",
      "Game number 1150 completed in 892.0606977939606 seconds with total loss 1.123104989528656\n",
      "1115 strategies were explored with an exploration rate of 0.5626576023320391\n",
      "Game number 1160 completed in 900.0887842178345 seconds with total loss 1.0642829418182373\n",
      "1125 strategies were explored with an exploration rate of 0.5602992784311844\n",
      "Game number 1170 completed in 907.8227081298828 seconds with total loss 1.191194760799408\n",
      "1135 strategies were explored with an exploration rate of 0.558174075354522\n",
      "Game number 1180 completed in 915.4839882850647 seconds with total loss 1.0258381843566895\n",
      "1144 strategies were explored with an exploration rate of 0.5558901327284992\n",
      "Game number 1190 completed in 923.3407559394836 seconds with total loss 1.18968109190464\n",
      "1153 strategies were explored with an exploration rate of 0.5537816534463759\n",
      "Game number 1200 completed in 930.9613676071167 seconds with total loss 1.0638620197772979\n",
      "1161 strategies were explored with an exploration rate of 0.5516811715806864\n",
      "Game number 1210 completed in 938.7243144512177 seconds with total loss 1.0760286092758178\n",
      "1171 strategies were explored with an exploration rate of 0.5494237966874738\n",
      "Game number 1220 completed in 945.7543759346008 seconds with total loss 1.1119343370199204\n",
      "1181 strategies were explored with an exploration rate of 0.5472303815830492\n",
      "Game number 1230 completed in 953.8827474117279 seconds with total loss 1.0865577816963197\n",
      "1191 strategies were explored with an exploration rate of 0.5451002330753622\n",
      "Game number 1240 completed in 961.6942918300629 seconds with total loss 1.0914368420839309\n",
      "1200 strategies were explored with an exploration rate of 0.5428697861364122\n",
      "Game number 1250 completed in 970.5952391624451 seconds with total loss 1.0353924497961997\n",
      "1209 strategies were explored with an exploration rate of 0.5407025360176028\n",
      "Game number 1260 completed in 980.1968023777008 seconds with total loss 1.1735641479492187\n",
      "1218 strategies were explored with an exploration rate of 0.5385439380161102\n",
      "Game number 1270 completed in 987.934736251831 seconds with total loss 1.028806495666504\n",
      "1226 strategies were explored with an exploration rate of 0.5366085688240284\n",
      "Game number 1280 completed in 995.9971516132355 seconds with total loss 1.1184727936983108\n",
      "1235 strategies were explored with an exploration rate of 0.5345732240978645\n",
      "Game number 1290 completed in 1005.1989307403564 seconds with total loss 1.1356620758771896\n",
      "1243 strategies were explored with an exploration rate of 0.5324390955959197\n",
      "Game number 1300 completed in 1013.3337399959564 seconds with total loss 1.0658372819423676\n",
      "1251 strategies were explored with an exploration rate of 0.5303134869828461\n",
      "Game number 1310 completed in 1021.2492372989655 seconds with total loss 1.1008310854434966\n",
      "1258 strategies were explored with an exploration rate of 0.5284605416755271\n",
      "Game number 1320 completed in 1029.0859065055847 seconds with total loss 0.9739152401685714\n",
      "1267 strategies were explored with an exploration rate of 0.5260876935063628\n",
      "Game number 1330 completed in 1036.6004548072815 seconds with total loss 1.0226979315280915\n",
      "1276 strategies were explored with an exploration rate of 0.5240398450079058\n",
      "Game number 1340 completed in 1045.8414425849915 seconds with total loss 0.9956463485956192\n",
      "1286 strategies were explored with an exploration rate of 0.5218955731891408\n",
      "Game number 1350 completed in 1054.0660257339478 seconds with total loss 1.1420238047838212\n",
      "1295 strategies were explored with an exploration rate of 0.5196041628943253\n",
      "Game number 1360 completed in 1061.8665192127228 seconds with total loss 0.9966812133789062\n",
      "1304 strategies were explored with an exploration rate of 0.5174262932427675\n",
      "Game number 1370 completed in 1069.916363477707 seconds with total loss 1.1816509276628495\n",
      "1314 strategies were explored with an exploration rate of 0.5153606188874374\n",
      "Game number 1380 completed in 1078.4299087524414 seconds with total loss 1.029359868168831\n",
      "1322 strategies were explored with an exploration rate of 0.5133545265903174\n",
      "Game number 1390 completed in 1086.3695199489594 seconds with total loss 1.1269477903842926\n",
      "1331 strategies were explored with an exploration rate of 0.511253977071251\n",
      "Game number 1400 completed in 1093.9576795101166 seconds with total loss 0.8805967554450035\n",
      "1340 strategies were explored with an exploration rate of 0.509111106400817\n",
      "Game number 1410 completed in 1101.0736014842987 seconds with total loss 0.9177584290504456\n",
      "1347 strategies were explored with an exploration rate of 0.5069265196399223\n",
      "Game number 1420 completed in 1107.9819703102112 seconds with total loss 0.7858681827783585\n",
      "1355 strategies were explored with an exploration rate of 0.5047513069022144\n",
      "Game number 1430 completed in 1115.0782444477081 seconds with total loss 0.8841848611831665\n",
      "1362 strategies were explored with an exploration rate of 0.5026356915330779\n",
      "Game number 1440 completed in 1122.0450236797333 seconds with total loss 0.9085294961929321\n",
      "1370 strategies were explored with an exploration rate of 0.500729205197637\n",
      "Game number 1450 completed in 1129.0087502002716 seconds with total loss 0.8927149593830108\n",
      "1379 strategies were explored with an exploration rate of 0.4984808738954867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 1460 completed in 1135.9645476341248 seconds with total loss 1.036261460185051\n",
      "1387 strategies were explored with an exploration rate of 0.49644118454691977\n",
      "Game number 1470 completed in 1143.1056106090546 seconds with total loss 1.0447117581963539\n",
      "1396 strategies were explored with an exploration rate of 0.49440984122095166\n",
      "Game number 1480 completed in 1150.1170563697815 seconds with total loss 0.8394294194877148\n",
      "1403 strategies were explored with an exploration rate of 0.49223910849542013\n",
      "Game number 1490 completed in 1157.0939407348633 seconds with total loss 1.073044988512993\n",
      "1413 strategies were explored with an exploration rate of 0.49012691917938683\n",
      "Game number 1500 completed in 1164.0314328670502 seconds with total loss 0.8730667263269425\n",
      "1423 strategies were explored with an exploration rate of 0.4882190515598172\n",
      "Game number 1510 completed in 1170.9901292324066 seconds with total loss 0.9932529509067536\n",
      "1432 strategies were explored with an exploration rate of 0.48612411223705504\n",
      "Game number 1520 completed in 1177.9399626255035 seconds with total loss 1.0148469984531403\n",
      "1440 strategies were explored with an exploration rate of 0.4842802539657064\n",
      "Game number 1530 completed in 1186.2423136234283 seconds with total loss 1.092619326710701\n",
      "1449 strategies were explored with an exploration rate of 0.48239514507006537\n",
      "Game number 1540 completed in 1194.5892839431763 seconds with total loss 0.9803803712129593\n",
      "1456 strategies were explored with an exploration rate of 0.4804693224102582\n",
      "Game number 1550 completed in 1202.4268362522125 seconds with total loss 0.791754686832428\n",
      "1464 strategies were explored with an exploration rate of 0.47835979627462477\n",
      "Game number 1560 completed in 1210.2164103984833 seconds with total loss 0.7721355810761452\n",
      "1472 strategies were explored with an exploration rate of 0.47625953212578087\n",
      "Game number 1570 completed in 1217.3727612495422 seconds with total loss 0.9398223981261253\n",
      "1482 strategies were explored with an exploration rate of 0.47445308999400154\n",
      "Game number 1580 completed in 1224.3303265571594 seconds with total loss 0.8304031044244766\n",
      "1490 strategies were explored with an exploration rate of 0.4722755091750486\n",
      "Game number 1590 completed in 1231.3178083896637 seconds with total loss 0.926604326069355\n",
      "1498 strategies were explored with an exploration rate of 0.47024898331091575\n",
      "Game number 1600 completed in 1240.061518907547 seconds with total loss 0.8212551340460778\n",
      "1506 strategies were explored with an exploration rate of 0.46813751168511913\n",
      "Game number 1610 completed in 1256.1264100074768 seconds with total loss 0.8988789051771164\n",
      "1513 strategies were explored with an exploration rate of 0.4662219816291974\n",
      "Game number 1620 completed in 1266.5473840236664 seconds with total loss 0.9994083374738694\n",
      "1521 strategies were explored with an exploration rate of 0.46426785813060306\n",
      "Game number 1630 completed in 1277.4988007545471 seconds with total loss 1.05889193713665\n",
      "1528 strategies were explored with an exploration rate of 0.4625069001596105\n",
      "Game number 1640 completed in 1290.2553746700287 seconds with total loss 0.7831351771950722\n",
      "1536 strategies were explored with an exploration rate of 0.4604301913711167\n",
      "Game number 1650 completed in 1300.9184634685516 seconds with total loss 0.7815998658537865\n",
      "1544 strategies were explored with an exploration rate of 0.4585003435907171\n",
      "Game number 1660 completed in 1312.402006149292 seconds with total loss 0.9153099596500397\n",
      "1550 strategies were explored with an exploration rate of 0.45662424700183896\n",
      "Game number 1670 completed in 1322.9410195350647 seconds with total loss 0.9277889803051949\n",
      "1560 strategies were explored with an exploration rate of 0.45475582704496337\n",
      "Game number 1680 completed in 1332.2524967193604 seconds with total loss 0.9754257559776306\n",
      "1566 strategies were explored with an exploration rate of 0.452985644907891\n",
      "Game number 1690 completed in 1341.7577722072601 seconds with total loss 0.8700375251471997\n",
      "1575 strategies were explored with an exploration rate of 0.45104189151045093\n",
      "Game number 1700 completed in 1351.1137065887451 seconds with total loss 0.9206528812646866\n",
      "1585 strategies were explored with an exploration rate of 0.44910647872449067\n",
      "Game number 1710 completed in 1360.4922988414764 seconds with total loss 0.790587241947651\n",
      "1594 strategies were explored with an exploration rate of 0.44726882005191576\n",
      "Game number 1720 completed in 1369.751249074936 seconds with total loss 0.8112275153398514\n",
      "1602 strategies were explored with an exploration rate of 0.44530506248916396\n",
      "Game number 1730 completed in 1379.1051981449127 seconds with total loss 0.9700399979948997\n",
      "1610 strategies were explored with an exploration rate of 0.44348295849202535\n",
      "Game number 1740 completed in 1387.7252888679504 seconds with total loss 0.78882517516613\n",
      "1619 strategies were explored with an exploration rate of 0.4415799809540706\n",
      "Game number 1750 completed in 1395.7739470005035 seconds with total loss 0.7809947803616524\n",
      "1629 strategies were explored with an exploration rate of 0.4397291419733002\n",
      "Game number 1760 completed in 1403.6702477931976 seconds with total loss 0.9093115657567978\n",
      "1637 strategies were explored with an exploration rate of 0.4378422719956924\n",
      "Game number 1770 completed in 1411.528086900711 seconds with total loss 0.9166047081351281\n",
      "1643 strategies were explored with an exploration rate of 0.4360507043241727\n",
      "Game number 1780 completed in 1419.3695855140686 seconds with total loss 0.8268527463078499\n",
      "1651 strategies were explored with an exploration rate of 0.43422304076159046\n",
      "Game number 1790 completed in 1427.2533960342407 seconds with total loss 0.7679016843438149\n",
      "1658 strategies were explored with an exploration rate of 0.4322733297311064\n",
      "Game number 1800 completed in 1435.1821172237396 seconds with total loss 0.9412654742598534\n",
      "1668 strategies were explored with an exploration rate of 0.43063372631616725\n",
      "Game number 1810 completed in 1443.3625512123108 seconds with total loss 0.7681338801980019\n",
      "1674 strategies were explored with an exploration rate of 0.4288287674831643\n",
      "Game number 1820 completed in 1451.440705537796 seconds with total loss 0.9631281584501267\n",
      "1681 strategies were explored with an exploration rate of 0.42703137395726515\n",
      "Game number 1830 completed in 1459.4361662864685 seconds with total loss 0.8713139295578003\n",
      "1690 strategies were explored with an exploration rate of 0.4252840424334739\n",
      "Game number 1840 completed in 1467.7331609725952 seconds with total loss 0.7832219794392585\n",
      "1698 strategies were explored with an exploration rate of 0.4235438606594205\n",
      "Game number 1850 completed in 1475.8394403457642 seconds with total loss 0.7428929597139359\n",
      "1705 strategies were explored with an exploration rate of 0.42172644143791505\n",
      "Game number 1860 completed in 1483.779560804367 seconds with total loss 0.7513048887252808\n",
      "1712 strategies were explored with an exploration rate of 0.41979085828103857\n",
      "Game number 1870 completed in 1491.6210453510284 seconds with total loss 0.8039881527423859\n",
      "1719 strategies were explored with an exploration rate of 0.4180313462610443\n",
      "Game number 1880 completed in 1499.5939445495605 seconds with total loss 0.6408352762460708\n",
      "1729 strategies were explored with an exploration rate of 0.4161127223537777\n",
      "Game number 1890 completed in 1507.4525351524353 seconds with total loss 0.7721616581082344\n",
      "1738 strategies were explored with an exploration rate of 0.4145344157740141\n",
      "Game number 1900 completed in 1515.2254507541656 seconds with total loss 0.7434256002306938\n",
      "1745 strategies were explored with an exploration rate of 0.412838219436808\n",
      "Game number 1910 completed in 1523.0786435604095 seconds with total loss 0.6675674691796303\n",
      "1750 strategies were explored with an exploration rate of 0.4110667379326058\n",
      "Game number 1920 completed in 1530.9344182014465 seconds with total loss 0.7354604482650757\n",
      "1756 strategies were explored with an exploration rate of 0.4094256732438461\n",
      "Game number 1930 completed in 1538.8185408115387 seconds with total loss 0.8729810178279876\n",
      "1762 strategies were explored with an exploration rate of 0.4077096058756414\n",
      "Game number 1940 completed in 1546.746529340744 seconds with total loss 0.7592643484473228\n",
      "1768 strategies were explored with an exploration rate of 0.40587894319496964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 1950 completed in 1555.7235283851624 seconds with total loss 0.8712817266583442\n",
      "1774 strategies were explored with an exploration rate of 0.404299019173487\n",
      "Game number 1960 completed in 1565.8629686832428 seconds with total loss 0.858348871767521\n",
      "1782 strategies were explored with an exploration rate of 0.4027655217150485\n",
      "Game number 1970 completed in 1577.0837473869324 seconds with total loss 0.6779641315340996\n",
      "1789 strategies were explored with an exploration rate of 0.4009570585367492\n",
      "Game number 1980 completed in 1587.8371484279633 seconds with total loss 0.6966237857937813\n",
      "1797 strategies were explored with an exploration rate of 0.3993164181744838\n",
      "Game number 1990 completed in 1598.5949659347534 seconds with total loss 0.6981629744172096\n",
      "1804 strategies were explored with an exploration rate of 0.39752344186488353\n",
      "Game number 2000 completed in 1609.3280186653137 seconds with total loss 0.5925506219267845\n",
      "1812 strategies were explored with an exploration rate of 0.39569894237200437\n",
      "Game number 2010 completed in 1620.0614931583405 seconds with total loss 0.5111702859401703\n",
      "1818 strategies were explored with an exploration rate of 0.3941192291483751\n",
      "Game number 2020 completed in 1630.7143938541412 seconds with total loss 0.8344376496970654\n",
      "1824 strategies were explored with an exploration rate of 0.3925065678892231\n",
      "Game number 2030 completed in 1643.1176855564117 seconds with total loss 0.6794668793678283\n",
      "1829 strategies were explored with an exploration rate of 0.390900505334586\n",
      "Game number 2040 completed in 1654.692543745041 seconds with total loss 0.7004698261618614\n",
      "1835 strategies were explored with an exploration rate of 0.3893399484786623\n",
      "Game number 2050 completed in 1665.404330253601 seconds with total loss 0.9829983413219452\n",
      "1840 strategies were explored with an exploration rate of 0.38774684313105356\n",
      "Game number 2060 completed in 1676.156848192215 seconds with total loss 0.6111698120832443\n",
      "1846 strategies were explored with an exploration rate of 0.3861216404432256\n",
      "Game number 2070 completed in 1686.8942849636078 seconds with total loss 0.7880804114043712\n",
      "1853 strategies were explored with an exploration rate of 0.3846570893901998\n",
      "Game number 2080 completed in 1697.626743555069 seconds with total loss 0.6153198927640915\n",
      "1857 strategies were explored with an exploration rate of 0.3829299351405713\n",
      "Game number 2090 completed in 1708.2676267623901 seconds with total loss 0.5217837952077389\n",
      "1863 strategies were explored with an exploration rate of 0.38143934240789334\n",
      "Game number 2100 completed in 1721.6647944450378 seconds with total loss 0.7250162936747074\n",
      "1869 strategies were explored with an exploration rate of 0.3797266362096021\n",
      "Game number 2110 completed in 1733.9140751361847 seconds with total loss 0.8914977580308914\n",
      "1874 strategies were explored with an exploration rate of 0.37821068778429495\n",
      "Game number 2120 completed in 1746.740263223648 seconds with total loss 0.6194195948541165\n",
      "1878 strategies were explored with an exploration rate of 0.37670079134325496\n",
      "Game number 2130 completed in 1759.3942475318909 seconds with total loss 0.6593082636594773\n",
      "1883 strategies were explored with an exploration rate of 0.37500936178026867\n",
      "Game number 2140 completed in 1770.2710494995117 seconds with total loss 0.7225267305970192\n",
      "1890 strategies were explored with an exploration rate of 0.3736243217926836\n",
      "Game number 2150 completed in 1780.8544194698334 seconds with total loss 0.6627326220273971\n",
      "1898 strategies were explored with an exploration rate of 0.37209552181371813\n",
      "Game number 2160 completed in 1791.5818779468536 seconds with total loss 0.5634232267737389\n",
      "1907 strategies were explored with an exploration rate of 0.37064710310893884\n",
      "Game number 2170 completed in 1804.4802963733673 seconds with total loss 0.5683266282081604\n",
      "1911 strategies were explored with an exploration rate of 0.3689828552973052\n",
      "Game number 2180 completed in 1816.5374929904938 seconds with total loss 0.5328043133020401\n",
      "1918 strategies were explored with an exploration rate of 0.3673995563856428\n",
      "Game number 2190 completed in 1827.4944939613342 seconds with total loss 0.5600384548306465\n",
      "1924 strategies were explored with an exploration rate of 0.3658962269680627\n",
      "Game number 2200 completed in 1838.3595011234283 seconds with total loss 0.5855424650013447\n",
      "1928 strategies were explored with an exploration rate of 0.36439904888980384\n",
      "Game number 2210 completed in 1849.0944707393646 seconds with total loss 0.8377339690923691\n",
      "1937 strategies were explored with an exploration rate of 0.3628717061810523\n",
      "Game number 2220 completed in 1860.1111552715302 seconds with total loss 0.6595559850335121\n",
      "1944 strategies were explored with an exploration rate of 0.36135076517877374\n",
      "Game number 2230 completed in 1870.9942543506622 seconds with total loss 0.5552731283009052\n",
      "1951 strategies were explored with an exploration rate of 0.3599081770871816\n",
      "Game number 2240 completed in 1881.6265137195587 seconds with total loss 0.8978509053587913\n",
      "1960 strategies were explored with an exploration rate of 0.3584713481100083\n",
      "Game number 2250 completed in 1889.7017929553986 seconds with total loss 0.6659047693014145\n",
      "1967 strategies were explored with an exploration rate of 0.35714738875822344\n",
      "Game number 2260 completed in 1897.5318558216095 seconds with total loss 0.7067802846431732\n",
      "1974 strategies were explored with an exploration rate of 0.35561487562433197\n",
      "Game number 2270 completed in 1905.2736451625824 seconds with total loss 0.5972406879067421\n",
      "1980 strategies were explored with an exploration rate of 0.35419518640763725\n",
      "Game number 2280 completed in 1913.2596926689148 seconds with total loss 0.5082539483904839\n",
      "1986 strategies were explored with an exploration rate of 0.3525695490994338\n",
      "Game number 2290 completed in 1920.9814112186432 seconds with total loss 0.6191800162196159\n",
      "1991 strategies were explored with an exploration rate of 0.35130251739556423\n",
      "Game number 2300 completed in 1929.288845539093 seconds with total loss 0.49242286533117297\n",
      "1998 strategies were explored with an exploration rate of 0.34969015646955554\n",
      "Game number 2310 completed in 1940.3555238246918 seconds with total loss 0.7225293636322021\n",
      "2005 strategies were explored with an exploration rate of 0.3482592905631553\n",
      "Game number 2320 completed in 1951.1394217014313 seconds with total loss 0.5635982118546963\n",
      "2007 strategies were explored with an exploration rate of 0.34686896638574793\n",
      "Game number 2330 completed in 1962.0722506046295 seconds with total loss 0.45767468214035034\n",
      "2013 strategies were explored with an exploration rate of 0.34548419267424535\n",
      "Game number 2340 completed in 1973.0740640163422 seconds with total loss 0.6425665885210037\n",
      "2019 strategies were explored with an exploration rate of 0.34403612972161823\n",
      "Game number 2350 completed in 1983.9918007850647 seconds with total loss 0.6559802666306496\n",
      "2024 strategies were explored with an exploration rate of 0.3425941361821777\n",
      "Game number 2360 completed in 1995.7869305610657 seconds with total loss 0.6471593856811524\n",
      "2030 strategies were explored with an exploration rate of 0.34122642849001\n",
      "Game number 2370 completed in 2006.8097131252289 seconds with total loss 0.5346661426126957\n",
      "2035 strategies were explored with an exploration rate of 0.3396942828665241\n",
      "Game number 2380 completed in 2017.6290230751038 seconds with total loss 0.6925798691809177\n",
      "2038 strategies were explored with an exploration rate of 0.33827048775036916\n",
      "Game number 2390 completed in 2028.4001381397247 seconds with total loss 0.5094406470656395\n",
      "2044 strategies were explored with an exploration rate of 0.33695373634378356\n",
      "Game number 2400 completed in 2039.3119328022003 seconds with total loss 0.7723909422755242\n",
      "2051 strategies were explored with an exploration rate of 0.3355749854568952\n",
      "Game number 2410 completed in 2050.138494491577 seconds with total loss 0.642965342849493\n",
      "2057 strategies were explored with an exploration rate of 0.33433559033490523\n",
      "Game number 2420 completed in 2061.2419650554657 seconds with total loss 0.6611050717532635\n",
      "2064 strategies were explored with an exploration rate of 0.3329675524057387\n",
      "Game number 2430 completed in 2071.9100155830383 seconds with total loss 0.6132316529750824\n",
      "2070 strategies were explored with an exploration rate of 0.33150564064289345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 2440 completed in 2082.775502681732 seconds with total loss 0.7741401694715023\n",
      "2076 strategies were explored with an exploration rate of 0.33014918233201546\n",
      "Game number 2450 completed in 2093.5383677482605 seconds with total loss 0.6076101452112198\n",
      "2082 strategies were explored with an exploration rate of 0.3287982743917001\n",
      "Game number 2460 completed in 2104.323798894882 seconds with total loss 0.7424820303916931\n",
      "2088 strategies were explored with an exploration rate of 0.32748564267514946\n",
      "Game number 2470 completed in 2114.934173345566 seconds with total loss 0.6519868932664394\n",
      "2096 strategies were explored with an exploration rate of 0.3260804075695379\n",
      "Game number 2480 completed in 2125.7122445106506 seconds with total loss 0.5689033217728138\n",
      "2102 strategies were explored with an exploration rate of 0.32464873418270185\n",
      "Game number 2490 completed in 2136.490528821945 seconds with total loss 0.5721254363656044\n",
      "2107 strategies were explored with an exploration rate of 0.3232880010013997\n",
      "Game number 2500 completed in 2147.2486844062805 seconds with total loss 0.5260397054255008\n",
      "2112 strategies were explored with an exploration rate of 0.3220939859859673\n",
      "Game number 2510 completed in 2158.081991672516 seconds with total loss 0.6412212640047074\n",
      "2119 strategies were explored with an exploration rate of 0.320743960780009\n",
      "Game number 2520 completed in 2168.6663947105408 seconds with total loss 0.4410396419465542\n",
      "2122 strategies were explored with an exploration rate of 0.3194954331170736\n",
      "Game number 2530 completed in 2179.5365331172943 seconds with total loss 0.4432228714227676\n",
      "2126 strategies were explored with an exploration rate of 0.31825176547183376\n",
      "Game number 2540 completed in 2190.4676718711853 seconds with total loss 0.42991223782300947\n",
      "2129 strategies were explored with an exploration rate of 0.31704464339052796\n",
      "Game number 2550 completed in 2201.2387092113495 seconds with total loss 0.3911337651312351\n",
      "2137 strategies were explored with an exploration rate of 0.31571578200654093\n",
      "Game number 2560 completed in 2212.5720648765564 seconds with total loss 0.6232414960861206\n",
      "2142 strategies were explored with an exploration rate of 0.31448682702792863\n",
      "Game number 2570 completed in 2224.1623678207397 seconds with total loss 0.4274542205035686\n",
      "2149 strategies were explored with an exploration rate of 0.31313736961055266\n",
      "Game number 2580 completed in 2235.202265739441 seconds with total loss 0.5948736824095249\n",
      "2156 strategies were explored with an exploration rate of 0.31170017393380023\n",
      "Game number 2590 completed in 2245.780474424362 seconds with total loss 0.6710634142160415\n",
      "2163 strategies were explored with an exploration rate of 0.3104558014226706\n",
      "Game number 2600 completed in 2256.5974814891815 seconds with total loss 0.5922537684440613\n",
      "2169 strategies were explored with an exploration rate of 0.3092163967077632\n",
      "Game number 2610 completed in 2267.256348848343 seconds with total loss 0.43305246233940126\n",
      "2174 strategies were explored with an exploration rate of 0.3079203466484205\n",
      "Game number 2620 completed in 2280.2539551258087 seconds with total loss 0.47442843914031985\n",
      "2177 strategies were explored with an exploration rate of 0.3066603948946896\n",
      "Game number 2630 completed in 2291.7083716392517 seconds with total loss 0.5712500460445881\n",
      "2184 strategies were explored with an exploration rate of 0.3053445205592941\n",
      "Game number 2640 completed in 2302.5136613845825 seconds with total loss 0.5577684134244919\n",
      "2190 strategies were explored with an exploration rate of 0.30403429261741866\n",
      "Game number 2650 completed in 2313.3128345012665 seconds with total loss 0.6801824413239956\n",
      "2196 strategies were explored with an exploration rate of 0.30285080899424877\n",
      "Game number 2660 completed in 2324.303262233734 seconds with total loss 0.545947726815939\n",
      "2199 strategies were explored with an exploration rate of 0.3015211263964791\n",
      "Game number 2670 completed in 2335.2088556289673 seconds with total loss 0.4824263259768486\n",
      "2202 strategies were explored with an exploration rate of 0.30016726211275035\n",
      "Game number 2680 completed in 2347.360421895981 seconds with total loss 0.5873480804264546\n",
      "2209 strategies were explored with an exploration rate of 0.29887924969538004\n",
      "Game number 2690 completed in 2358.5137231349945 seconds with total loss 0.5718001864850522\n",
      "2215 strategies were explored with an exploration rate of 0.2976265267692084\n",
      "Game number 2700 completed in 2369.278458595276 seconds with total loss 0.36530189514160155\n",
      "2220 strategies were explored with an exploration rate of 0.2963197816606386\n",
      "Game number 2710 completed in 2379.8881561756134 seconds with total loss 0.37740551605820655\n",
      "2227 strategies were explored with an exploration rate of 0.2950482787138981\n",
      "Game number 2720 completed in 2390.6626024246216 seconds with total loss 0.3906652592122555\n",
      "2232 strategies were explored with an exploration rate of 0.29384099702482763\n",
      "Game number 2730 completed in 2401.85142326355 seconds with total loss 0.5185406811535358\n",
      "2236 strategies were explored with an exploration rate of 0.29258013049915577\n",
      "Game number 2740 completed in 2412.5224175453186 seconds with total loss 0.33345401510596273\n",
      "2239 strategies were explored with an exploration rate of 0.2913538097104357\n",
      "Game number 2750 completed in 2423.135196685791 seconds with total loss 0.5071940734982491\n",
      "2243 strategies were explored with an exploration rate of 0.2902196861240073\n",
      "Game number 2760 completed in 2433.6561138629913 seconds with total loss 0.4363170564174652\n",
      "2246 strategies were explored with an exploration rate of 0.28906106822834304\n",
      "Game number 2770 completed in 2444.482709169388 seconds with total loss 0.36246134266257285\n",
      "2248 strategies were explored with an exploration rate of 0.2879070757784784\n",
      "Game number 2780 completed in 2455.3281269073486 seconds with total loss 0.509507866203785\n",
      "2252 strategies were explored with an exploration rate of 0.2867003416381783\n",
      "Game number 2790 completed in 2468.045407772064 seconds with total loss 0.3069666538387537\n",
      "2258 strategies were explored with an exploration rate of 0.2854986654051265\n",
      "Game number 2800 completed in 2479.734206676483 seconds with total loss 0.5959225475788117\n",
      "2263 strategies were explored with an exploration rate of 0.2844157751258442\n",
      "Game number 2810 completed in 2491.1168015003204 seconds with total loss 0.43205496594309806\n",
      "2267 strategies were explored with an exploration rate of 0.2831670325255786\n",
      "Game number 2820 completed in 2501.722576856613 seconds with total loss 0.45921255126595495\n",
      "2271 strategies were explored with an exploration rate of 0.28200836664619544\n",
      "Game number 2830 completed in 2512.5158393383026 seconds with total loss 0.34615320414304734\n",
      "2276 strategies were explored with an exploration rate of 0.28079827372838023\n",
      "Game number 2840 completed in 2523.3217284679413 seconds with total loss 0.513728243112564\n",
      "2278 strategies were explored with an exploration rate of 0.27950950367342353\n",
      "Game number 2850 completed in 2534.1193056106567 seconds with total loss 0.4743201985955238\n",
      "2283 strategies were explored with an exploration rate of 0.2783379671291789\n",
      "Game number 2860 completed in 2544.9938521385193 seconds with total loss 0.3413206279277802\n",
      "2287 strategies were explored with an exploration rate of 0.27708819787750044\n",
      "Game number 2870 completed in 2555.663876056671 seconds with total loss 0.29449813552200793\n",
      "2288 strategies were explored with an exploration rate of 0.27581645582843095\n",
      "Game number 2880 completed in 2566.3557257652283 seconds with total loss 0.5772530920803547\n",
      "2290 strategies were explored with an exploration rate of 0.2746329322905151\n",
      "Game number 2890 completed in 2577.2551250457764 seconds with total loss 0.39334671869874\n",
      "2293 strategies were explored with an exploration rate of 0.27339979906828493\n",
      "Game number 2900 completed in 2588.210441350937 seconds with total loss 0.4199929729104042\n",
      "2296 strategies were explored with an exploration rate of 0.2723627995280292\n",
      "Game number 2910 completed in 2600.3715314865112 seconds with total loss 0.3935934595763683\n",
      "2298 strategies were explored with an exploration rate of 0.27108563420060156\n",
      "Game number 2920 completed in 2612.3979613780975 seconds with total loss 0.3398811973631382\n",
      "2300 strategies were explored with an exploration rate of 0.2697874763189657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 2930 completed in 2623.0526719093323 seconds with total loss 0.42849722430109977\n",
      "2303 strategies were explored with an exploration rate of 0.2686029600432822\n",
      "Game number 2940 completed in 2633.97288441658 seconds with total loss 0.41584568843245506\n",
      "2306 strategies were explored with an exploration rate of 0.2673434253784668\n",
      "Game number 2950 completed in 2645.209620475769 seconds with total loss 0.34032911211252215\n",
      "2310 strategies were explored with an exploration rate of 0.2661430228745518\n",
      "Game number 2960 completed in 2656.2331392765045 seconds with total loss 0.4068708911538124\n",
      "2315 strategies were explored with an exploration rate of 0.26497450776571224\n",
      "Game number 2970 completed in 2666.9829666614532 seconds with total loss 0.38934386894106865\n",
      "2322 strategies were explored with an exploration rate of 0.2638638932246643\n",
      "Game number 2980 completed in 2677.6482248306274 seconds with total loss 0.38845999278128146\n",
      "2322 strategies were explored with an exploration rate of 0.2627842121362489\n",
      "Game number 2990 completed in 2688.1038155555725 seconds with total loss 0.4255479495972395\n",
      "2324 strategies were explored with an exploration rate of 0.26176129854050467\n",
      "Game number 3000 completed in 2702.770635843277 seconds with total loss 0.4092705503106117\n",
      "2325 strategies were explored with an exploration rate of 0.2605599018273898\n",
      "Game number 3010 completed in 2715.1581354141235 seconds with total loss 0.3219762608408928\n",
      "2325 strategies were explored with an exploration rate of 0.2595196920153622\n",
      "Game number 3020 completed in 2727.3258726596832 seconds with total loss 0.3539628092199564\n",
      "2327 strategies were explored with an exploration rate of 0.2584577865757202\n",
      "Game number 3030 completed in 2738.964471101761 seconds with total loss 0.4702549919486046\n",
      "2330 strategies were explored with an exploration rate of 0.2573230139059389\n",
      "Game number 3040 completed in 2751.2335801124573 seconds with total loss 0.37532614544034004\n",
      "2332 strategies were explored with an exploration rate of 0.25624446984749366\n",
      "Game number 3050 completed in 2760.6695199012756 seconds with total loss 0.3465704955160618\n",
      "2335 strategies were explored with an exploration rate of 0.2551449293555594\n",
      "Game number 3060 completed in 2771.2550756931305 seconds with total loss 0.45215205326676366\n",
      "2339 strategies were explored with an exploration rate of 0.2540501069724455\n",
      "Game number 3070 completed in 2781.542382955551 seconds with total loss 0.35402656346559525\n",
      "2342 strategies were explored with an exploration rate of 0.252884102046641\n",
      "Game number 3080 completed in 2792.327167749405 seconds with total loss 0.28826524391770364\n",
      "2344 strategies were explored with an exploration rate of 0.2518241632498828\n",
      "Game number 3090 completed in 2808.7427990436554 seconds with total loss 0.3740459345281124\n",
      "2346 strategies were explored with an exploration rate of 0.25081882833963376\n",
      "Game number 3100 completed in 2819.7617604732513 seconds with total loss 0.3512991897761822\n",
      "2352 strategies were explored with an exploration rate of 0.24979252518664313\n",
      "Game number 3110 completed in 2830.5476212501526 seconds with total loss 0.4155401669442654\n",
      "2355 strategies were explored with an exploration rate of 0.24869579780824633\n",
      "Game number 3120 completed in 2841.190459251404 seconds with total loss 0.43001088574528695\n",
      "2362 strategies were explored with an exploration rate of 0.24775250001187046\n",
      "Game number 3130 completed in 2851.7849690914154 seconds with total loss 0.338332924246788\n",
      "2364 strategies were explored with an exploration rate of 0.246566078379703\n",
      "Game number 3140 completed in 2864.8028354644775 seconds with total loss 0.42797848880290984\n",
      "2369 strategies were explored with an exploration rate of 0.24550806769452677\n",
      "Game number 3150 completed in 2876.0025119781494 seconds with total loss 0.3929422996938229\n",
      "2373 strategies were explored with an exploration rate of 0.24445459691450416\n",
      "Game number 3160 completed in 2887.2803976535797 seconds with total loss 0.2352810688316822\n",
      "2374 strategies were explored with an exploration rate of 0.24350303316303956\n",
      "Game number 3170 completed in 2900.911780834198 seconds with total loss 0.31874851807951926\n",
      "2376 strategies were explored with an exploration rate of 0.24245816595075978\n",
      "Game number 3180 completed in 2914.9193580150604 seconds with total loss 0.27734928503632544\n",
      "2377 strategies were explored with an exploration rate of 0.24144192643786755\n",
      "Game number 3190 completed in 2925.79128074646 seconds with total loss 0.31223660968244077\n",
      "2380 strategies were explored with an exploration rate of 0.24050208980488158\n",
      "Game number 3200 completed in 2937.7803852558136 seconds with total loss 0.2673973612487316\n",
      "2380 strategies were explored with an exploration rate of 0.23932645344349443\n",
      "Game number 3210 completed in 2951.883662223816 seconds with total loss 0.3812060318887234\n",
      "2384 strategies were explored with an exploration rate of 0.23834717491249632\n",
      "Game number 3220 completed in 2962.973571538925 seconds with total loss 0.27755017057061193\n",
      "2387 strategies were explored with an exploration rate of 0.23730069895371914\n",
      "Game number 3230 completed in 2973.897187232971 seconds with total loss 0.3838000290095806\n",
      "2389 strategies were explored with an exploration rate of 0.23623519172158297\n",
      "Game number 3240 completed in 2984.6802818775177 seconds with total loss 0.5148702636361122\n",
      "2393 strategies were explored with an exploration rate of 0.23517446873857353\n",
      "Game number 3250 completed in 2995.7632908821106 seconds with total loss 0.3625961448997259\n",
      "2395 strategies were explored with an exploration rate of 0.23407168716234658\n",
      "Game number 3260 completed in 3006.696422815323 seconds with total loss 0.5198747634887695\n",
      "2399 strategies were explored with an exploration rate of 0.2331838583021884\n",
      "Game number 3270 completed in 3017.825254917145 seconds with total loss 0.4893568014726043\n",
      "2403 strategies were explored with an exploration rate of 0.23222971410740734\n",
      "Game number 3280 completed in 3032.444578409195 seconds with total loss 0.3515873573720455\n",
      "2406 strategies were explored with an exploration rate of 0.23121009718593005\n",
      "Game number 3290 completed in 3047.59051656723 seconds with total loss 0.3996282286942005\n",
      "2411 strategies were explored with an exploration rate of 0.23014892025738135\n",
      "Game number 3300 completed in 3062.2166125774384 seconds with total loss 0.28813412934541704\n",
      "2415 strategies were explored with an exploration rate of 0.22916135530759488\n",
      "Game number 3310 completed in 3072.317132949829 seconds with total loss 0.2857655595988035\n",
      "2417 strategies were explored with an exploration rate of 0.22817802798155626\n",
      "Game number 3320 completed in 3085.3144478797913 seconds with total loss 0.30266736075282097\n",
      "2419 strategies were explored with an exploration rate of 0.22724436669659276\n",
      "Game number 3330 completed in 3099.1326327323914 seconds with total loss 0.28558345325291157\n",
      "2419 strategies were explored with an exploration rate of 0.2262466382082948\n",
      "Game number 3340 completed in 3115.3453743457794 seconds with total loss 0.36170811057090757\n",
      "2421 strategies were explored with an exploration rate of 0.22525329030003424\n",
      "Game number 3350 completed in 3126.0176877975464 seconds with total loss 0.3101164635270834\n",
      "2425 strategies were explored with an exploration rate of 0.22415219401095965\n",
      "Game number 3360 completed in 3136.3614189624786 seconds with total loss 0.3116730563342571\n",
      "2427 strategies were explored with an exploration rate of 0.22305648016507248\n",
      "Game number 3370 completed in 3147.090002298355 seconds with total loss 0.22141137570142747\n",
      "2427 strategies were explored with an exploration rate of 0.2220327256085225\n",
      "Game number 3380 completed in 3158.3474509716034 seconds with total loss 0.3455663798376918\n",
      "2429 strategies were explored with an exploration rate of 0.2211242097372557\n",
      "Game number 3390 completed in 3167.2164850234985 seconds with total loss 0.4086951188743114\n",
      "2431 strategies were explored with an exploration rate of 0.220131336788654\n",
      "Game number 3400 completed in 3175.3599956035614 seconds with total loss 0.3121917363256216\n",
      "2431 strategies were explored with an exploration rate of 0.21918675711250782\n",
      "Game number 3410 completed in 3183.506161928177 seconds with total loss 0.25675338953733445\n",
      "2435 strategies were explored with an exploration rate of 0.21824623061106002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 3420 completed in 3191.2770552635193 seconds with total loss 0.28876030892133714\n",
      "2440 strategies were explored with an exploration rate of 0.21730973989220953\n",
      "Game number 3430 completed in 3198.9695076942444 seconds with total loss 0.23363870568573475\n",
      "2442 strategies were explored with an exploration rate of 0.21644219380357624\n",
      "Game number 3440 completed in 3206.9018297195435 seconds with total loss 0.1990511853247881\n",
      "2444 strategies were explored with an exploration rate of 0.21549189283036949\n",
      "Game number 3450 completed in 3214.751481771469 seconds with total loss 0.20981490947306156\n",
      "2446 strategies were explored with an exploration rate of 0.21456722092626315\n",
      "Game number 3460 completed in 3222.854291677475 seconds with total loss 0.19047650173306466\n",
      "2448 strategies were explored with an exploration rate of 0.21351836090559825\n",
      "Game number 3470 completed in 3231.172473669052 seconds with total loss 0.294950070604682\n",
      "2449 strategies were explored with an exploration rate of 0.21251712928442837\n",
      "Game number 3480 completed in 3239.2498037815094 seconds with total loss 0.3900121796876192\n",
      "2454 strategies were explored with an exploration rate of 0.2116475494315403\n",
      "Game number 3490 completed in 3250.774532556534 seconds with total loss 0.313798139244318\n",
      "2455 strategies were explored with an exploration rate of 0.21080260799555486\n",
      "Game number 3500 completed in 3260.4455556869507 seconds with total loss 0.5093600895255804\n",
      "2458 strategies were explored with an exploration rate of 0.20994004363913618\n",
      "Game number 3510 completed in 3269.78790307045 seconds with total loss 0.49976662881672385\n",
      "2461 strategies were explored with an exploration rate of 0.20901829070233863\n",
      "Game number 3520 completed in 3279.3107826709747 seconds with total loss 0.4341896533966064\n",
      "2463 strategies were explored with an exploration rate of 0.2080797747123983\n",
      "Game number 3530 completed in 3288.6178081035614 seconds with total loss 0.2725114326924086\n",
      "2465 strategies were explored with an exploration rate of 0.20710404574330946\n",
      "Game number 3540 completed in 3298.347268342972 seconds with total loss 0.32416711188852787\n",
      "2468 strategies were explored with an exploration rate of 0.2061535075193012\n",
      "Game number 3550 completed in 3308.068051099777 seconds with total loss 0.3934036113321781\n",
      "2470 strategies were explored with an exploration rate of 0.20520733194746638\n",
      "Game number 3560 completed in 3319.5524044036865 seconds with total loss 0.32753687016665933\n",
      "2472 strategies were explored with an exploration rate of 0.20434722563493019\n",
      "Game number 3570 completed in 3329.096645116806 seconds with total loss 0.12879326976835728\n",
      "2475 strategies were explored with an exploration rate of 0.20338899935810664\n",
      "Game number 3580 completed in 3342.720939397812 seconds with total loss 0.22663646154105663\n",
      "2475 strategies were explored with an exploration rate of 0.20249600912993984\n",
      "Game number 3590 completed in 3352.1228363513947 seconds with total loss 0.20717964991927146\n",
      "2476 strategies were explored with an exploration rate of 0.20158677892894186\n",
      "Game number 3600 completed in 3361.522382736206 seconds with total loss 0.447304368019104\n",
      "2481 strategies were explored with an exploration rate of 0.20070170144550237\n",
      "Game number 3610 completed in 3370.9127340316772 seconds with total loss 0.324488353356719\n",
      "2484 strategies were explored with an exploration rate of 0.199820509941867\n",
      "Game number 3620 completed in 3380.3702449798584 seconds with total loss 0.4195630311965942\n",
      "2485 strategies were explored with an exploration rate of 0.19894318735643454\n",
      "Game number 3630 completed in 3389.9524965286255 seconds with total loss 0.48623303733766077\n",
      "2486 strategies were explored with an exploration rate of 0.19810933658873803\n",
      "Game number 3640 completed in 3399.5347924232483 seconds with total loss 0.3080820195376873\n",
      "2488 strategies were explored with an exploration rate of 0.19720008106799392\n",
      "Game number 3650 completed in 3408.892469406128 seconds with total loss 0.21432867236435413\n",
      "2489 strategies were explored with an exploration rate of 0.1962949987256384\n",
      "Game number 3660 completed in 3417.992486476898 seconds with total loss 0.41776374317705633\n",
      "2493 strategies were explored with an exploration rate of 0.19543315508486536\n",
      "Game number 3670 completed in 3426.1512084007263 seconds with total loss 0.21362763978540897\n",
      "2495 strategies were explored with an exploration rate of 0.19453618234114153\n",
      "Game number 3680 completed in 3436.188465356827 seconds with total loss 0.26326628029346466\n",
      "2498 strategies were explored with an exploration rate of 0.19362396206933202\n",
      "Game number 3690 completed in 3445.6068291664124 seconds with total loss 0.5521502211689949\n",
      "2500 strategies were explored with an exploration rate of 0.19279312506968588\n",
      "Game number 3700 completed in 3455.0631453990936 seconds with total loss 0.5099998116493225\n",
      "2502 strategies were explored with an exploration rate of 0.19194665659160554\n",
      "Game number 3710 completed in 3466.350116252899 seconds with total loss 0.2892048083245754\n",
      "2503 strategies were explored with an exploration rate of 0.19104657913998851\n",
      "Game number 3720 completed in 3477.0230062007904 seconds with total loss 0.28913040794432165\n",
      "2507 strategies were explored with an exploration rate of 0.19024582622756908\n",
      "Game number 3730 completed in 3489.9137189388275 seconds with total loss 0.34242624156177043\n",
      "2510 strategies were explored with an exploration rate of 0.1893916007464687\n",
      "Game number 3740 completed in 3512.576888561249 seconds with total loss 0.20813163965940476\n",
      "2511 strategies were explored with an exploration rate of 0.1885223567142262\n",
      "Game number 3750 completed in 3522.476845741272 seconds with total loss 0.2225975377485156\n",
      "2513 strategies were explored with an exploration rate of 0.1877885147523478\n",
      "Game number 3760 completed in 3534.146599292755 seconds with total loss 0.4136637795716524\n",
      "2515 strategies were explored with an exploration rate of 0.18687055596715443\n",
      "Game number 3770 completed in 3546.160928249359 seconds with total loss 0.5743407964706421\n",
      "2516 strategies were explored with an exploration rate of 0.1860873064443954\n",
      "Game number 3780 completed in 3556.7156076431274 seconds with total loss 0.32692934274673463\n",
      "2520 strategies were explored with an exploration rate of 0.18525175319192094\n",
      "Game number 3790 completed in 3566.9232189655304 seconds with total loss 0.19967537708580493\n",
      "2522 strategies were explored with an exploration rate of 0.18445684119261962\n",
      "Game number 3800 completed in 3577.647250175476 seconds with total loss 0.11101513989269733\n",
      "2522 strategies were explored with an exploration rate of 0.18357352584128422\n",
      "Game number 3810 completed in 3587.938221216202 seconds with total loss 0.40054129585623743\n",
      "2525 strategies were explored with an exploration rate of 0.1827127117262714\n",
      "Game number 3820 completed in 3598.0899465084076 seconds with total loss 0.14834616146981716\n",
      "2528 strategies were explored with an exploration rate of 0.18194688939797074\n",
      "Game number 3830 completed in 3608.1383004188538 seconds with total loss 0.1957110997289419\n",
      "2530 strategies were explored with an exploration rate of 0.1812023971782196\n",
      "Game number 3840 completed in 3618.5385563373566 seconds with total loss 0.19131714925169946\n",
      "2531 strategies were explored with an exploration rate of 0.18037073884831273\n",
      "Game number 3850 completed in 3628.726701974869 seconds with total loss 0.3721062883734703\n",
      "2534 strategies were explored with an exploration rate of 0.17959677119530582\n",
      "Game number 3860 completed in 3639.0255093574524 seconds with total loss 0.3676047772169113\n",
      "2536 strategies were explored with an exploration rate of 0.1787724821510387\n",
      "Game number 3870 completed in 3649.256111383438 seconds with total loss 0.40830819942057134\n",
      "2540 strategies were explored with an exploration rate of 0.17800537258927643\n",
      "Game number 3880 completed in 3659.7524602413177 seconds with total loss 0.23817330971360207\n",
      "2540 strategies were explored with an exploration rate of 0.17717066869299974\n",
      "Game number 3890 completed in 3670.010385274887 seconds with total loss 0.1343995276838541\n",
      "2541 strategies were explored with an exploration rate of 0.17635751464687097\n",
      "Game number 3900 completed in 3678.201989889145 seconds with total loss 0.26896153427660463\n",
      "2548 strategies were explored with an exploration rate of 0.17556564927046747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 3910 completed in 3686.1251652240753 seconds with total loss 0.2274445801973343\n",
      "2549 strategies were explored with an exploration rate of 0.17472491150132277\n",
      "Game number 3920 completed in 3693.9898562431335 seconds with total loss 0.329318792745471\n",
      "2551 strategies were explored with an exploration rate of 0.17390559036350942\n",
      "Game number 3930 completed in 3701.9304683208466 seconds with total loss 0.32457045242190363\n",
      "2554 strategies were explored with an exploration rate of 0.17314204861194815\n",
      "Game number 3940 completed in 3709.838301420212 seconds with total loss 0.21245211362838745\n",
      "2555 strategies were explored with an exploration rate of 0.17231291682962038\n",
      "Game number 3950 completed in 3717.695582151413 seconds with total loss 0.16274898126721382\n",
      "2556 strategies were explored with an exploration rate of 0.17152205823799274\n",
      "Game number 3960 completed in 3725.502264738083 seconds with total loss 0.309716010093689\n",
      "2560 strategies were explored with an exploration rate of 0.17078606011711686\n",
      "Game number 3970 completed in 3733.378214120865 seconds with total loss 0.3177378837019205\n",
      "2561 strategies were explored with an exploration rate of 0.169985209065624\n",
      "Game number 3980 completed in 3741.2140419483185 seconds with total loss 0.516743964701891\n",
      "2566 strategies were explored with an exploration rate of 0.16922195606883897\n",
      "Game number 3990 completed in 3748.995744228363 seconds with total loss 0.2785441342741251\n",
      "2570 strategies were explored with an exploration rate of 0.1684621301651541\n",
      "Game number 4000 completed in 3756.915378332138 seconds with total loss 0.18604643754661082\n",
      "2573 strategies were explored with an exploration rate of 0.16768894539493331\n",
      "Game number 4010 completed in 3764.764284849167 seconds with total loss 0.17503706514835357\n",
      "2574 strategies were explored with an exploration rate of 0.16688592709125377\n",
      "Game number 4020 completed in 3772.581559419632 seconds with total loss 0.1681634834036231\n",
      "2575 strategies were explored with an exploration rate of 0.16616982252720178\n",
      "Game number 4030 completed in 3780.4602909088135 seconds with total loss 0.27564420476555823\n",
      "2578 strategies were explored with an exploration rate of 0.16544024507610405\n",
      "Game number 4040 completed in 3788.33087348938 seconds with total loss 0.2193400079384446\n",
      "2579 strategies were explored with an exploration rate of 0.1646809297466052\n",
      "Game number 4050 completed in 3796.159169435501 seconds with total loss 0.34258373770862816\n",
      "2581 strategies were explored with an exploration rate of 0.16394149356957935\n",
      "Game number 4060 completed in 3806.491408109665 seconds with total loss 0.1730107543990016\n",
      "2582 strategies were explored with an exploration rate of 0.16320537754541353\n",
      "Game number 4070 completed in 3816.722441673279 seconds with total loss 0.07668087147176265\n",
      "2583 strategies were explored with an exploration rate of 0.16242382987022697\n",
      "Game number 4080 completed in 3827.0914084911346 seconds with total loss 0.11275179218500853\n",
      "2583 strategies were explored with an exploration rate of 0.1616945283293506\n",
      "Game number 4090 completed in 3836.22181725502 seconds with total loss 0.11024352125823497\n",
      "2584 strategies were explored with an exploration rate of 0.160904123692553\n",
      "Game number 4100 completed in 3846.44428563118 seconds with total loss 0.2568152444437146\n",
      "2586 strategies were explored with an exploration rate of 0.1601816458059271\n",
      "Game number 4110 completed in 3856.7323083877563 seconds with total loss 0.1093500405550003\n",
      "2589 strategies were explored with an exploration rate of 0.15946241192750116\n",
      "Game number 4120 completed in 3867.3022689819336 seconds with total loss 0.22042503952980042\n",
      "2592 strategies were explored with an exploration rate of 0.15877816153583346\n",
      "Game number 4130 completed in 3878.2917215824127 seconds with total loss 0.21975771840661765\n",
      "2594 strategies were explored with an exploration rate of 0.15803361799678317\n",
      "Game number 4140 completed in 3888.478722333908 seconds with total loss 0.06598954014480114\n",
      "2594 strategies were explored with an exploration rate of 0.15727683651933258\n",
      "Game number 4150 completed in 3898.7892050743103 seconds with total loss 0.19530789125710726\n",
      "2597 strategies were explored with an exploration rate of 0.15655498850170366\n",
      "Game number 4160 completed in 3909.057188510895 seconds with total loss 0.16506424602121114\n",
      "2599 strategies were explored with an exploration rate of 0.15585203872864767\n",
      "Game number 4170 completed in 3919.327181816101 seconds with total loss 0.3038973480463028\n",
      "2600 strategies were explored with an exploration rate of 0.15515224528033197\n",
      "Game number 4180 completed in 3929.4882209300995 seconds with total loss 0.19981822203844785\n",
      "2601 strategies were explored with an exploration rate of 0.1544555939844983\n",
      "Game number 4190 completed in 3939.8193080425262 seconds with total loss 0.2542939383536577\n",
      "2603 strategies were explored with an exploration rate of 0.15377744847737088\n",
      "Game number 4200 completed in 3950.2806854248047 seconds with total loss 0.2691030170768499\n",
      "2605 strategies were explored with an exploration rate of 0.15310228040419052\n",
      "Game number 4210 completed in 3960.808617591858 seconds with total loss 0.16782615333795547\n",
      "2606 strategies were explored with an exploration rate of 0.15236911380689286\n",
      "Game number 4220 completed in 3971.2131674289703 seconds with total loss 0.5235928382724524\n",
      "2608 strategies were explored with an exploration rate of 0.1517608243267378\n",
      "Game number 4230 completed in 3981.5770559310913 seconds with total loss 0.42131267338991163\n",
      "2610 strategies were explored with an exploration rate of 0.15101897820200214\n",
      "Game number 4240 completed in 3991.580510854721 seconds with total loss 0.4270965099334717\n",
      "2612 strategies were explored with an exploration rate of 0.1504160787465359\n",
      "Game number 4250 completed in 4001.7222855091095 seconds with total loss 0.3597924433648586\n",
      "2616 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4260 completed in 4011.919130563736 seconds with total loss 0.09523572027683258\n",
      "2616 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4270 completed in 4022.283135175705 seconds with total loss 0.17177439499646424\n",
      "2616 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4280 completed in 4032.3645997047424 seconds with total loss 0.1610885987058282\n",
      "2617 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4290 completed in 4042.528369665146 seconds with total loss 0.16020376347005366\n",
      "2617 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4300 completed in 4052.748863220215 seconds with total loss 0.12333790306001902\n",
      "2618 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4310 completed in 4063.023418903351 seconds with total loss 0.12918273359537125\n",
      "2619 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4320 completed in 4073.148200273514 seconds with total loss 0.18467338625341653\n",
      "2620 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4330 completed in 4083.3273837566376 seconds with total loss 0.15353244300931693\n",
      "2622 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4340 completed in 4093.5125114917755 seconds with total loss 0.1824014589190483\n",
      "2622 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4350 completed in 4103.742177963257 seconds with total loss 0.3495217764750123\n",
      "2625 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4360 completed in 4113.887392997742 seconds with total loss 0.4726930264383554\n",
      "2629 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4370 completed in 4124.193811893463 seconds with total loss 0.5445936322212219\n",
      "2633 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4380 completed in 4134.424253702164 seconds with total loss 0.34467699825763704\n",
      "2635 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4390 completed in 4146.370576620102 seconds with total loss 0.5400207981467247\n",
      "2639 strategies were explored with an exploration rate of 0.14999548180636801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 4400 completed in 4158.0884346961975 seconds with total loss 0.7259138241410256\n",
      "2645 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4410 completed in 4167.977702379227 seconds with total loss 0.44546606130898\n",
      "2647 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4420 completed in 4178.391187906265 seconds with total loss 0.2362911246716976\n",
      "2648 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4430 completed in 4188.639376878738 seconds with total loss 0.13676571995019912\n",
      "2648 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4440 completed in 4199.2345662117 seconds with total loss 0.26361405849456787\n",
      "2650 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4450 completed in 4208.0580224990845 seconds with total loss 0.13131762202829123\n",
      "2651 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4460 completed in 4216.077525854111 seconds with total loss 0.11055321414023637\n",
      "2653 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4470 completed in 4223.887795209885 seconds with total loss 0.16837241388857366\n",
      "2655 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4480 completed in 4231.681041717529 seconds with total loss 0.2148963574320078\n",
      "2658 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4490 completed in 4240.829421758652 seconds with total loss 0.4749764449894428\n",
      "2659 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4500 completed in 4249.548184394836 seconds with total loss 0.3164603650569916\n",
      "2661 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4510 completed in 4257.577430725098 seconds with total loss 0.19685388542711735\n",
      "2662 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4520 completed in 4265.427670955658 seconds with total loss 0.2072130639106035\n",
      "2665 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4530 completed in 4273.372895479202 seconds with total loss 0.19574439525604248\n",
      "2665 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4540 completed in 4283.652719259262 seconds with total loss 0.14944237433373928\n",
      "2667 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4550 completed in 4295.196090698242 seconds with total loss 0.3182558134198189\n",
      "2667 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4560 completed in 4305.599143266678 seconds with total loss 0.11452406104654074\n",
      "2668 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4570 completed in 4316.542630195618 seconds with total loss 0.15890958439558744\n",
      "2669 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4580 completed in 4327.123407363892 seconds with total loss 0.08837305717170238\n",
      "2669 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4590 completed in 4336.936999320984 seconds with total loss 0.059059379249811174\n",
      "2669 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4600 completed in 4347.299115180969 seconds with total loss 0.18868779614567757\n",
      "2670 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4610 completed in 4357.649169921875 seconds with total loss 0.04547877013683319\n",
      "2671 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4620 completed in 4368.176184654236 seconds with total loss 0.3113400826230645\n",
      "2675 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4630 completed in 4378.571946144104 seconds with total loss 0.1362860407680273\n",
      "2676 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4640 completed in 4388.743473768234 seconds with total loss 0.07330380156636238\n",
      "2676 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4650 completed in 4398.852146148682 seconds with total loss 0.0712953956797719\n",
      "2676 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4660 completed in 4408.9858140945435 seconds with total loss 0.1339158546179533\n",
      "2677 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4670 completed in 4419.434141159058 seconds with total loss 0.04784163795411587\n",
      "2677 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4680 completed in 4429.594838380814 seconds with total loss 0.23107971716672182\n",
      "2677 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4690 completed in 4439.86271572113 seconds with total loss 0.17191535085439683\n",
      "2678 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4700 completed in 4450.361724853516 seconds with total loss 0.2725014867261052\n",
      "2680 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4710 completed in 4462.177373886108 seconds with total loss 0.15161574203521014\n",
      "2682 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4720 completed in 4472.509379863739 seconds with total loss 0.05459077544510364\n",
      "2682 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4730 completed in 4482.934637546539 seconds with total loss 0.055586723424494264\n",
      "2683 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4740 completed in 4493.1819813251495 seconds with total loss 0.2343317897990346\n",
      "2685 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4750 completed in 4503.349419116974 seconds with total loss 0.16226239427924155\n",
      "2685 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4760 completed in 4513.474431037903 seconds with total loss 0.4148117460310459\n",
      "2686 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4770 completed in 4523.584208488464 seconds with total loss 0.5599439583718777\n",
      "2691 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4780 completed in 4533.757661581039 seconds with total loss 0.3538727965205908\n",
      "2693 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4790 completed in 4543.995396852493 seconds with total loss 0.1106333963572979\n",
      "2694 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4800 completed in 4554.62205696106 seconds with total loss 0.19611933473497628\n",
      "2694 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4810 completed in 4564.98544216156 seconds with total loss 0.21057386919856072\n",
      "2694 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4820 completed in 4575.336931467056 seconds with total loss 0.10950450934469699\n",
      "2694 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4830 completed in 4585.335828065872 seconds with total loss 0.14058019109070302\n",
      "2694 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4840 completed in 4595.650648832321 seconds with total loss 0.04912167228758335\n",
      "2695 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4850 completed in 4605.973168849945 seconds with total loss 0.26343126855790616\n",
      "2695 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4860 completed in 4617.085560083389 seconds with total loss 0.41601256728172303\n",
      "2696 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4870 completed in 4627.769218683243 seconds with total loss 0.3462680421769619\n",
      "2699 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4880 completed in 4638.251914978027 seconds with total loss 0.5196598820388317\n",
      "2705 strategies were explored with an exploration rate of 0.14999548180636801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 4890 completed in 4648.659229516983 seconds with total loss 0.44177092537283896\n",
      "2709 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4900 completed in 4658.963869571686 seconds with total loss 0.18763784915208817\n",
      "2709 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4910 completed in 4669.111378669739 seconds with total loss 0.17946300953626632\n",
      "2709 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4920 completed in 4679.525584459305 seconds with total loss 0.2153186745941639\n",
      "2710 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4930 completed in 4689.971220254898 seconds with total loss 0.17570740040391683\n",
      "2712 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4940 completed in 4700.121260404587 seconds with total loss 0.11130333766341209\n",
      "2712 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4950 completed in 4711.269283056259 seconds with total loss 0.4123280342668295\n",
      "2714 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4960 completed in 4721.742748498917 seconds with total loss 0.3153548026457429\n",
      "2715 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4970 completed in 4731.955099821091 seconds with total loss 0.16005191076546907\n",
      "2716 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4980 completed in 4742.293953180313 seconds with total loss 0.24159032106399536\n",
      "2716 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 4990 completed in 4753.33392739296 seconds with total loss 0.11477354355156422\n",
      "2717 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "Game number 5000 completed in 4763.846886873245 seconds with total loss 0.4262479741126299\n",
      "2719 strategies were explored with an exploration rate of 0.14999548180636801\n",
      "4763.84773182869\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "agents = [Minimax(Player.X, max_depth = 3), Minimax(Player.O, max_depth = 3)]\n",
    "games = 5000\n",
    "\n",
    "strats = Strategy()\n",
    "\n",
    "for g in range(1, games+1):\n",
    "    \n",
    "    env = BG()\n",
    "    s, r, d, m = env.reset()\n",
    "    \n",
    "    index = 0\n",
    "\n",
    "    while not d: \n",
    "        #print(f\"Player {agents[index].__class__} {agents[index].player} turn\")\n",
    "        a = agents[index].choose_action(env)\n",
    "        s, r, d, m = env.step(a, agents[index].player)\n",
    "        #env.render()\n",
    "\n",
    "        index = 1 - index\n",
    "        \n",
    "    agents[0].reset_metric()\n",
    "    agents[1].reset_metric()\n",
    "\n",
    "    strats.add_strat(env.actions_taken)\n",
    "    \n",
    "    if g % 10 == 0: \n",
    "        l = (sum(agents[0].loss_h[-10:]) + sum(agents[1].loss_h[-10:]))/20\n",
    "        print(f\"Game number {g} completed in {time.time() - start} seconds with total loss {l}\")\n",
    "        print(f\"{strats.get_total} strategies were explored with an exploration rate of {agents[0].epsilon}\")\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_strats = list(set(strats.all_strats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'used_strats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f7a8a9de5d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mused_strats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'used_strats' is not defined"
     ]
    }
   ],
   "source": [
    "e = BG()\n",
    "p = [Player.X, Player.O]\n",
    "i = 0\n",
    "\n",
    "for l in used_strats:\n",
    "    l = str(l)\n",
    "    for c in l: \n",
    "        e.step(int(c), p[i])\n",
    "        i = 1-i\n",
    "    e.render()\n",
    "    e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3929505944252014), (1, 0.3929505944252014), (2, 0.07110754400491714), (3, -0.02802792750298977), (4, 0.2469054013490677), (5, -0.1680048108100891), (6, 0.022360941395163536), (7, -0.05789629742503166), (8, -0.010488195344805717)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Minimax(Player.X, max_depth = 3, training = False, debug = True, explore = False)\n",
    "e = BG()\n",
    "m.choose_action(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a4b998d940>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = agents[0].loss_h\n",
    "x2 = agents[1].loss_h\n",
    "\n",
    "plt.plot(np.arange(len(x)), x)\n",
    "plt.plot(np.arange(len(x2)), x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "value_network = keras.models.load_model(\"larger_value_net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[-1.],\n",
       "       [-1.],\n",
       "       [-1.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(r, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "t = tf.keras.models.load_model('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = USE_LATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inp (InputLayer)             [(None, 18)]              0         \n",
      "_________________________________________________________________\n",
      "layer_1 (Dense)              (None, 50)                950       \n",
      "_________________________________________________________________\n",
      "layer_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "value (Dense)                (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,251\n",
      "Trainable params: 2,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "value_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | | \n",
      "-----\n",
      " |O| \n",
      "-----\n",
      "X| |X\n",
      "\n",
      "[(0, nan), (1, nan), (2, nan), (3, nan), (5, nan), (7, nan)]\n",
      "[(0, nan), (1, nan), (2, nan), (3, nan), (5, nan), (7, nan)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = BG()\n",
    "test.step(6, Player.X)\n",
    "test.step(4, Player.O)\n",
    "test.step(8, Player.X)\n",
    "#test.step(0, Player.O)\n",
    "\n",
    "m2 = Minimax(Player.O, max_depth = 1, debug = True, training = False)\n",
    "m = Minimax(Player.O, max_depth = 2, debug = True, training = False)\n",
    "\n",
    "test.render()\n",
    "\n",
    "m2.choose_action(test)\n",
    "m.choose_action(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Minimax"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = agents2[1]\n",
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(18,), dtype=float64, numpy=\n",
       " array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "        0.])>,\n",
       " <tf.Tensor: shape=(18,), dtype=float64, numpy=\n",
       " array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "        0.])>,\n",
       " <tf.Tensor: shape=(18,), dtype=float64, numpy=\n",
       " array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "        0.])>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 18), dtype=float64, numpy=\n",
       " array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         0., 0.]])>,\n",
       " <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       " array([[-0.99413484],\n",
       "        [-0.99961734],\n",
       "        [-0.9999187 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.008788705, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "s = tf.stack(m.states)\n",
    "r = tf.expand_dims(tf.stack(m.rewards), axis = 1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    r_pred = value_network(s)\n",
    "    loss = tf.norm(keras.losses.MSE(r, r_pred))\n",
    "        \n",
    "#print(loss, r_pred, r)\n",
    "gradient2 = tape.gradient(loss, value_network.trainable_weights)\n",
    "#m.optimizer.apply_gradients(zip(gradient, value_network.trainable_weights))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.7195134e-10], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    r_pred = value_network(s)\n",
    "    loss = m.loss_fn(r, r_pred)\n",
    "    \n",
    "print(loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(82, 18)\n",
      "    r: float32 Tensor, shape=(82,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(82,)\n",
      "\n",
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(22, 18)\n",
      "    r: float32 Tensor, shape=(22,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(22,)\n",
      "\n",
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(50, 18)\n",
      "    r: float32 Tensor, shape=(50,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(50,)\n",
      "\n",
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(10, 18)\n",
      "    r: float32 Tensor, shape=(10,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(10,)\n",
      "\n",
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(82, 18)\n",
      "    r: float32 Tensor, shape=(82,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(82,)\n",
      "\n",
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(1, 18)\n",
      "    r: int32 Tensor, shape=(1,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(1,)\n",
      "\n",
      "update(s, r)\n",
      "  Args:\n",
      "    s: float64 Tensor, shape=(8, 18)\n",
      "    r: float32 Tensor, shape=(8,)\n",
      "  Returns:\n",
      "    float32 Tensor, shape=(8,)\n"
     ]
    }
   ],
   "source": [
    "print(agents[0].update.pretty_printed_concrete_signatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.7653072]], shape=(1, 1), dtype=float32)\n",
      "X|O| \n",
      "-----\n",
      "X|O| \n",
      "-----\n",
      " | | \n",
      "\n",
      "[(2, -0.9972026348114014), (5, -0.9897016882896423), (6, 1.0), (7, -0.9968457818031311), (8, -0.9954777956008911)]\n",
      "6\n",
      "[(2, -1.0), (5, -1.0), (6, 1.0), (7, 0.0), (8, -1.0)]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "e = BG()\n",
    "e.step(3, Player.X)\n",
    "e.step(4, Player.O)\n",
    "e.step(0, Player.X)\n",
    "e.step(1, Player.O)\n",
    "\n",
    "print(value_network(e.game_state))\n",
    "e.render()\n",
    "\n",
    "print(Minimax(Player.X, max_depth = 1, debug = True, training = False, explore = False).choose_action(e))\n",
    "print(Minimax(Player.X, max_depth = 6, debug = True, training = False, explore = False).choose_action(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(0, 0.1627739816904068), (1, 0.08172322809696198), (2, -0.04083317518234253), (3, -0.26001784205436707), (4, 0.26637932658195496), (5, -0.19680044054985046), (6, 0.10193490982055664), (7, -0.1815129518508911), (8, -0.0024103287141770124)]\n",
      " | | \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | | \n",
      "\n",
      "Player <class '__main__.Minimax'> Player.O turn\n",
      "[(0, 0.9611228108406067), (1, 0.8898664712905884), (2, 0.8822572231292725), (3, 0.8105031251907349), (5, 0.983535885810852), (6, 0.8679623603820801), (7, 0.9558358788490295), (8, 0.37650495767593384)]\n",
      " | | \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | |O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(0, 0.17581771314144135), (1, 0.2469054013490677), (2, 0.1729261726140976), (3, -0.032016970217227936), (5, 0.21819420158863068), (6, 0.2091735601425171), (7, 0.17156630754470825)]\n",
      " |X| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | |O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.O turn\n",
      "[(0, 0.9931862950325012), (2, 0.9570872783660889), (3, 0.9548094272613525), (5, 0.9705055952072144), (6, 0.9235170483589172), (7, 0.2954329550266266)]\n",
      " |X| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " |O|O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(0, -0.9171115756034851), (2, -0.8247581124305725), (3, -0.9744757413864136), (5, -0.9527405500411987), (6, 0.17551521956920624)]\n",
      " |X| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.O turn\n",
      "[(0, 0.9896542429924011), (2, 0.16786423325538635), (3, 0.8666820526123047), (5, 0.9387598633766174)]\n",
      " |X|O\n",
      "-----\n",
      " |X| \n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(0, -0.871913492679596), (3, -0.6932361125946045), (5, -0.06688834726810455)]\n",
      " |X|O\n",
      "-----\n",
      " |X|X\n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.O turn\n",
      "[(0, 0.8985496163368225), (3, -0.10979525744915009)]\n",
      " |X|O\n",
      "-----\n",
      "O|X|X\n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(0, 0.0)]\n",
      "X|X|O\n",
      "-----\n",
      "O|X|X\n",
      "-----\n",
      "X|O|O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agents2 = [Minimax(Player.X, max_depth = 1, debug = True, explore = False, training = False), \n",
    "           Minimax(Player.O, max_depth = 1, debug = True, explore = False, training = False)]\n",
    "#agents2 = [Human(Player.X), Minimax(Player.O, max_depth = 1, debug = True, training = False, explore = False)]\n",
    "#agents2 = [RandomBot(Player.X), Minimax(Player.O, max_depth = 1, debug = True, training = False, explore = False)]\n",
    "    \n",
    "env = BG()\n",
    "s, r, d, m = env.reset()\n",
    "\n",
    "#env.step(0, Player.X)   #\n",
    "\n",
    "#index = 1    #\n",
    "index = 0\n",
    "\n",
    "while not d: \n",
    "    print(f\"Player {agents2[index].__class__} {agents2[index].player} turn\")\n",
    "    a = agents2[index].choose_action(env)\n",
    "    s, r, d, m = env.step(a, agents2[index].player)\n",
    "    env.render()\n",
    "    \n",
    "    index = 1 - index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(0, 0.1627739816904068), (1, 0.08172322809696198), (2, -0.04083317518234253), (3, -0.26001784205436707), (4, 0.26637932658195496), (5, -0.19680044054985046), (6, 0.10193490982055664), (7, -0.1815129518508911), (8, -0.0024103287141770124)]\n",
      " | | \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | | \n",
      "\n",
      "Player <class '__main__.Human'> Player.O turn\n",
      "Take an action please: 0\n",
      "O| | \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | | \n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(1, 0.8334336280822754), (2, 0.8249943852424622), (3, 0.7283322215080261), (5, 0.45005443692207336), (6, 0.7693401575088501), (7, 0.3870547115802765), (8, 0.5855384469032288)]\n",
      "O|X| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | | \n",
      "\n",
      "Player <class '__main__.Human'> Player.O turn\n",
      "Take an action please: 7\n",
      "O|X| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " |O| \n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(2, 0.5379315614700317), (3, 0.810063362121582), (5, 0.009353315457701683), (6, 0.9132905602455139), (8, 0.5674870610237122)]\n",
      "O|X| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      "X|O| \n",
      "\n",
      "Player <class '__main__.Human'> Player.O turn\n",
      "Take an action please: 2\n",
      "O|X|O\n",
      "-----\n",
      " |X| \n",
      "-----\n",
      "X|O| \n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(3, -0.6632874608039856), (5, 0.5572575926780701), (8, 0.7396500110626221)]\n",
      "O|X|O\n",
      "-----\n",
      " |X| \n",
      "-----\n",
      "X|O|X\n",
      "\n",
      "Player <class '__main__.Human'> Player.O turn\n",
      "Take an action please: 3\n",
      "O|X|O\n",
      "-----\n",
      "O|X| \n",
      "-----\n",
      "X|O|X\n",
      "\n",
      "Player <class '__main__.Minimax'> Player.X turn\n",
      "[(5, 0.0)]\n",
      "O|X|O\n",
      "-----\n",
      "O|X|X\n",
      "-----\n",
      "X|O|X\n",
      "\n",
      "16.574187755584717\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "#agents2 = [Minimax(Player.X, max_depth = 3, debug = True, explore = False, training = False), \n",
    " #          Minimax(Player.O, max_depth = 3, debug = True, explore = False, training = False)]\n",
    "agents2 = [Minimax(Player.X, max_depth = 1, debug = True, training = False, explore = False), Human(Player.O)]\n",
    "#agents2 = [RandomBot(Player.X), Minimax(Player.O, max_depth = 1, debug = True, training = False, explore = False)]\n",
    "    \n",
    "env = BG()\n",
    "s, r, d, m = env.reset()\n",
    "\n",
    "#env.step(0, Player.X)   #\n",
    "\n",
    "#index = 1    #\n",
    "index = 0\n",
    "\n",
    "while not d: \n",
    "    print(f\"Player {agents2[index].__class__} {agents2[index].player} turn\")\n",
    "    a = agents2[index].choose_action(env)\n",
    "    s, r, d, m = env.step(a, agents2[index].player)\n",
    "    env.render()\n",
    "    \n",
    "    index = 1 - index\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(n = 100):\n",
    "    env = BG()\n",
    "    \n",
    "    records = [0, 0, 0]\n",
    "    for _ in range(n):\n",
    "        s, r, d, m = env.reset()\n",
    "        \n",
    "        #agents = [Minimax(Player.X, max_depth = 1, debug = False, training = False, explore = False), RandomBot(Player.O)]\n",
    "        agents = [RandomBot(Player.X), Minimax(Player.O, max_depth = 1, debug = False, training = False, explore = False)]\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        while not d: \n",
    "            a = agents[index].choose_action(env)\n",
    "            s, r, d, m = env.step(a, agents[index].player)\n",
    "\n",
    "            index = 1 - index\n",
    "\n",
    "        records[int(r + 1)] += 1\n",
    "        print(f\"{_+1} games passed    \", end = '\\r')\n",
    "    return records\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 games passed    \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[89, 11, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: larger_value_net\\assets\n"
     ]
    }
   ],
   "source": [
    "value_network.save(\"larger_value_net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X|O| \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "\n",
      "[(2, -0.9999363422393799), (3, 0.9999933242797852), (4, 1.0), (5, -0.9999459385871887), (6, 0.9992682933807373), (7, -0.9999882578849792), (8, -0.9999363422393799)]\n",
      "X|O| \n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = BG()\n",
    "env.step(0, Player.X)\n",
    "env.step(1, Player.O)\n",
    "\n",
    "\n",
    "env.render()\n",
    "\n",
    "\n",
    "m_agent = Minimax(Player.X, debug = True, training = False)\n",
    "\n",
    "a = m_agent.choose_action(env)\n",
    "env.step(a, m_agent.player)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(n = 100):\n",
    "    env = BG()\n",
    "    \n",
    "    records = [0, 0, 0]\n",
    "    for _ in range(n):\n",
    "        s, r, d, m = env.reset()\n",
    "        \n",
    "        agents = [RandomBot(Player.X), RandomBot(Player.O)] #[RandomBot(Player.X), Minimax(Player.O, max_depth = 4)]\n",
    "        index = 0\n",
    "\n",
    "        while not d: \n",
    "            a = agents[index].choose_action(env)\n",
    "            s, r, d, m = env.step(a, agents[index].player)\n",
    "\n",
    "            index = 1 - index\n",
    "\n",
    "        records[int(r + 1)] += 1\n",
    "                \n",
    "    return records\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2916, 1328, 5756]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'game_state': array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]]), 'state': array([1., 0., 0., 0., 0., 0., 0., 0., 0.]), 'mask': array([0., 1., 1., 1., 1., 1., 1., 1., 1.]), 'reward': 0, 'done': False, 'moves_left': 8, 'actions_taken': [0]}\n",
      "X| | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1.]]), 'state': array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.]), 'mask': array([0., 1., 1., 1., 1., 1., 1., 1., 0.]), 'reward': 0, 'done': False, 'moves_left': 7, 'actions_taken': [0, 8]}\n",
      "X| | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1.]]), 'state': array([ 1.,  0.,  0.,  0.,  0.,  1.,  0.,  0., -1.]), 'mask': array([0., 1., 1., 1., 1., 0., 1., 1., 0.]), 'reward': 0, 'done': False, 'moves_left': 6, 'actions_taken': [0, 8, 5]}\n",
      "X| | \n",
      "-----\n",
      " | |X\n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1.]]), 'state': array([ 1.,  0., -1.,  0.,  0.,  1.,  0.,  0., -1.]), 'mask': array([0., 1., 0., 1., 1., 0., 1., 1., 0.]), 'reward': 0, 'done': False, 'moves_left': 5, 'actions_taken': [0, 8, 5, 2]}\n",
      "X| |O\n",
      "-----\n",
      " | |X\n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1.]]), 'state': array([ 1.,  0., -1.,  0.,  1.,  1.,  0.,  0., -1.]), 'mask': array([0., 1., 0., 1., 0., 0., 1., 1., 0.]), 'reward': 0, 'done': False, 'moves_left': 4, 'actions_taken': [0, 8, 5, 2, 4]}\n",
      "X| |O\n",
      "-----\n",
      " |X|X\n",
      "-----\n",
      " | |O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1.]]), 'state': array([ 1.,  0., -1.,  0.,  1.,  1.,  0., -1., -1.]), 'mask': array([0., 1., 0., 1., 0., 0., 1., 0., 0.]), 'reward': 0, 'done': False, 'moves_left': 3, 'actions_taken': [0, 8, 5, 2, 4, 7]}\n",
      "X| |O\n",
      "-----\n",
      " |X|X\n",
      "-----\n",
      " |O|O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1.]]), 'state': array([ 1.,  0., -1.,  0.,  1.,  1.,  1., -1., -1.]), 'mask': array([0., 1., 0., 1., 0., 0., 0., 0., 0.]), 'reward': 0, 'done': False, 'moves_left': 2, 'actions_taken': [0, 8, 5, 2, 4, 7, 6]}\n",
      "X| |O\n",
      "-----\n",
      " |X|X\n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1.]]), 'state': array([ 1., -1., -1.,  0.,  1.,  1.,  1., -1., -1.]), 'mask': array([0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'reward': 0, 'done': False, 'moves_left': 1, 'actions_taken': [0, 8, 5, 2, 4, 7, 6, 1]}\n",
      "X|O|O\n",
      "-----\n",
      " |X|X\n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "\n",
      "{'game_state': array([[1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1.]]), 'state': array([ 1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.]), 'mask': array([0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'reward': 1.0, 'done': True, 'moves_left': 0, 'actions_taken': [0, 8, 5, 2, 4, 7, 6, 1, 3]}\n",
      "X|O|O\n",
      "-----\n",
      "X|X|X\n",
      "-----\n",
      "X|O|O\n",
      "\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "env = BG()\n",
    "s, _, d, m = env.reset()\n",
    "\n",
    "agents = [RandomBot(Player.X), Minimax(Player.O, max_depth = 4)]\n",
    "index = env.calculate_turn()\n",
    "\n",
    "while not d: \n",
    "    a = agents[index].choose_action(env)\n",
    "    s, _, d, m = env.step(a, agents[index].player)\n",
    "    \n",
    "    print(env.get_internal())\n",
    "    env.render()\n",
    "    print()\n",
    "    index = 1 - index\n",
    "    \n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
