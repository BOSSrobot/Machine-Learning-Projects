{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "Here I try to code an AI to deal solve the game of Toot and Otto (kind of like connect four - you can probably already guess what the game is from here). It is an interesting spin and one I decided to try to solve. \n",
    "\n",
    "From the start I had a feeling that policy gradients wouldn't be the best for such a problem. Either way I went ahead with it hoping that the problem would be simple enough to solve without MCTS (that will be the next iteration of algorithms I create - not ready for that now lolrip). The reason this project failed was not because of this decision however. The problem stems from the fact that this environment is not the classical MDP I have faced before. It is a zero sum game where naively letting the agent play against itself (self-play) leads to convergence around suboptimal policies - ones that converges when playing against each other but are no where near the nash equilibrium or optimal policy (this is seen when it horribly loses against a random bot after training - infact, it does better before training). When training against a random agent, it does not converge (The messy repeated code is my desperation attempt to salvage the algorithm by using this method of direct play). This may be due to the incorrect formulation of the problem setting I am using. It is interesting because the Tic Tac Toe DQN did converge to a number. This may be because Policy Gradients are more complicated and so the effect is multiplied, also this problem space is much more difficult, running this algorithm on Tic Tac Toe could lead to some further insight about why DQN still works. Finally, DQN may not have the same problems due to it's implicit modelling of the policy. Updating the value of a state is very different from updating an agent to beat itself.\n",
    "\n",
    "I think the steps to take are clear. I need to do some more theoretical research, and understand why this problem mathematically arises. I also need to learn how this situation is formally dealt with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from itertools import product \n",
    "\n",
    "import tensorflow as tf                        # Fast machine learning \n",
    "from tensorflow.keras import layers            # Makes it easier to use functional API\n",
    "from typing import Any, List, Sequence, Tuple  # Lets us use type checking by giving us names to call\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class board:\n",
    "    \n",
    "    def __init__(self):    \n",
    "        #Game specific Variables\n",
    "        self.moves_left = 24\n",
    "        self.turn = 1\n",
    "        \n",
    "        #These two variables keep track of the state and how much the player can place \n",
    "        self.state = np.array([[0 for _ in range(6)] for __ in range(4)], dtype=np.float32)\n",
    "        self.level = np.array([0 for _ in range(6)], dtype=np.int32) #Represents height of the column (must be less than 4!)\n",
    "        \n",
    "        #Each row represents one player's chips. \n",
    "        self.chips = np.array([[6, 6], [6, 6]], dtype=np.int32) \n",
    "\n",
    "        # First row represents player 1, second row represents player 2\n",
    "        # First 6 actions represent a 1 in the column, the next 6 represent a -1 in the column\n",
    "        self.mask = np.array([[1 for _ in range(12)] for _ in range(2)])\n",
    "        \n",
    "    @staticmethod\n",
    "    def check(tup):\n",
    "        a, b, c, d = tup \n",
    "        if a == 1 and b == -1 and c == -1 and d == 1: \n",
    "            return 1\n",
    "        elif a == -1 and b == 1 and c == 1 and d == -1: \n",
    "            return -1 \n",
    "        return 0 \n",
    "        \n",
    "    def calculate_winner(self):\n",
    "        pts = 0 \n",
    "        #Horizontal Check \n",
    "        for i, j in product(range(4), range(3)):\n",
    "            pts += self.check(self.state[i][j: j+4])\n",
    "            \n",
    "        #Vertical Check \n",
    "        for i in range(6): \n",
    "            pts += self.check(self.state[:, i])\n",
    "            \n",
    "        #Diagonal Check\n",
    "        for i in range(3): \n",
    "            pts += self.check([self.state[0][i], self.state[1][i+1], self.state[2][i+2], self.state[3][i+3]])\n",
    "            pts += self.check([self.state[3][i], self.state[2][i+1], self.state[1][i+2], self.state[0][i+3]])\n",
    "            \n",
    "        return pts\n",
    "                \n",
    "    def update(self, action):\n",
    "        action = int(action)\n",
    "        chip = action//6\n",
    "        loc = action%6\n",
    "\n",
    "        placed = 1 if chip == 0 else -1\n",
    "        player = 0 if self.turn == 1 else 1\n",
    "        \n",
    "        if self.level[loc] > 3: \n",
    "            raise ValueError(\"Too many chips in column!\", self.state, self.turn, self.moves_left, self.level, self.chips, action)\n",
    "            return \"invalid move\"\n",
    "        \n",
    "        if self.chips[player][chip] <= 0: \n",
    "            raise ValueError(\"Not enough chips\", self.state, self.turn, self.moves_left, self.level, self.chips, action)\n",
    "            return \"invalid move\"\n",
    "        \n",
    "        self.state[3-self.level[loc]][loc] = placed\n",
    "        self.level[loc] += 1\n",
    "        self.chips[player][chip] -= 1\n",
    "        self.moves_left -= 1\n",
    "        self.turn = (self.turn == 1)*(-1)+(self.turn == -1)*(1)\n",
    "        \n",
    "        if self.level[loc] == 4: \n",
    "            #Change masks of both players to not play on that column for either chip\n",
    "            self.mask[0][[loc, loc+6]] = 0\n",
    "            self.mask[1][[loc, loc+6]] = 0\n",
    "\n",
    "        if self.chips[player][chip] == 0: \n",
    "            self.mask[player][6*chip:6*chip+6] = 0\n",
    "            \n",
    "        if self.moves_left: \n",
    "            return \"valid move\"\n",
    "        else: \n",
    "            pts = self.calculate_winner()\n",
    "            if pts > 0: \n",
    "                winner = 1 \n",
    "            elif pts == 0: \n",
    "                winner = 0 \n",
    "            else: \n",
    "                winner = -1 \n",
    "            return f\"{winner} won\"\n",
    "        \n",
    "    def reset(self):\n",
    "        self.moves_left = 24\n",
    "        self.turn = 1\n",
    "        \n",
    "        self.state = np.array([[0 for _ in range(6)] for __ in range(4)], dtype = np.float32)\n",
    "        self.level = np.array([0 for _ in range(6)], dtype = np.int32) #Represents height of the column (must be less than 4!)\n",
    "        \n",
    "        self.chips = np.array([[6, 6], [6, 6]], dtype=np.int32) \n",
    "        self.mask = np.array([[1 for _ in range(12)] for _ in range(2)], dtype = np.float32)\n",
    "\n",
    "class game:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #RL Variables\n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "        \n",
    "        #Tic Tac Toe Board\n",
    "        self.gameboard = board()\n",
    "        \n",
    "    def step(self, action):                    \n",
    "        result = self.gameboard.update(action)\n",
    "        \n",
    "        if result == 'invalid move':\n",
    "            self.reward = -1\n",
    "            self.terminal = True\n",
    "            \n",
    "        elif result == 'valid move':\n",
    "            self.reward = 0\n",
    "            self.terminal = False\n",
    "            \n",
    "        elif result == '1 won':\n",
    "            self.reward = 1\n",
    "            self.terminal = True\n",
    "            \n",
    "        elif result == '-1 won':\n",
    "            self.reward = -1\n",
    "            self.terminal = True\n",
    "        \n",
    "        elif result == '0 won':\n",
    "            self.reward = 0\n",
    "            self.terminal = True\n",
    "            \n",
    "        return self.get_state_features(), self.get_mask(), self.reward, self.terminal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.gameboard.reset()\n",
    "        \n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "        return self.get_state_features(), self.get_mask()        \n",
    "    \n",
    "    def get_state_features(self):\n",
    "        return self.gameboard.state.reshape(-1) * self.gameboard.turn\n",
    "\n",
    "    def get_mask(self):\n",
    "        player = 0 if self.gameboard.turn == 1 else 1\n",
    "        return self.gameboard.mask[player]\n",
    "    \n",
    "    def render(self):\n",
    "        def convert(n):\n",
    "            n = int(n)\n",
    "            return \"T\"*(n==1) + \" \"*(n==0) + \"O\"*(n==-1)\n",
    "        \n",
    "        print(f\"Player 1 chips: {self.gameboard.chips[0][0]} T's and {self.gameboard.chips[0][1]} O's\")\n",
    "        print(f\"Player 2 chips: {self.gameboard.chips[1][0]} T's and {self.gameboard.chips[1][1]} O's\")\n",
    "        for row in self.gameboard.state: \n",
    "            for col in row[:-1]: \n",
    "                print(convert(col), end = \" | \")\n",
    "            print(convert(row[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class human:\n",
    "    def evaluate(self, s):\n",
    "        return \"IS A PLAYER\"\n",
    "    \n",
    "    def choose_action(self, s, m):\n",
    "        return int(input(\"Where would you like to move? \"))\n",
    "    \n",
    "class random_bot: \n",
    "    def evaluate(self, s):\n",
    "        return \"IS A RANDOM BOT\"\n",
    "    \n",
    "    def choose_action(self, s, m):\n",
    "        return (np.random.uniform(low = 0.5, high = 1, size=[12]) * m).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(players = [random_bot(), random_bot()], display = True, steps = 24): \n",
    "\n",
    "    b = game()    \n",
    "    s, m = b.reset()\n",
    "\n",
    "    playing = 0 \n",
    "    for t in range(steps):\n",
    "        if display: print(f\"player {playing} thinks values are {players[playing].evaluate(s)}\")\n",
    "        if display: b.render()\n",
    "            \n",
    "        a = players[playing].choose_action(s, m)\n",
    "        s, m, r, done = b.step(a)\n",
    "        \n",
    "        if display: print(f\"action was {a} and the reward was {r}\\n\")\n",
    "        if done: break\n",
    "        \n",
    "        playing = 0 if playing == 1 else 1 \n",
    "    \n",
    "    if display: b.render()\n",
    "    return b.reward, b\n",
    "\n",
    "def arena(players = [random_bot(), random_bot()], display = False, num_games = 10000): \n",
    "    p1_wins, p2_wins, draws = 0, 0, 0\n",
    "    for i in range(1, num_games+1):\n",
    "        winner, b = simulate(players, display)\n",
    "        if winner > 0: \n",
    "            p1_wins += 1 \n",
    "        elif winner < 0: \n",
    "            p2_wins += 1\n",
    "        else: \n",
    "            draws += 1\n",
    "                    \n",
    "        if i%100==0:\n",
    "            msg1 = f'player 1 {str(players[0].__class__)[17:-2]} won {p1_wins} times'\n",
    "            msg2 = f' and player 2 {str(players[1].__class__)[17:-2]} won {p2_wins} times.' \n",
    "            msg3 = f' They drew {draws} times.'\n",
    "            msg4 = f' They played {i} games.    '\n",
    "            print(msg1 + msg2 + msg3 + msg4 + '        ', end = '\\r')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 random_bot won 368 times and player 2 random_bot won 414 times. They drew 218 times. They played 1000 games.            \r"
     ]
    }
   ],
   "source": [
    "arena(num_games = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions: int, num_hidden_units: List[int]):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units[0])\n",
    "        self.common2 = layers.Dense(num_hidden_units[1])\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        x2 = self.common2(x)\n",
    "        return self.actor(x2), self.critic(x2)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_actions: int = 12, num_hidden_units: List[int] = [128, 64], gamma: int = 1):\n",
    "        self.model = ActorCritic(num_actions, num_hidden_units)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    @tf.function()\n",
    "    def evaluate(self, s):\n",
    "        s = tf.expand_dims(s, 0)\n",
    "        return self(s)\n",
    "    \n",
    "    @tf.function()\n",
    "    def choose_action(self, s, m):\n",
    "        s, mask = [tf.expand_dims(x, 0) for x in [s, m]]\n",
    "        probs, _ = self(s)\n",
    "        \n",
    "        mask_bool = tf.cast(mask, dtype = tf.bool)\n",
    "        adj = tf.where(mask_bool, probs, -1e+8)\n",
    "        probs = tf.nn.softmax(adj)\n",
    "        \n",
    "        return tf.cast(tf.argmax(probs[0]), tf.int32)\n",
    "    \n",
    "    @tf.function()\n",
    "    def __call__(self, s): \n",
    "        return self.model(s)\n",
    "        \n",
    "def init_agent(gamma = 1): \n",
    "    a = Agent(gamma = gamma)\n",
    "    a.evaluate(game().reset()[0])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = init_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, <__main__.game at 0x24965f29a60>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(players = [agent, random_bot()], display = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 Agent won 49 times and player 2 random_bot won 41 times. They drew 10 times. They played 100 games.            \r"
     ]
    }
   ],
   "source": [
    "arena(players = [agent, random_bot()], num_games = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset() -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(env.reset, [], [tf.float32, tf.float32])\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    state, mask, reward, done = env.step(action)\n",
    "    return state.astype(np.float32), mask.astype(np.float32), np.array(reward), np.array(done, np.int32)\n",
    "\n",
    "def tf_env_step(action: tf.Tensor):\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.float32, tf.int32, tf.int32])\n",
    "\n",
    "@tf.function()\n",
    "def run_episode(initial_state: tf.Tensor, initial_mask: tf.Tensor, agent: Agent) -> List[tf.Tensor]:\n",
    "    \n",
    "    action_probs = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    action_probs_2 = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    values_2 = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    rewards_2 = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state \n",
    "    \n",
    "    initial_mask_shape = initial_mask.shape\n",
    "    mask = initial_mask \n",
    "    \n",
    "    for t in tf.range(12): \n",
    "        #Get state/mask into shape\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask_bool = tf.cast(mask, dtype = tf.bool)\n",
    "        \n",
    "        #Get probabilities and apply mask to them\n",
    "        action_logits_t, value = agent(state)\n",
    "        adj = tf.where(mask_bool, action_logits_t, -1e+8)\n",
    "        \n",
    "        #Choose action and get probabilities \n",
    "        action = tf.random.categorical(adj, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(adj)\n",
    "        \n",
    "        #Store values and probabilities\n",
    "        values = values.write(t, tf.squeeze(value)) #Squeezing gets rid of the batch aspect\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        \n",
    "        #Advance to the next stage \n",
    "        state, mask, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        mask.set_shape(initial_mask_shape)\n",
    "        \n",
    "        ############### Second player moves \n",
    "        \n",
    "        #Similar deal, get state/mask into shape\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask_bool = tf.cast(mask, dtype = tf.bool)\n",
    "        \n",
    "        #Get probabilities and apply mask to them\n",
    "        action_logits_t_2, value_2 = agent(state)\n",
    "        adj_2 = tf.where(mask_bool, action_logits_t_2, -1e+8)\n",
    "        \n",
    "        #Choose action and get probabilities \n",
    "        action_2 = tf.random.categorical(adj_2, 1)[0, 0]\n",
    "        action_probs_t_2 = tf.nn.softmax(adj_2)\n",
    "        \n",
    "        #Store values and probabilities\n",
    "        values_2 = values_2.write(t, tf.squeeze(value_2)) #Squeezing gets rid of the batch aspect\n",
    "        action_probs_2 = action_probs_2.write(t, action_probs_t_2[0, action_2])\n",
    "        \n",
    "        #Advance to the next stage \n",
    "        state, mask, reward, done = tf_env_step(action_2)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        mask.set_shape(initial_mask_shape)\n",
    "        \n",
    "        #Intensionally use same reward: This means that p1 and p2 both use the second reward in a sequence. \n",
    "        #Allows for both to access winning reward. Since the game returns the true winner, p2 has to multiply by negative 1 \n",
    "        rewards = rewards.write(t, reward)\n",
    "        rewards_2 = rewards_2.write(t, reward*-1)\n",
    "        \n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    \n",
    "    action_probs_2 = action_probs_2.stack()\n",
    "    values_2 = values_2.stack()\n",
    "    rewards_2 = rewards_2.stack()\n",
    "    \n",
    "    return [action_probs, values, rewards], [action_probs_2, values_2, rewards_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "  array([0.08333334, 0.0870977 , 0.1161226 , 0.07653689, 0.08573595,\n",
       "         0.12388834, 0.08547961, 0.0906362 , 0.10513576, 0.26571584,\n",
       "         0.50966215, 0.58264416], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "  array([0.        , 0.38940352, 0.5622256 , 0.3527979 , 0.36429882,\n",
       "         0.63375   , 0.45862365, 0.43792194, 0.5889814 , 0.44887596,\n",
       "         0.4393671 , 0.3982233 ], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1])>],\n",
       " [<tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "  array([0.09037085, 0.06396628, 0.09874069, 0.11120116, 0.1024255 ,\n",
       "         0.14124674, 0.1355187 , 0.1714785 , 0.12955573, 0.17962563,\n",
       "         0.31799874, 1.        ], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "  array([-0.0232748 , -0.3288091 , -0.42937094, -0.31275058, -0.581812  ,\n",
       "         -0.4382762 , -0.44735333, -0.44337407, -0.73845744, -0.47359443,\n",
       "         -0.38285884, -0.34879267], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(12,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])>])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(*reset(), agent)\n",
    "run_episode(*reset(), agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def get_expected_return(rewards: tf.Tensor, gamma: float = 1) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "    \n",
    "    # Create the variables we must track. We use tf.shape(rewards)[0] because that is the epsiode length\n",
    "    n = tf.shape(rewards)[0]                           \n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "    \n",
    "    # To efficiently compute the return, we start by computing it for the last epsiode and then * by gamma followed by + r. \n",
    "    # We basically work from down up to find the returns\n",
    "    \n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32) # We use [::-1] to order the rewards from last one to first\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    \n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum \n",
    "        discounted_sum.set_shape(discounted_sum_shape) # Allows us to ensure that everything is going correctly\n",
    "        returns = returns.write(i, discounted_sum) \n",
    "        \n",
    "    returns = returns.stack()[::-1] # Now the returns are ordered from last to first so we swap em again. Shape is (n, )\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  0  0  0  0  0  0  0  0  0  0 -1], shape=(12,), dtype=int32)\n",
      "tf.Tensor([-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.], shape=(12,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a, b = run_episode(*reset(), agent) # Note that I'm being lazy, env.reset() should be a Tensor\n",
    "print(a[2])\n",
    "returns = get_expected_return(a[2])\n",
    "returns = get_expected_return(a[2])\n",
    "returns = get_expected_return(a[2])\n",
    "\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "@tf.function()\n",
    "def compute_loss(action_probs: tf.Tensor, values: tf.Tensor, returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    " \n",
    "    advantage = returns - values\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "    \n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-8.05492>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = run_episode(*reset(), agent)\n",
    "f,g,h = a\n",
    "\n",
    "action_probs, values, returns = f, g, get_expected_return(h)\n",
    "\n",
    "advantage = returns - values\n",
    "action_log_probs = tf.math.log(action_probs)\n",
    "\n",
    "compute_loss(f, g, get_expected_return(h))\n",
    "compute_loss(f, g, get_expected_return(h))\n",
    "compute_loss(f, g, get_expected_return(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(init_vars: List[tf.Tensor], agent: Agent, optimizer: tf.keras.optimizers.Optimizer) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        \n",
    "        # Run the model for one episode to collect training data\n",
    "        set1, set2 = run_episode(*init_vars, agent)\n",
    "        \n",
    "        # Calculate expected returns\n",
    "        returns1 = get_expected_return(set1[2], agent.gamma)\n",
    "        returns2 = get_expected_return(set2[2], agent.gamma)\n",
    "        \n",
    "        # Convert the data to get the correct shape: (n, 1) \n",
    "        set1 = [tf.expand_dims(x, 1) for x in set1]\n",
    "        set2 = [tf.expand_dims(x, 1) for x in set2]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(set1[0], set1[1], returns1)\n",
    "        loss2 = compute_loss(set2[0], set2[1], returns2)\n",
    "        \n",
    "    # Compute the gradients from the loss and apply the gradients to the model's parameters\n",
    "    grads = tape.gradient(loss, agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, agent.model.trainable_variables))\n",
    "        \n",
    "    # Compute the gradients from the loss and apply the gradients to the model's parameters\n",
    "    grads = tape.gradient(loss2, agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, agent.model.trainable_variables))\n",
    "        \n",
    "    return loss, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step_random(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    state, mask, reward, done = env.step(action)\n",
    "    \n",
    "    s2, m2, r2, d2 = env.step((np.random.uniform(low = 0.5, high = 1, size=[12]) * mask).argmax())\n",
    "    return s2.astype(np.float32), m2.astype(np.float32), np.array(r2), np.array(d2, np.int32)\n",
    "\n",
    "def tf_env_step_random(action: tf.Tensor):\n",
    "    return tf.numpy_function(env_step_random, [action], [tf.float32, tf.float32, tf.int32, tf.int32])\n",
    "\n",
    "@tf.function()\n",
    "def run_episode_random(initial_state: tf.Tensor, initial_mask: tf.Tensor, agent: Agent) -> List[tf.Tensor]:\n",
    "    \n",
    "    action_probs = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state \n",
    "    \n",
    "    initial_mask_shape = initial_mask.shape\n",
    "    mask = initial_mask \n",
    "    \n",
    "    for t in tf.range(12): \n",
    "        #Get state/mask into shape\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask_bool = tf.cast(mask, dtype = tf.bool)\n",
    "        \n",
    "        #Get probabilities and apply mask to them\n",
    "        action_logits_t, value = agent(state)\n",
    "        adj = tf.where(mask_bool, action_logits_t, -1e+8)\n",
    "        \n",
    "        #Choose action and get probabilities \n",
    "        action = tf.random.categorical(adj, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(adj)\n",
    "        \n",
    "        #Store values and probabilities\n",
    "        values = values.write(t, tf.squeeze(value)) #Squeezing gets rid of the batch aspect\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        \n",
    "        #Advance to the next stage \n",
    "        state, mask, reward, done = tf_env_step_random(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        mask.set_shape(initial_mask_shape)\n",
    "        \n",
    "        #Intensionally use same reward: This means that p1 and p2 both use the second reward in a sequence. \n",
    "        #Allows for both to access winning reward. Since the game returns the true winner, p2 has to multiply by negative 1 \n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    \n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "@tf.function()\n",
    "def train_step_random(init_vars: List[tf.Tensor], agent: Agent, optimizer: tf.keras.optimizers.Optimizer) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards = run_episode_random(*init_vars, agent)\n",
    "        \n",
    "        # Calculate expected returns\n",
    "        returns = get_expected_return(rewards, agent.gamma)\n",
    "        \n",
    "        # Convert the data to get the correct shape: (n, 1) \n",
    "        action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "        \n",
    "    # Compute the gradients from the loss and apply the gradients to the model's parameters\n",
    "    grads = tape.gradient(loss, agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, agent.model.trainable_variables))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-9.416092>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_random(reset(), agent, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_v2():\n",
    "    _, __ = env.reset()\n",
    "    s, m, r, d = env.step(np.random.uniform(low = 0.5, high = 1, size=[12]).argmax())\n",
    "    return s, m\n",
    "    \n",
    "def tf_reset_v2() -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(reset_v2, [], [tf.float32, tf.float32])\n",
    "\n",
    "def env_step_random_v2(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    state, mask, reward, done = env.step(action)\n",
    "    if not done: \n",
    "        s2, m2, r2, d2 = env.step((np.random.uniform(low = 0.5, high = 1, size=[12]) * mask).argmax())\n",
    "        return s2.astype(np.float32), m2.astype(np.float32), np.array(reward), np.array(d2, np.int32)\n",
    "    else: \n",
    "        return state.astype(np.float32), mask.astype(np.float32), np.array(reward), np.array(done, np.int32)\n",
    "\n",
    "def tf_env_step_random_v2(action: tf.Tensor):\n",
    "    return tf.numpy_function(env_step_random_v2, [action], [tf.float32, tf.float32, tf.int32, tf.int32])\n",
    "\n",
    "@tf.function()\n",
    "def run_episode_random_v2(initial_state: tf.Tensor, initial_mask: tf.Tensor, agent: Agent) -> List[tf.Tensor]:\n",
    "    \n",
    "    action_probs = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True)\n",
    "    rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state \n",
    "    \n",
    "    initial_mask_shape = initial_mask.shape\n",
    "    mask = initial_mask \n",
    "    \n",
    "    for t in tf.range(12): \n",
    "        #Get state/mask into shape\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        mask = tf.expand_dims(mask, 0)\n",
    "        mask_bool = tf.cast(mask, dtype = tf.bool)\n",
    "        \n",
    "        #Get probabilities and apply mask to them\n",
    "        action_logits_t, value = agent(state)\n",
    "        adj = tf.where(mask_bool, action_logits_t, -1e+8)\n",
    "        \n",
    "        #Choose action and get probabilities \n",
    "        action = tf.random.categorical(adj, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(adj)\n",
    "        \n",
    "        #Store values and probabilities\n",
    "        values = values.write(t, tf.squeeze(value)) #Squeezing gets rid of the batch aspect\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        \n",
    "        #Advance to the next stage \n",
    "        state, mask, reward, done = tf_env_step_random_v2(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        mask.set_shape(initial_mask_shape)\n",
    "        \n",
    "        #Intensionally use same reward: This means that p1 and p2 both use the second reward in a sequence. \n",
    "        #Allows for both to access winning reward. Since the game returns the true winner, p2 has to multiply by negative 1 \n",
    "        rewards = rewards.write(t, reward)\n",
    "        \n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    \n",
    "    return action_probs, values, rewards\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "@tf.function()\n",
    "def train_step_random_v2(init_vars: List[tf.Tensor], agent: Agent, optimizer: tf.keras.optimizers.Optimizer) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards = run_episode_random_v2(*init_vars, agent)\n",
    "        \n",
    "        # Calculate expected returns\n",
    "        returns = get_expected_return(rewards, agent.gamma)\n",
    "        \n",
    "        # Convert the data to get the correct shape: (n, 1) \n",
    "        action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "        \n",
    "    # Compute the gradients from the loss and apply the gradients to the model's parameters\n",
    "    grads = tape.gradient(loss, agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, agent.model.trainable_variables))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-16.990185>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_random_v2(tf_reset_v2(), agent, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 Agent won 74 times and player 2 random_bot won 9 times. They drew 17 times. They played 100 games.            \r"
     ]
    }
   ],
   "source": [
    "arena(players = [agent, random_bot()], num_games = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 random_bot won 44 times and player 2 Agent won 20 times. They drew 36 times. They played 100 games.            \r"
     ]
    }
   ],
   "source": [
    "arena(players = [random_bot(), agent], num_games = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 random_bot won 424 times and player 2 Agent won 22 times. They drew 54 times. They played 500 games.            \n",
      "player 1 random_bot won 416 times and player 2 Agent won 22 times. They drew 62 times. They played 500 games.            \n",
      "player 1 random_bot won 421 times and player 2 Agent won 21 times. They drew 58 times. They played 500 games.            \n",
      "player 1 random_bot won 404 times and player 2 Agent won 41 times. They drew 55 times. They played 500 games.            \n",
      "player 1 random_bot won 423 times and player 2 Agent won 32 times. They drew 45 times. They played 500 games.            \n",
      "player 1 random_bot won 415 times and player 2 Agent won 28 times. They drew 57 times. They played 500 games.            \n",
      "player 1 random_bot won 442 times and player 2 Agent won 9 times. They drew 49 times. They played 500 games.            \n",
      "player 1 random_bot won 420 times and player 2 Agent won 27 times. They drew 53 times. They played 500 games.            \n",
      "player 1 random_bot won 430 times and player 2 Agent won 20 times. They drew 50 times. They played 500 games.            \n",
      "player 1 random_bot won 428 times and player 2 Agent won 14 times. They drew 58 times. They played 500 games.            \n",
      "player 1 random_bot won 424 times and player 2 Agent won 20 times. They drew 56 times. They played 500 games.            \n",
      "player 1 random_bot won 446 times and player 2 Agent won 18 times. They drew 36 times. They played 500 games.            \n",
      "player 1 random_bot won 400 times and player 2 Agent won 25 times. They drew 75 times. They played 500 games.            \n",
      "player 1 random_bot won 437 times and player 2 Agent won 23 times. They drew 40 times. They played 500 games.            \n",
      "player 1 random_bot won 438 times and player 2 Agent won 20 times. They drew 42 times. They played 500 games.            \n",
      "player 1 random_bot won 442 times and player 2 Agent won 9 times. They drew 49 times. They played 500 games.            \n",
      "player 1 random_bot won 357 times and player 2 Agent won 56 times. They drew 87 times. They played 500 games.            \n",
      "player 1 random_bot won 431 times and player 2 Agent won 22 times. They drew 47 times. They played 500 games.            \n",
      "player 1 random_bot won 408 times and player 2 Agent won 31 times. They drew 61 times. They played 500 games.            \n",
      "player 1 random_bot won 431 times and player 2 Agent won 13 times. They drew 56 times. They played 500 games.            \n",
      "player 1 random_bot won 424 times and player 2 Agent won 21 times. They drew 55 times. They played 500 games.            \n",
      "player 1 random_bot won 439 times and player 2 Agent won 14 times. They drew 47 times. They played 500 games.            \n",
      "player 1 random_bot won 377 times and player 2 Agent won 38 times. They drew 85 times. They played 500 games.            \n",
      "player 1 random_bot won 414 times and player 2 Agent won 29 times. They drew 57 times. They played 500 games.            \n",
      "player 1 random_bot won 440 times and player 2 Agent won 19 times. They drew 41 times. They played 500 games.            \n",
      "player 1 random_bot won 436 times and player 2 Agent won 18 times. They drew 46 times. They played 500 games.            \n",
      "player 1 random_bot won 431 times and player 2 Agent won 21 times. They drew 48 times. They played 500 games.            \n",
      "Interrupted training, displaying current statistics                               \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "option = 'random'\n",
    "\n",
    "max_episodes = 100000\n",
    "losses = []\n",
    "try: \n",
    "    for ep in range(1, max_episodes+1): \n",
    "        if option == 'random': \n",
    "            #l = train_step_random(reset(), agent, optimizer)\n",
    "            l2 = train_step_random_v2(tf_reset_v2(), agent, optimizer)\n",
    "            loss = l2.numpy()#l.numpy() + l2.numpy()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if ep%100 == 0: \n",
    "                print(f\"{ep} Episodes have been finished. So far: loss - {losses[-1]}     \", end = \"\\r\" )\n",
    "\n",
    "            if ep%2500 == 0: \n",
    "                arena(players = [random_bot(), agent], num_games = 500)\n",
    "                print()\n",
    "\n",
    "        else: \n",
    "            loss1, loss2 = train_step(reset(), agent, optimizer)\n",
    "            losses.append((loss1.numpy(), loss2.numpy()))\n",
    "\n",
    "            if ep %100 == 0: \n",
    "                print(f\"{ep} Episodes have been finished. So far: loss1 - {loss} and loss2 - {loss2}    \", end = \"\\r\" )\n",
    "                \n",
    "except KeyboardInterrupt: \n",
    "    print(\"Interrupted training, displaying current statistics                               \")\n",
    "\n",
    "if option == \"random\": \n",
    "    plt.plot(range(len(losses)), losses)\n",
    "else: \n",
    "    fig, axes = plt.subplots(1,2)\n",
    "    a, b = zip(*losses[300:])\n",
    "\n",
    "    axes[0].plot(range(len(a)), a)\n",
    "    axes[1].plot(range(len(b)), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 Agent won 831 times and player 2 random_bot won 63 times. They drew 106 times. They played 1000 games.            \r"
     ]
    }
   ],
   "source": [
    "arena(players = [agent, random_bot()], num_games = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 Agent won 100 times and player 2 Agent won 0 times. They drew 0 times. They played 100 games.            \r"
     ]
    }
   ],
   "source": [
    "arena(players = [agent, agent], num_games = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.2894943, -3.5588078, -6.1133685, -0.9993383, -4.0170364,\n",
      "        -1.9503982, -3.4126441,  2.4485414,  6.2898607, 15.036212 ,\n",
      "        -6.402041 , -1.25827  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7847097]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 6 O's\n",
      "Player 2 chips: 6 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 6 T's and 5 O's\n",
      "Player 2 chips: 6 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 5.6983075 , -3.764673  , -7.134369  , -2.1153278 , -6.538213  ,\n",
      "        -2.343559  , -4.21098   ,  2.2303834 ,  4.562322  , 20.639086  ,\n",
      "        -8.05112   , -0.32483327]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8139659]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 5 O's\n",
      "Player 2 chips: 5 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   | O | T |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 6 T's and 4 O's\n",
      "Player 2 chips: 5 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O | T |  \n",
      "action was 6 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.5995884, -4.9851418, -8.324585 , -0.8336165, -2.5210543,\n",
      "        -7.1884604, -4.740108 ,  1.5259798, 10.324363 , 22.612429 ,\n",
      "        -7.854655 ,  0.877706 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8261894]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 4 O's\n",
      "Player 2 chips: 5 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "O |   |   | O | T |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 6 T's and 3 O's\n",
      "Player 2 chips: 5 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "O |   |   | O | T |  \n",
      "action was 2 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-0.65131664, -4.9558024 , -6.326979  ,  0.33657438,  0.6330374 ,\n",
      "        -7.330354  , -3.6681027 ,  0.4500622 , 12.51251   , 16.207     ,\n",
      "        -6.2941074 ,  1.3332076 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.93226945]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 3 O's\n",
      "Player 2 chips: 4 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "O |   | T | O | T |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 6 T's and 2 O's\n",
      "Player 2 chips: 4 T's and 5 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "O |   | T | O | T |  \n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-4.502708 , -4.2661266, -4.3929515,  1.5341336,  6.223889 ,\n",
      "        -9.8411455, -2.4225147, -1.6847885, 16.149782 ,  9.9633255,\n",
      "        -4.5434823,  2.1040294]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.91136086]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 2 O's\n",
      "Player 2 chips: 4 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "O | O | T | O | T |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 6 T's and 1 O's\n",
      "Player 2 chips: 4 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   | O | O |   |  \n",
      "O | O | T | O | T |  \n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-1.7186551 , -4.8883233 , -5.127045  ,  1.267799  ,  2.722081  ,\n",
      "        -6.047564  , -1.9336348 , -0.5659353 , 13.330281  , 11.653214  ,\n",
      "        -5.786697  ,  0.18313952]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9484552]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 1 O's\n",
      "Player 2 chips: 3 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "T |   | O | O |   |  \n",
      "O | O | T | O | T |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 6 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "O | O | T | O | T |  \n",
      "action was 10 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.3589323 , -6.255763  , -5.551744  ,  2.90028   , -1.387472  ,\n",
      "        -0.01815692, -1.1464617 ,  1.1989453 , 10.1840105 , 10.222786  ,\n",
      "        -7.4577494 , -3.3542106 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.2329262]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 3 O's\n",
      "  |   |   | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O | O |  \n",
      "O | O | T | O | T |  \n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 5 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 3 O's\n",
      "  |   |   | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O | O |  \n",
      "O | O | T | O | T |  \n",
      "action was 1 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 0.6794422 , -4.6930656 , -3.9256682 ,  3.2940106 , -1.512021  ,\n",
      "         2.962776  ,  0.34687138,  1.0823214 ,  7.263226  ,  4.371827  ,\n",
      "        -6.328582  , -4.788547  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.0736064]], dtype=float32)>)\n",
      "Player 1 chips: 5 T's and 0 O's\n",
      "Player 2 chips: 2 T's and 3 O's\n",
      "  |   |   | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | T | O | O | O |  \n",
      "O | O | T | O | T |  \n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 4 T's and 0 O's\n",
      "Player 2 chips: 2 T's and 3 O's\n",
      "  |   |   | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | T | O | O | O |  \n",
      "O | O | T | O | T | T\n",
      "action was 2 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-1.9391963 , -4.4574246 , -3.2546818 ,  4.0570607 ,  1.3414081 ,\n",
      "         0.6219727 ,  0.39509264,  0.35230783,  9.71656   ,  2.4835925 ,\n",
      "        -5.242501  , -4.0588856 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.015362]], dtype=float32)>)\n",
      "Player 1 chips: 4 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 3 O's\n",
      "  |   | T | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | T | O | O | O |  \n",
      "O | O | T | O | T | T\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 3 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 3 O's\n",
      "  |   | T | O |   |  \n",
      "T |   | O | O | T |  \n",
      "T | T | O | O | O |  \n",
      "O | O | T | O | T | T\n",
      "action was 10 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-4.416244  , -3.629108  , -1.6070079 ,  4.117582  ,  3.4657881 ,\n",
      "         1.239494  ,  1.3431531 , -0.19391102,  9.058931  , -2.735776  ,\n",
      "        -2.616609  , -4.2502656 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8989748]], dtype=float32)>)\n",
      "Player 1 chips: 3 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 2 O's\n",
      "  |   | T | O | O |  \n",
      "T |   | O | O | T |  \n",
      "T | T | O | O | O |  \n",
      "O | O | T | O | T | T\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 2 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 2 O's\n",
      "  |   | T | O | O |  \n",
      "T |   | O | O | T |  \n",
      "T | T | O | O | O | T\n",
      "O | O | T | O | T | T\n",
      "action was 6 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-2.380489 , -2.7685552, -1.0983131,  3.2412736,  1.6946677,\n",
      "         3.7933385,  1.8533753, -0.6742066,  6.786008 , -2.9113085,\n",
      "        -3.4927154, -5.4190774]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.83052504]], dtype=float32)>)\n",
      "Player 1 chips: 2 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 1 O's\n",
      "O |   | T | O | O |  \n",
      "T |   | O | O | T |  \n",
      "T | T | O | O | O | T\n",
      "O | O | T | O | T | T\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 1 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 1 O's\n",
      "O |   | T | O | O |  \n",
      "T |   | O | O | T | T\n",
      "T | T | O | O | O | T\n",
      "O | O | T | O | T | T\n",
      "action was 1 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-0.6322842 , -1.8725376 , -0.63836765,  2.5030124 ,  0.58367926,\n",
      "         4.425488  ,  2.6119323 , -1.673548  ,  4.560367  , -2.038184  ,\n",
      "        -3.840183  , -5.458408  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6870482]], dtype=float32)>)\n",
      "Player 1 chips: 1 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 1 O's\n",
      "O |   | T | O | O |  \n",
      "T | T | O | O | T | T\n",
      "T | T | O | O | O | T\n",
      "O | O | T | O | T | T\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are IS A RANDOM BOT\n",
      "Player 1 chips: 0 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 1 O's\n",
      "O |   | T | O | O | T\n",
      "T | T | O | O | T | T\n",
      "T | T | O | O | O | T\n",
      "O | O | T | O | T | T\n",
      "action was 7 and the reward was 1\n",
      "\n",
      "Player 1 chips: 0 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 0 O's\n",
      "O | O | T | O | O | T\n",
      "T | T | O | O | T | T\n",
      "T | T | O | O | O | T\n",
      "O | O | T | O | T | T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, <__main__.game at 0x1d3d33cabe0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(players = [agent, random_bot()], display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.2894943, -3.5588078, -6.1133685, -0.9993383, -4.0170364,\n",
      "        -1.9503982, -3.4126441,  2.4485414,  6.2898607, 15.036212 ,\n",
      "        -6.402041 , -1.25827  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7847097]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 6 O's\n",
      "Player 2 chips: 6 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-0.20830628, -3.6928768 , -5.200972  ,  0.40047786, -0.8297248 ,\n",
      "        -3.164492  , -2.9004767 ,  2.0265985 ,  8.109579  , 11.517474  ,\n",
      "        -5.1710997 , -1.197225  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7737506]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 5 O's\n",
      "Player 2 chips: 6 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.111312 , -4.092389 , -8.00474  , -1.4417447, -4.969177 ,\n",
      "        -3.4253604, -4.4098644,  2.9773624,  8.0592985, 18.568245 ,\n",
      "        -7.2440467, -0.4363125]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8436669]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 5 O's\n",
      "Player 2 chips: 6 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 6.2877717, -2.9371364, -5.522063 , -1.7602566, -6.0827847,\n",
      "         1.2569834, -2.7215512,  2.2917905,  1.6666873, 16.996807 ,\n",
      "        -7.704561 , -3.2386904]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.59739673]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 4 O's\n",
      "Player 2 chips: 6 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-4.450666 , -3.8709311, -5.6953545,  0.9001673,  2.418275 ,\n",
      "        -7.2502604, -3.076796 ,  1.3013958, 13.414736 ,  9.349407 ,\n",
      "        -3.9250028,  1.2267264]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.94620526]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 4 O's\n",
      "Player 2 chips: 6 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  9.598844 ,  -3.1292531,  -5.833844 ,  -2.931978 , -10.316147 ,\n",
      "          4.93999  ,  -2.982615 ,   2.891839 ,  -1.5006042,  19.810219 ,\n",
      "         -9.4630575,  -4.446228 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6009388]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 3 O's\n",
      "Player 2 chips: 6 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   | O | O |   |  \n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-4.5971212 , -3.4355435 , -6.1816535 ,  0.60899985,  0.5000851 ,\n",
      "        -6.6849394 , -3.7985983 ,  2.8165257 , 11.907439  ,  8.302165  ,\n",
      "        -3.148554  ,  1.6101514 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.0042554]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 3 O's\n",
      "Player 2 chips: 5 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "T |   | O | O |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 7.749806 , -3.081227 , -5.1803374, -2.7260125, -7.697843 ,\n",
      "         1.7518001, -3.079705 ,  1.4538813,  1.3952384, 20.857433 ,\n",
      "        -9.097635 , -3.5461807]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.4588619]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 2 O's\n",
      "Player 2 chips: 5 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-1.813067  , -4.057742  , -6.915746  ,  0.34266523, -3.0017226 ,\n",
      "        -2.8913603 , -3.3097167 ,  3.9353788 ,  9.087939  ,  9.992054  ,\n",
      "        -4.391768  , -0.31073827]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.0413495]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 2 O's\n",
      "Player 2 chips: 4 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   |   | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.0858119 , -2.507393  , -4.596081  , -2.348956  ,  1.0247848 ,\n",
      "        -7.480989  , -3.577247  , -1.10799   ,  8.454771  , 20.18776   ,\n",
      "        -6.3621764 ,  0.83686197]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.26685858]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 1 O's\n",
      "Player 2 chips: 4 T's and 4 O's\n",
      "  |   |   | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  4.493821 ,  -4.605868 ,  -8.096602 ,  -0.3685607, -11.321397 ,\n",
      "          5.8721385,  -3.235227 ,   6.6564717,   1.9824711,  10.889412 ,\n",
      "         -7.481269 ,  -4.1853385]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.3696008]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 1 O's\n",
      "Player 2 chips: 4 T's and 3 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.1946772, -2.8918748, -5.054842 , -1.689089 ,  2.0660367,\n",
      "        -9.354627 , -3.8087077, -0.9284342,  9.461679 , 21.700478 ,\n",
      "        -5.898919 ,  1.402552 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.19490969]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 0 O's\n",
      "Player 2 chips: 4 T's and 3 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O |   |  \n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 4.2953234 , -4.565675  , -7.280497  , -0.02576094, -9.433973  ,\n",
      "         3.8465736 , -3.302748  ,  5.1854153 ,  3.2102215 , 10.456083  ,\n",
      "        -7.323299  , -2.9246094 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.3928066]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 3 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O | T |  \n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 0.37784195, -3.242183  , -5.5543714 , -2.8923295 ,  0.698695  ,\n",
      "        -8.482547  , -4.6040883 ,  0.31559977, 10.262805  , 22.13118   ,\n",
      "        -5.8520193 ,  1.345095  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.30154175]], dtype=float32)>)\n",
      "Player 1 chips: 5 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 3 O's\n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O | T |  \n",
      "action was 11 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 4.819141 , -4.115429 , -6.877407 ,  0.8494418, -9.323653 ,\n",
      "         4.6306057, -2.2822328,  4.660098 ,  2.0060549,  8.824736 ,\n",
      "        -6.998788 , -3.7593584]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.2590817]], dtype=float32)>)\n",
      "Player 1 chips: 5 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O | T | O\n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 3.6937711, -2.997404 , -5.5337725, -3.0088603, -2.8374615,\n",
      "        -3.7723577, -4.0182743,  0.7083396,  7.051562 , 22.260914 ,\n",
      "        -7.868973 , -1.3255357]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.33676732]], dtype=float32)>)\n",
      "Player 1 chips: 4 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 2 O's\n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O | T | O\n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.4705297, -4.992665 , -7.7109084,  1.5358653, -4.370854 ,\n",
      "        -2.3741941, -3.2187974,  3.7014353,  7.5556345, 11.147633 ,\n",
      "        -5.6930466, -0.6936934]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.2670428]], dtype=float32)>)\n",
      "Player 1 chips: 4 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 1 O's\n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O |   |  \n",
      "T | O | O | O | T | O\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 5.006628  , -2.980721  , -5.2089663 , -2.8189752 , -6.0191417 ,\n",
      "         0.67859745, -3.5919325 ,  2.1472986 ,  3.7749083 , 20.113548  ,\n",
      "        -8.298606  , -3.2223885 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.3972564]], dtype=float32)>)\n",
      "Player 1 chips: 3 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 1 O's\n",
      "T |   | O | O |   |  \n",
      "T |   | O | O |   |  \n",
      "T | O | O | O |   | T\n",
      "T | O | O | O | T | O\n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-0.2888454 , -4.7209272 , -7.1298184 ,  1.0638808 , -3.320366  ,\n",
      "        -3.4237807 , -3.7502255 ,  3.7144387 ,  8.327042  , 10.321503  ,\n",
      "        -5.012254  ,  0.42115468]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.2366799]], dtype=float32)>)\n",
      "Player 1 chips: 3 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 0 O's\n",
      "T |   | O | O |   |  \n",
      "T | O | O | O |   |  \n",
      "T | O | O | O |   | T\n",
      "T | O | O | O | T | O\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 3.6391776 , -3.2116466 , -4.8067064 , -1.4376903 , -2.7661247 ,\n",
      "        -0.9191611 , -2.3495655 ,  0.87782997,  6.0693955 , 18.427883  ,\n",
      "        -7.4127426 , -3.4323814 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.35599923]], dtype=float32)>)\n",
      "Player 1 chips: 2 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 0 O's\n",
      "T |   | O | O |   |  \n",
      "T | O | O | O |   |  \n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | O\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.8268108, -3.5939837, -7.0721307, -1.0556657, -7.68437  ,\n",
      "        -1.1938729, -4.2340336,  3.9845648,  3.8069136, 12.880295 ,\n",
      "        -6.2455873,  0.5918161]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.13446]], dtype=float32)>)\n",
      "Player 1 chips: 2 T's and 0 O's\n",
      "Player 2 chips: 2 T's and 0 O's\n",
      "T |   | O | O |   |  \n",
      "T | O | O | O |   | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | O\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.4218946 , -3.871461  , -4.897306  , -1.0143162 , -0.21537204,\n",
      "        -2.3836205 , -2.3489962 ,  0.91538274,  9.591419  , 17.358624  ,\n",
      "        -6.852416  , -3.5118952 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.52039444]], dtype=float32)>)\n",
      "Player 1 chips: 1 T's and 0 O's\n",
      "Player 2 chips: 2 T's and 0 O's\n",
      "T |   | O | O |   | T\n",
      "T | O | O | O |   | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | O\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.7108877 , -2.8693454 , -6.229859  , -0.48378265, -5.6707554 ,\n",
      "        -1.774728  , -3.6865106 ,  3.4375124 ,  2.6655018 ,  9.742342  ,\n",
      "        -4.190239  ,  1.311054  ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9531417]], dtype=float32)>)\n",
      "Player 1 chips: 1 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 0 O's\n",
      "T |   | O | O |   | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | O\n",
      "action was 1 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 2.1220105, -4.4594293, -7.2436132, -2.2676694, -0.7309282,\n",
      "        -3.445884 , -3.6006298,  1.4046427, 11.329879 , 21.497948 ,\n",
      "        -8.803974 , -3.0589375]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.58565116]], dtype=float32)>)\n",
      "Player 1 chips: 0 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 0 O's\n",
      "T | T | O | O |   | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | O\n",
      "action was 4 and the reward was 1\n",
      "\n",
      "Player 1 chips: 0 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 0 O's\n",
      "T | T | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | O\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, <__main__.game at 0x1d3d33b1460>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(players = [agent, agent], display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
       " array([[   8.088424 ,  -74.3431   ,  222.30336  ,    3.5896626,\n",
       "          149.9978   , -139.16635  ,  394.24637  ,   21.704138 ,\n",
       "          -46.30921  ,  -30.399351 ,  -83.8813   , -264.43298  ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-2.728084]], dtype=float32)>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.evaluate(reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -3.4284356, -20.83711  ,   0.2610071, -14.561428 , -13.153704 ,\n",
      "         11.598646 , -15.956387 ,   4.3295913,  40.47456  ,  23.40714  ,\n",
      "        -10.860404 ,  -8.658392 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7122537]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 6 O's\n",
      "Player 2 chips: 6 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -7.749812 , -25.141796 ,   3.4055479, -18.255762 , -14.380478 ,\n",
      "         15.746751 , -21.681639 ,   4.2820807,  49.972404 ,  25.661001 ,\n",
      "        -11.641884 ,  -9.526993 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6585663]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 5 O's\n",
      "Player 2 chips: 6 T's and 6 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   | O |   |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -1.3078512 , -18.536766  ,  -0.62704086, -11.663771  ,\n",
      "        -11.562499  ,   9.22791   , -12.5976925 ,   4.717011  ,\n",
      "         32.233128  ,  20.926792  ,  -9.774208  ,  -7.432398  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7307395]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 5 O's\n",
      "Player 2 chips: 6 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   |   |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -1.7983766, -20.787323 ,  -1.0280635, -12.953367 , -15.438433 ,\n",
      "         12.031811 , -14.520169 ,   3.8488748,  41.72953  ,  26.441854 ,\n",
      "        -12.245481 ,  -8.887928 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7619875]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 4 O's\n",
      "Player 2 chips: 6 T's and 5 O's\n",
      "  |   |   |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -5.795706  , -18.879406  ,  -0.09762481, -18.450085  ,\n",
      "        -10.546541  ,   8.882058  , -18.517889  ,   5.1003833 ,\n",
      "         44.986897  ,  21.150276  , -10.245015  ,  -9.608883  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6500282]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 4 O's\n",
      "Player 2 chips: 6 T's and 4 O's\n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  0.11542337, -24.238867  ,   0.6775329 ,  -9.825689  ,\n",
      "        -16.18172   ,  15.37294   , -13.795265  ,   4.4522758 ,\n",
      "         34.723938  ,  26.36291   , -11.638459  ,  -7.2502103 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.7633894]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 3 O's\n",
      "Player 2 chips: 6 T's and 4 O's\n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O | O |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -7.553428 , -21.930777 ,   1.1410687, -22.819128 , -10.886549 ,\n",
      "          9.778032 , -21.650576 ,   6.4088545,  51.776558 ,  22.698164 ,\n",
      "        -11.081194 , -11.518608 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6053187]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 3 O's\n",
      "Player 2 chips: 6 T's and 3 O's\n",
      "  |   | O |   |   |  \n",
      "  |   | O |   |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  3.9546444, -26.274328 ,   2.9048605,  -2.6974134, -19.442762 ,\n",
      "         19.594154 , -10.913982 ,   4.2741647,  23.306416 ,  25.335934 ,\n",
      "         -9.815301 ,  -3.293407 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.91742355]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 2 O's\n",
      "Player 2 chips: 6 T's and 3 O's\n",
      "  |   | O |   |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "action was 9 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -7.6085997,  -9.831774 ,  -6.1772213, -25.541405 ,  -3.3195837,\n",
      "         -3.2974417, -16.719763 ,   5.8643913,  51.24993  ,  18.29377  ,\n",
      "        -10.216182 , -14.249989 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.5156508]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 2 O's\n",
      "Player 2 chips: 6 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -0.53637296, -34.381783  ,   9.593648  ,  -2.3160005 ,\n",
      "        -22.442156  ,  29.172098  , -15.442525  ,   3.0896199 ,\n",
      "         24.963371  ,  26.834326  ,  -9.2073555 ,  -1.7377077 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.92824143]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 1 O's\n",
      "Player 2 chips: 6 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  | O | O | O |   |  \n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -7.0952654,  -7.4965034,  -9.088873 , -28.507643 ,  -3.4299161,\n",
      "         -6.4781895, -17.580837 ,   6.0841265,  58.880455 ,  20.020107 ,\n",
      "        -12.864763 , -16.041245 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.46801764]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 1 O's\n",
      "Player 2 chips: 5 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  | O | O | O |   | T\n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  1.8385818 , -28.65416   ,   5.056907  ,  -0.58689135,\n",
      "        -23.127333  ,  23.86089   , -11.327778  ,   2.0790136 ,\n",
      "         25.854519  ,  28.61242   , -11.2113695 ,  -4.123334  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9595648]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 0 O's\n",
      "Player 2 chips: 5 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  | O | O | O |   |  \n",
      "  | O | O | O |   | T\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -7.710308 , -15.910186 ,  -3.8434312, -28.68434  ,  -5.562125 ,\n",
      "          1.8529088, -21.89883  ,   7.073594 ,  57.746735 ,  19.88731  ,\n",
      "        -11.918462 , -13.339197 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.46867666]], dtype=float32)>)\n",
      "Player 1 chips: 6 T's and 0 O's\n",
      "Player 2 chips: 4 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   |  \n",
      "  | O | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -1.8564324 , -27.681442  ,   6.403826  ,  -0.55123174,\n",
      "        -21.835106  ,  24.319027  , -11.406044  ,   0.03811085,\n",
      "         25.4294    ,  27.268671  ,  -9.66087   ,  -4.354566  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9361401]], dtype=float32)>)\n",
      "Player 1 chips: 5 T's and 0 O's\n",
      "Player 2 chips: 4 T's and 2 O's\n",
      "  |   | O | O |   |  \n",
      "  |   | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -9.406629 , -10.580391 ,  -7.675687 , -31.504406 ,  -5.4061027,\n",
      "         -3.057065 , -21.807066 ,   6.0821443,  68.33892  ,  23.033134 ,\n",
      "        -14.958108 , -15.366182 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.44782898]], dtype=float32)>)\n",
      "Player 1 chips: 5 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 2 O's\n",
      "  |   | O | O |   | T\n",
      "  |   | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  1.7752913, -27.809244 ,   6.182708 ,   1.9330468, -20.826328 ,\n",
      "         23.322897 ,  -9.007488 ,   1.6965386,  15.779177 ,  24.86171  ,\n",
      "         -7.3003163,  -3.4829981]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9964697]], dtype=float32)>)\n",
      "Player 1 chips: 4 T's and 0 O's\n",
      "Player 2 chips: 3 T's and 2 O's\n",
      "  |   | O | O |   | T\n",
      "  |   | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "  | O | O | O | T | T\n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-10.527183  ,  -6.130498  ,  -7.9198523 , -28.838694  ,\n",
      "         -0.11385602,  -5.601724  , -17.840294  ,   4.2169023 ,\n",
      "         56.22797   ,  16.690222  , -10.741004  , -12.620756  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.46377268]], dtype=float32)>)\n",
      "Player 1 chips: 4 T's and 0 O's\n",
      "Player 2 chips: 2 T's and 2 O's\n",
      "  |   | O | O |   | T\n",
      "  |   | O | O |   | T\n",
      "  | O | O | O |   | T\n",
      "T | O | O | O | T | T\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  5.257953 , -30.52586  ,   7.863569 ,   1.9904556, -23.232767 ,\n",
      "         24.383997 , -10.074559 ,   4.7020698,  16.920298 ,  24.316067 ,\n",
      "         -9.258458 ,  -3.4531202]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.98017704]], dtype=float32)>)\n",
      "Player 1 chips: 3 T's and 0 O's\n",
      "Player 2 chips: 2 T's and 2 O's\n",
      "  |   | O | O |   | T\n",
      "  |   | O | O |   | T\n",
      "  | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ -8.702571 , -11.413018 ,  -8.472148 , -29.946352 ,  -3.1130078,\n",
      "         -1.1881151, -20.084375 ,   4.1699786,  58.43021  ,  21.766577 ,\n",
      "        -11.417973 , -13.108631 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.47331893]], dtype=float32)>)\n",
      "Player 1 chips: 3 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 2 O's\n",
      "  |   | O | O |   | T\n",
      "  |   | O | O |   | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  0.48933983, -25.951637  ,   8.089167  ,   0.726925  ,\n",
      "        -19.723654  ,  20.792576  ,  -9.620359  ,   3.6447437 ,\n",
      "         18.327593  ,  20.718632  ,  -7.802456  ,  -3.747609  ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9374295]], dtype=float32)>)\n",
      "Player 1 chips: 2 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 2 O's\n",
      "  |   | O | O |   | T\n",
      "  |   | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-10.447587 ,  -8.705743 ,  -9.198968 , -28.57248  ,  -2.409763 ,\n",
      "         -2.950288 , -18.683376 ,   3.2480109,  58.457706 ,  21.904709 ,\n",
      "        -10.854447 , -13.662821 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.46991074]], dtype=float32)>)\n",
      "Player 1 chips: 2 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 1 O's\n",
      "  |   | O | O |   | T\n",
      "  | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 13.461383, -45.20287 ,  11.517186,  -4.528034, -29.170702,\n",
      "         32.232212, -19.15567 ,  13.258355,  25.482908,  28.28487 ,\n",
      "        -14.376533,  -4.459452]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.92315024]], dtype=float32)>)\n",
      "Player 1 chips: 1 T's and 0 O's\n",
      "Player 2 chips: 1 T's and 1 O's\n",
      "  |   | O | O | T | T\n",
      "  | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player 0 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[-16.156532 ,   0.6704851, -10.992809 , -24.063986 ,   2.3908865,\n",
      "         -7.9275107, -12.310725 ,  -2.3132322,  51.191437 ,  19.092249 ,\n",
      "         -8.229643 , -11.911879 ]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.531728]], dtype=float32)>)\n",
      "Player 1 chips: 1 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 1 O's\n",
      "  |   | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action was 1 and the reward was 0\n",
      "\n",
      "player 1 thinks values are (<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[  8.246423 , -34.71486  ,  11.1182995,   0.725356 , -25.427876 ,\n",
      "         27.568989 , -12.925352 ,   7.175428 ,  17.529087 ,  22.859497 ,\n",
      "         -9.740168 ,  -4.2615166]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.9465646]], dtype=float32)>)\n",
      "Player 1 chips: 0 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 1 O's\n",
      "  | T | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "action was 6 and the reward was 1\n",
      "\n",
      "Player 1 chips: 0 T's and 0 O's\n",
      "Player 2 chips: 0 T's and 0 O's\n",
      "O | T | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n",
      "T | O | O | O | T | T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, <__main__.game at 0x2496f7323d0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(players = [agent, agent], display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
