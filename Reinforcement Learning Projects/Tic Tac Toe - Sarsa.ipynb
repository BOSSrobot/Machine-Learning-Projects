{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After failing with Deep Neural Network using Sarsa I needed to make sure I could do this without func approx and lo and behold, this took me like literally just 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class board:\n",
    "    \n",
    "    def __init__(self):    \n",
    "        #Game specific Variables\n",
    "        self.turn = 1\n",
    "        self.moves_left = 9\n",
    "        self.state = np.array([[0 for _ in range(9)]])\n",
    "        self.mask = np.array([[1 for _ in range(9)]])\n",
    "        \n",
    "    def update(self, action):\n",
    "        \n",
    "        if self.state[0][action] != 0:\n",
    "            raise ValueError('invalid move!', self.state, self.turn, self.moves_left, action)\n",
    "            return 'invalid move'\n",
    "        \n",
    "        self.state[0][action] = self.turn\n",
    "        self.mask[0][action] = 0\n",
    "        self.turn = (self.turn == 1)*(-1)+(self.turn == -1)*(1)\n",
    "        self.moves_left -= 1\n",
    "        \n",
    "        if self.moves_left < 5:\n",
    "            win_cond = -3*self.turn\n",
    "                        \n",
    "            # row check\n",
    "            temp = action//3 * 3\n",
    "            if self.state[0][temp] + self.state[0][temp+1] + self.state[0][temp+2] == win_cond:\n",
    "                return f\"{-1*self.turn} won\"\n",
    "            \n",
    "            # col check\n",
    "            if self.state[0][action] + self.state[0][(action+3)%9] + self.state[0][(action+6)%9] == win_cond:\n",
    "                return f\"{-1*self.turn} won\"\n",
    "            \n",
    "            #diag check\n",
    "            if action%2 == 0:\n",
    "                if self.state[0][0] + self.state[0][4] + self.state[0][8] == win_cond \\\n",
    "                or self.state[0][2] + self.state[0][4] + self.state[0][6] == win_cond:\n",
    "                    return f\"{-1*self.turn} won\"\n",
    "                            \n",
    "            if self.moves_left == 0:\n",
    "                return 'tie'\n",
    "        \n",
    "        return 'valid move'\n",
    "\n",
    "    def reset(self):\n",
    "        self.turn = 1\n",
    "        self.moves_left = 9\n",
    "        self.state = np.array([[0 for _ in range(9)]])\n",
    "        self.mask = np.array([[1 for _ in range(9)]])\n",
    "\n",
    "class game:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #RL Variables\n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "        \n",
    "        #Tic Tac Toe Board\n",
    "        self.gameboard = board()\n",
    "        \n",
    "    def update(self, action):                    \n",
    "        result = self.gameboard.update(action)\n",
    "        \n",
    "        if result == 'invalid move':\n",
    "            self.reward = -1\n",
    "            self.terminal = True\n",
    "            \n",
    "        elif result == 'valid move':\n",
    "            self.reward = 0\n",
    "            self.terminal = False\n",
    "            \n",
    "        elif result == '1 won':\n",
    "            self.reward = 1 * -1 * self.gameboard.turn\n",
    "            self.terminal = True\n",
    "            \n",
    "        elif result == '-1 won':\n",
    "            self.reward = -1 * -1 * self.gameboard.turn\n",
    "            self.terminal = True\n",
    "        \n",
    "        elif result == 'tie':\n",
    "            self.reward = 0\n",
    "            self.terminal = True\n",
    "                \n",
    "    def reset(self):\n",
    "        self.gameboard.reset()\n",
    "        \n",
    "        self.terminal = False\n",
    "        self.reward = 0\n",
    "    \n",
    "    def return_state_features(self):\n",
    "        return self.gameboard.state.reshape(1,9)*self.gameboard.turn\n",
    "    \n",
    "    def get_mask(self):\n",
    "        return self.gameboard.mask\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "    \n",
    "class illustrator:\n",
    "    \n",
    "    def __init__(self, host_game):\n",
    "        self.host = host_game\n",
    "    \n",
    "    def draw(self, state=None):\n",
    "        if state is not None:\n",
    "            st = state\n",
    "        else:\n",
    "            st = self.host.gameboard.state*self.host.gameboard.turn\n",
    "        \n",
    "        def convert(n):\n",
    "            return \"X\"*(n==1) + \" \"*(n==0) + \"O\"*(n==-1)\n",
    "        temp = [[convert(int(st[0][3*j+i])) for i in range(3)] for j in range(3)]\n",
    "        \n",
    "        print(f\"{temp[0][0]}|{temp[0][1]}|{temp[0][2]}\")\n",
    "        print(\"-----\")\n",
    "        print(f\"{temp[1][0]}|{temp[1][1]}|{temp[1][2]}\")\n",
    "        print(\"-----\")\n",
    "        print(f\"{temp[2][0]}|{temp[2][1]}|{temp[2][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class player:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0\n",
    "        \n",
    "    def q_values(self, s):\n",
    "        return \"IS A PLAYER\"\n",
    "    \n",
    "    def choose_action(self, s, m):\n",
    "        return int(input(\"Where would you like to move? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_bot:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0\n",
    "        \n",
    "    def q_values(self, s):\n",
    "        return \"IS A RANDOM BOT\"\n",
    "    \n",
    "    def choose_action(self, s, m):\n",
    "        return (np.random.uniform(low = 0.5, high = 1, size=[1,9]) * m).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sarsa_simple:\n",
    "    def __init__(self, num_states, num_actions, alpha, epsilon, gamma, decay=0.99999):\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = 0.1\n",
    "        self.decay = decay\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def pre_process(state_features):\n",
    "        st = 0\n",
    "        for i in range(9):\n",
    "            st+= 3**i * (state_features[0][i]+1)\n",
    "        return st\n",
    "    \n",
    "    def q_values(self, state_features):\n",
    "        return self.q_table[sarsa_simple.pre_process(state_features)]\n",
    "        \n",
    "    def choose_action(self, state_features, mask):\n",
    "        \n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon*=self.decay\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            p = np.random.uniform(size=(1,9)) * mask\n",
    "            return int(p.argmax())\n",
    "        \n",
    "        q_values = self.q_values(state_features)\n",
    "        q_values = (q_values + 1) * mask\n",
    "        \n",
    "        return int(q_values.argmax())\n",
    "        \n",
    "    def update_normal(self, s, a, r, s2, a2):\n",
    "        s = sarsa_simple.pre_process(s)\n",
    "        s2 = sarsa_simple.pre_process(s2)\n",
    "        self.q_table[s][a] += self.alpha*(r+self.gamma*self.q_table[s2][a2]-self.q_table[s][a])\n",
    "    \n",
    "    def update_final(self, s, a, r):\n",
    "        s = sarsa_simple.pre_process(s)\n",
    "        self.q_table[s][a] += self.alpha*(r-self.q_table[s][a])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def save_simple(agent, filename = 'simple_agent'):\n",
    "    \n",
    "    data = {\"q_table\": agent.q_table.tolist(),\n",
    "            \"gamma\": agent.gamma,\n",
    "            \"alpha\": agent.alpha,\n",
    "            \"epsilon\": agent.epsilon,\n",
    "            \"decay\": agent.decay}\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_simple(filename = 'simple_agent'):\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    agent = sarsa_simple(1,1,0,0,0)\n",
    "    \n",
    "    agent.q_table = np.array(data[\"q_table\"])\n",
    "    agent.alpha = data['alpha']\n",
    "    agent.gamma = data['gamma']\n",
    "    agent.epsilon = data['epsilon']\n",
    "    agent.decay = data['decay']\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999 steps are done! Agent epsilon: 0.09993486931725679   \n",
      "Training finished. Took -47.2914116 seconds\n"
     ]
    }
   ],
   "source": [
    "b = game()\n",
    "i = illustrator(b)\n",
    "agent = sarsa_simple(19683, 9, 0.05, 1, 0.9, 0.9991)\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "training_steps = 100000\n",
    "for step in range(training_steps):\n",
    "    if step%100==0:\n",
    "        print(f\"{step} steps are done! Agent epsilon: {agent.epsilon}   \", end='\\r')\n",
    "    \n",
    "    s1, m = b.return_state_features(), b.get_mask()\n",
    "    a1 = agent.choose_action(s1, m) \n",
    "    b.update(a1)\n",
    "    r1 = b.get_reward()\n",
    "\n",
    "    s2, m = b.return_state_features(), b.get_mask()\n",
    "    a2 = agent.choose_action(s2, m) \n",
    "    b.update(a2)\n",
    "    r2 = b.get_reward()\n",
    "\n",
    "    remember = deque([(s1, a1, r1), (s2, a2, r2)])\n",
    "\n",
    "    while not b.terminal:\n",
    "\n",
    "        s, m = b.return_state_features(), b.get_mask()\n",
    "        a = agent.choose_action(s, m)\n",
    "        b.update(a)\n",
    "        r = b.get_reward()\n",
    "\n",
    "        remember.append((s,a,r))\n",
    "        sb, ab, rb = remember.popleft()\n",
    "        agent.update_normal(sb, ab, rb, s, a)\n",
    "\n",
    "    sb2, ab2, _ = remember.popleft() #Idiot action\n",
    "    sb1, ab1, r = remember.popleft() #Winner boy\n",
    "    agent.update_final(sb2, ab2, -1*r)\n",
    "    agent.update_final(sb1, ab1, r)\n",
    "    \n",
    "    b.reset()\n",
    "    \n",
    "    if step == training_steps - 1: \n",
    "        print(f\"{step} steps are done! Agent epsilon: {agent.epsilon}   \")\n",
    "    \n",
    "save_simple(agent, 'agent_{}'.format(training_steps))\n",
    "print(f\"Training finished. Took {time.perf_counter() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player1 thinks values are [ 0.00755753  0.04452375  0.01543171  0.00230985  0.08720691 -0.00197139\n",
      "  0.00100311  0.01238209  0.00348525]\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "action was 4 and the reward was 0\n",
      "\n",
      "player2 thinks values are [-0.22853082 -0.68060082 -0.17105761 -0.63959181  0.         -0.69608456\n",
      " -0.23204509 -0.69973772 -0.27887964]\n",
      " | | \n",
      "-----\n",
      " |O| \n",
      "-----\n",
      " | | \n",
      "action was 2 and the reward was 0\n",
      "\n",
      "player1 thinks values are [0.05822488 0.05409265 0.         0.03412084 0.         0.04989984\n",
      " 0.02719885 0.03442169 0.03167058]\n",
      " | |O\n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | | \n",
      "action was 0 and the reward was 0\n",
      "\n",
      "player2 thinks values are [ 0.         -0.99412706  0.         -0.94052659  0.         -0.84120004\n",
      " -0.9713807  -0.95150947  0.03054105]\n",
      "O| |X\n",
      "-----\n",
      " |O| \n",
      "-----\n",
      " | | \n",
      "action was 8 and the reward was 0\n",
      "\n",
      "player1 thinks values are [ 0.         -0.73555571  0.         -0.67646646  0.          0.10177298\n",
      " -0.74255199 -0.6739063   0.        ]\n",
      "X| |O\n",
      "-----\n",
      " |X| \n",
      "-----\n",
      " | |O\n",
      "action was 5 and the reward was 0\n",
      "\n",
      "player2 thinks values are [ 0.         -0.9638465   0.         -0.08361864  0.          0.\n",
      " -0.88747948 -0.82772983  0.        ]\n",
      "O| |X\n",
      "-----\n",
      " |O|O\n",
      "-----\n",
      " | |X\n",
      "action was 3 and the reward was 0\n",
      "\n",
      "player1 thinks values are [0.         0.12591393 0.         0.         0.         0.\n",
      " 0.         0.00100254 0.        ]\n",
      "X| |O\n",
      "-----\n",
      "O|X|X\n",
      "-----\n",
      " | |O\n",
      "action was 1 and the reward was 0\n",
      "\n",
      "player2 thinks values are [ 0.  0.  0.  0.  0.  0. -1.  0.  0.]\n",
      "O|O|X\n",
      "-----\n",
      "X|O|O\n",
      "-----\n",
      " | |X\n",
      "action was 7 and the reward was 0\n",
      "\n",
      "player1 thinks values are [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "X|X|O\n",
      "-----\n",
      "O|X|X\n",
      "-----\n",
      " |O|O\n",
      "action was 6 and the reward was 0\n",
      "\n",
      "O|O|X\n",
      "-----\n",
      "X|O|O\n",
      "-----\n",
      "O|X|X\n"
     ]
    }
   ],
   "source": [
    "b = game()\n",
    "i = illustrator(b)\n",
    "player1 = load_simple('agent_10000')#\n",
    "player1 = load_simple('agent_{}'.format(training_steps))\n",
    "player2 = player()#player2 = load_simple('good_agent')\n",
    "\n",
    "init_e1, init_e2 = player1.epsilon, player2.epsilon\n",
    "player1.epsilon, player2.epsilon = 0, 0\n",
    "\n",
    "while not b.terminal:\n",
    "    s, m = b.return_state_features(), b.get_mask()\n",
    "    print(f\"player1 thinks values are {player1.q_values(s)}\")\n",
    "    i.draw()\n",
    "    a = player1.choose_action(s, m)\n",
    "    b.update(a)\n",
    "    r = b.get_reward()\n",
    "    print(f\"action was {a} and the reward was {r}\\n\")\n",
    "    \n",
    "\n",
    "    if not b.terminal:\n",
    "        s, m = b.return_state_features(), b.get_mask()\n",
    "        print(f\"player2 thinks values are {player2.q_values(s)}\")\n",
    "        i.draw()\n",
    "        a = player2.choose_action(s, m)\n",
    "        b.update(a)\n",
    "        r = b.get_reward()\n",
    "        print(f\"action was {a} and the reward was {r}\\n\")\n",
    "            \n",
    "player1.epsilon, player2.epsilon = init_e1, init_e2\n",
    "i.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 random_bot won 0 times and player 2 sarsa_simple won 17837 times. They drew 2063 times. They played 19900 games.            \r"
     ]
    }
   ],
   "source": [
    "b = game()\n",
    "i = illustrator(b)\n",
    "\n",
    "player1 = random_bot()#load_simple('good_agent')\n",
    "player2 = load_simple('agent_300000')\n",
    "\n",
    "init_e1, init_e2 = player1.epsilon, player2.epsilon\n",
    "expl_rate = 0\n",
    "player1.epsilon, player2.epsilon = expl_rate, expl_rate \n",
    "\n",
    "num_matches = 20000\n",
    "p1_wins = 0\n",
    "p2_wins = 0\n",
    "for i in range(num_matches):\n",
    "    \n",
    "    while not b.terminal:\n",
    "        s, m = b.return_state_features(), b.get_mask()\n",
    "        a = player1.choose_action(s, m)\n",
    "        b.update(a)\n",
    "        r = b.get_reward()\n",
    "        if r == 1:\n",
    "            p1_wins +=1\n",
    "\n",
    "\n",
    "        if not b.terminal:\n",
    "            s, m = b.return_state_features(), b.get_mask()\n",
    "            a = player2.choose_action(s, m)\n",
    "            b.update(a)\n",
    "            r = b.get_reward()\n",
    "            if r == 1:\n",
    "                p2_wins+=1\n",
    "                    \n",
    "    b.reset()\n",
    "    \n",
    "    if i%100==0:\n",
    "        msg1 = f'player 1 {str(player1.__class__)[17:-2]} won {p1_wins} times'\n",
    "        msg2 = f' and player 2 {str(player2.__class__)[17:-2]} won {p2_wins} times.' \n",
    "        msg3 = f' They drew {i-p1_wins-p2_wins} times.'\n",
    "        msg4 = f' They played {i} games.    '\n",
    "        print(msg1 + msg2 + msg3 + msg4 + '        ', end = '\\r')\n",
    "    #print(f\"hi {i}\", end='\\r') \n",
    "player1.epsilon, player2.epsilon = init_e1, init_e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
